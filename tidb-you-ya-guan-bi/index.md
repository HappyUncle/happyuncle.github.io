# TiDB ä¼˜é›…å…³é—­


---



## èƒŒæ™¯

ä»Šå¤©ä½¿ç”¨tiupåšå®éªŒçš„äº‹åï¼Œå°†tidbèŠ‚ç‚¹ä»2ä¸ªç¼©åˆ°1ä¸ªï¼Œå‘ç°tiupè¿”å›æˆåŠŸä½†æ˜¯tidb-serverè¿›ç¨‹è¿˜åœ¨ã€‚

è¿™å°±å¼•å‘çš„æˆ‘çš„å¥½å¥‡å¿ƒï¼Œwhyï¼Ÿ

## å®éªŒå¤ç°

### å¯åŠ¨é›†ç¾¤

```bash
#( 07/31/23@ 8:32ä¸‹åˆ )( happy@ZBMAC-f298743e3 ):~/docker/tiup/tiproxy
   tiup playground v6.4.0 --db 2 --kv 1 --pd 1 --tiflash 0 --without-monitor --db.config tidb.toml
tiup is checking updates for component playground ...
Starting component `playground`: /Users/happy/.tiup/components/playground/v1.12.5/tiup-playground v6.4.0 --db 2 --kv 1 --pd 1 --tiflash 0 --without-monitor --db.config tidb.toml
Start pd instance:v6.4.0
Start tikv instance:v6.4.0
Start tidb instance:v6.4.0
Start tidb instance:v6.4.0
Waiting for tidb instances ready
127.0.0.1:4000 ... Done
127.0.0.1:4001 ... Done

ğŸ‰ TiDB Playground Cluster is started, enjoy!

Connect TiDB:   mysql --comments --host 127.0.0.1 --port 4000 -u root
Connect TiDB:   mysql --comments --host 127.0.0.1 --port 4001 -u root
TiDB Dashboard: http://127.0.0.1:2379/dashboard
```

### æŸ¥çœ‹èŠ‚ç‚¹ä¿¡æ¯

```bash
#( 07/31/23@ 8:32ä¸‹åˆ )( happy@ZBMAC-f298743e3 ):~
   tiup playground display
tiup is checking updates for component playground ...
Starting component `playground`: /Users/happy/.tiup/components/playground/v1.12.5/tiup-playground display
Pid    Role  Uptime
---    ----  ------
10113  pd    49.376485092s
10114  tikv  49.32262974s
10115  tidb  49.283144092s
10116  tidb  49.245069308s
```

### ç¼©æ‰ä¸€ä¸ªtidbèŠ‚ç‚¹

```bash
#( 07/31/23@ 8:34ä¸‹åˆ )( happy@ZBMAC-f298743e3 ):~
   tiup playground scale-in --pid 10115
tiup is checking updates for component playground ...
Starting component `playground`: /Users/happy/.tiup/components/playground/v1.12.5/tiup-playground scale-in --pid 10115
scale in tidb success
```

è¿™é‡Œå¯ä»¥çœ‹åˆ°å·²ç»è¿”å›äº† scale in tidb success

### æŸ¥çœ‹è¿›ç¨‹

```bash
#( 07/31/23@ 8:34ä¸‹åˆ )( happy@ZBMAC-f298743e3 ):~
   ps -ef | grep 10115
  502 11371 99718   0  8:34ä¸‹åˆ ttys001    0:00.00 grep --color=auto --exclude-dir=.bzr --exclude-dir=CVS --exclude-dir=.git --exclude-dir=.hg --exclude-dir=.svn --exclude-dir=.idea --exclude-dir=.tox 10115
  502 10115 10111   0  8:32ä¸‹åˆ ttys005    0:04.29 /Users/happy/.tiup/components/tidb/v6.4.0/tidb-server -P 4000 --store=tikv --host=127.0.0.1 --status=10080 --path=127.0.0.1:2379 --log-file=/Users/happy/.tiup/data/TlaeoSj/tidb-0/tidb.log --config=/Users/happy/.tiup/data/TlaeoSj/tidb-0/tidb.toml
```

è¿›ç¨‹è¿˜æ˜¯å­˜åœ¨

## åˆ†æ

äºæ˜¯æŸ¥çœ‹äº† v6.4.0 çš„ tidb-server ä»£ç ã€‚é¦–å…ˆæƒ³åˆ°å»mainå‡½æ•°çœ‹ä¸‹closeçš„æµç¨‹

### main

```go
func main() {
    //..
    signal.SetupSignalHandler(func(graceful bool) {
        svr.Close()
        cleanup(svr, storage, dom, graceful)
        cpuprofile.StopCPUProfiler()
        close(exited)
    })
    // ...
}
```

åœ¨è¿™é‡Œå‘ç°ä¸¤ä¸ªé‡è¦çš„é€»è¾‘ svr.Close()ï¼Œcleanup(svr, storage, dom, graceful)

### svr.Close()

```go
// Close closes the server.
func (s *Server) Close() {
	s.startShutdown()
	s.rwlock.Lock() // prevent new connections
	defer s.rwlock.Unlock()

	if s.listener != nil {
		err := s.listener.Close()
		terror.Log(errors.Trace(err))
		s.listener = nil
	}
	if s.socket != nil {
		err := s.socket.Close()
		terror.Log(errors.Trace(err))
		s.socket = nil
	}
	if s.statusServer != nil {
		err := s.statusServer.Close()
		terror.Log(errors.Trace(err))
		s.statusServer = nil
	}
	if s.grpcServer != nil {
		s.grpcServer.Stop()
		s.grpcServer = nil
	}
	if s.autoIDService != nil {
		s.autoIDService.Close()
	}
	if s.authTokenCancelFunc != nil {
		s.authTokenCancelFunc()
	}
	s.wg.Wait()
	metrics.ServerEventCounter.WithLabelValues(metrics.EventClose).Inc()
}

func (s *Server) startShutdown() {
	s.rwlock.RLock()
	logutil.BgLogger().Info("setting tidb-server to report unhealthy (shutting-down)")
	s.inShutdownMode = true
	s.rwlock.RUnlock()
	// give the load balancer a chance to receive a few unhealthy health reports
	// before acquiring the s.rwlock and blocking connections.
	waitTime := time.Duration(s.cfg.GracefulWaitBeforeShutdown) * time.Second
	if waitTime > 0 {
		logutil.BgLogger().Info("waiting for stray connections before starting shutdown process", zap.Duration("waitTime", waitTime))
		time.Sleep(waitTime)
	}
}
```

ä»ä¸Šé¢çš„é€»è¾‘å¯ä»¥çœ‹åˆ°ï¼Œcloseçš„æ—¶å€™å…ˆstartShutdownå†è¿›è¡Œèµ„æºå›æ”¶ã€‚è€Œåœ¨æ‰§è¡ŒstartShutdownçš„æ—¶å€™ï¼Œå±…ç„¶æœ‰ä¸ªtime.Sleep(waitTime)ã€‚

ç„¶åç ”ç©¶ä¸‹ [graceful-wait-before-shutdown](https://docs.pingcap.com/zh/tidb/v6.5/tidb-configuration-file#graceful-wait-before-shutdown-%E4%BB%8E-v50-%E7%89%88%E6%9C%AC%E5%BC%80%E5%A7%8B%E5%BC%95%E5%85%A5) å‚æ•°ï¼Œå‘ç°å‚æ•°æ˜¯0ï¼Œä¸æ˜¯æ­¤å¤„å¯¼è‡´çš„ã€‚

> åœ¨ TiDB ç­‰å¾…æœåŠ¡å™¨å…³é—­æœŸé—´ï¼ŒHTTP çŠ¶æ€ä¼šæ˜¾ç¤ºå¤±è´¥ï¼Œä½¿å¾—è´Ÿè½½å‡è¡¡å™¨å¯ä»¥é‡æ–°è·¯ç”±æµé‡
> é»˜è®¤å€¼ï¼š0
> æŒ‡å®šå…³é—­æœåŠ¡å™¨æ—¶ TiDB ç­‰å¾…çš„ç§’æ•°ï¼Œä½¿å¾—å®¢æˆ·ç«¯æœ‰æ—¶é—´æ–­å¼€è¿æ¥ã€‚

### cleanup()

åœ¨ cleanup ä¸­çœ‹åˆ°äº† GracefulDown å’Œ TryGracefulDown ä¸¤ä¸ªæ–¹æ³•

```go
func cleanup(svr *server.Server, storage kv.Storage, dom *domain.Domain, graceful bool) {
	if graceful {
		done := make(chan struct{})
		svr.GracefulDown(context.Background(), done)
	} else {
		svr.TryGracefulDown()
	}
	plugin.Shutdown(context.Background())
	closeDomainAndStorage(storage, dom)
	disk.CleanUp()
	topsql.Close()
}
```

#### TryGracefulDown

ç ”ç©¶å‘ç°ä½¿ç”¨ SIGHUP ç»ˆæ­¢è¿›ç¨‹æ—¶ä½¿ç”¨ TryGracefulDown æ–¹æ³•ï¼Œå…¶ä»–æ—¶å€™ä½¿ç”¨ GracefulDownã€‚å¯¹æ¯” TryGracefulDown å’Œ GracefulDown å®ç°ï¼Œ TryGracefulDown åªæ˜¯å¤šä¸ª15sçš„è¶…æ—¶å¤„ç†ï¼Œåº•å±‚é€»è¾‘è¿˜æ˜¯ GracefulDown

```go
var gracefulCloseConnectionsTimeout = 15 * time.Second

// TryGracefulDown will try to gracefully close all connection first with timeout. if timeout, will close all connection directly.
func (s *Server) TryGracefulDown() {
	ctx, cancel := context.WithTimeout(context.Background(), gracefulCloseConnectionsTimeout)
	defer cancel()
	done := make(chan struct{})
	go func() {
		s.GracefulDown(ctx, done)
	}()
	select {
	case <-ctx.Done():
		s.KillAllConnections()
	case <-done:
		return
	}
}
```

#### GracefulDown

ä¸‹é¢æ˜¯ GracefulDown å®ç°ï¼ŒåŸæ¥åœ¨è¿™é‡Œä¼šé—´éš”1sï¼Œä¸€ç›´åˆ¤æ–­å®¢æˆ·ç«¯è¿æ¥æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨æ‰é€€å‡ºã€‚

```go
// GracefulDown waits all clients to close.
func (s *Server) GracefulDown(ctx context.Context, done chan struct{}) {
	logutil.Logger(ctx).Info("[server] graceful shutdown.")
	metrics.ServerEventCounter.WithLabelValues(metrics.EventGracefulDown).Inc()

	count := s.ConnectionCount()
	for i := 0; count > 0; i++ {
		s.kickIdleConnection()

		count = s.ConnectionCount()
		if count == 0 {
			break
		}
		// Print information for every 30s.
		if i%30 == 0 {
			logutil.Logger(ctx).Info("graceful shutdown...", zap.Int("conn count", count))
		}
		ticker := time.After(time.Second)
		select {
		case <-ctx.Done():
			return
		case <-ticker:
		}
	}
	close(done)
}
```

#### ConnectionCount

åˆ¤æ–­è¿æ¥ä¸ªæ•°çš„é€»è¾‘ä¹Ÿå¾ˆç®€å•ï¼Œå°±æ˜¯å¯¹ç®—ä¸‹ s.clients çš„ length

```go
// ConnectionCount gets current connection count.
func (s *Server) ConnectionCount() int {
	s.rwlock.RLock()
	cnt := len(s.clients)
	s.rwlock.RUnlock()
	return cnt
}
```

å…¶ä¸­è¿˜æœ‰ä¸€ä¸ªå¥‡æ€ªçš„å‡½æ•° kickIdleConnectionï¼Œè¿™ä¸ªæ˜¯åšä»€ä¹ˆçš„ï¼Ÿ

#### kickIdleConnection

çœ‹é€»è¾‘æ˜¯æ”¶é›†å¯ä»¥è¢«closeçš„ä¼šè¯ç„¶åcloseæ‰ã€‚

```go
func (s *Server) kickIdleConnection() {
	var conns []*clientConn
	s.rwlock.RLock()
	for _, cc := range s.clients {
		if cc.ShutdownOrNotify() {
			// Shutdowned conn will be closed by us, and notified conn will exist themselves.
			conns = append(conns, cc)
		}
	}
	s.rwlock.RUnlock()

	for _, cc := range conns {
		err := cc.Close()
		if err != nil {
			logutil.BgLogger().Error("close connection", zap.Error(err))
		}
	}
}
```

é‚£ä¹ˆä»€ä¹ˆæ ·çš„ä¼šè¯å¯ä»¥è¢«closeå‘¢ï¼Ÿ

#### ShutdownOrNotify

æœ‰ä¸‰ç±»ï¼š

- client çŠ¶æ€å¤„äº ServerStatusInTransï¼›
- çŠ¶æ€å¤„äº connStatusReading
- ä»¥åŠå¤„äº connStatusDispatching åœ¨ clientConn.Run æ–¹æ³•ä¸­è¢«å›æ”¶

```go
// ShutdownOrNotify will Shutdown this client connection, or do its best to notify.
func (cc *clientConn) ShutdownOrNotify() bool {
	if (cc.ctx.Status() & mysql.ServerStatusInTrans) > 0 {
		return false
	}
	// If the client connection status is reading, it's safe to shutdown it.
	if atomic.CompareAndSwapInt32(&cc.status, connStatusReading, connStatusShutdown) {
		return true
	}
	// If the client connection status is dispatching, we can't shutdown it immediately,
	// so set the status to WaitShutdown as a notification, the loop in clientConn.Run
	// will detect it and then exit.
	atomic.StoreInt32(&cc.status, connStatusWaitShutdown)
	return false
}

const (
	connStatusDispatching int32 = iota
	connStatusReading
	connStatusShutdown     // Closed by server.
	connStatusWaitShutdown // Notified by server to close.
)
```


## ç ´æ¡ˆ

é€šè¿‡ä¸Šé¢çš„åˆ†æï¼Œæˆ‘ä»¬æ³¨æ„åˆ°äº†å¤„äº ServerStatusInTrans çŠ¶æ€çš„è¿æ¥ä¸ä¼šè¢«å…³é—­ï¼Œç„¶åè¿æ¥è¯¥èŠ‚ç‚¹æ‰§è¡Œshow processlistå‘ç°çš„ç¡®æœ‰ä¸ªå¤„äºäº‹åŠ¡ä¸­çš„ä¼šè¯

```mysql
mysql> show processlist;
+---------------------+------+-----------------+------+---------+------+----------------------------+------------------+
| Id                  | User | Host            | db   | Command | Time | State                      | Info             |
+---------------------+------+-----------------+------+---------+------+----------------------------+------------------+
| 7794237818187809175 | root | 127.0.0.1:61293 | a    | Query   |    0 | in transaction; autocommit | show processlist |
+---------------------+------+-----------------+------+---------+------+----------------------------+------------------+
1 row in set (0.00 sec)

```

å¹³æ—¶mysqlä½¿ç”¨çš„å¤šï¼Œmysqlåœ¨å…³é—­çš„æ—¶å€™ä¸ç®¡ä¼šè¯å¤„äºä»€ä¹ˆé˜¶æ®µï¼Œä¸ç®¡ä¸é¡¾ç›´æ¥åœæœï¼Œè€Œtidbçš„è¿™æ ·å¤„ç†ç€å®è®©æˆ‘æƒ³ä¸åˆ°ã€‚

## æ€»ç»“

æœ¬æ–‡ç®€çŸ­çš„åˆ†æäº†ä¸‹ tidb è¿›ç¨‹å…³é—­çš„å¤„ç†æµç¨‹ï¼Œæœ€ç»ˆå®šä½åˆ°è¿›ç¨‹æ²¡æœ‰åŠæ—¶å…³é—­çš„åŸå› ã€‚

å¯¹æ¯”äºmysqlçš„åœæœè¡Œä¸ºï¼Œè®©æˆ‘ä»¬å¯¹tidbçš„å¤„ç†æ–¹å¼æœ‰äº†ä¸ä¸€æ ·çš„ç†è§£ã€‚

å¯¹äº â€œgraceful-wait-before-shutdown å‚æ•°â€ã€â€œåœæœæ—¶ç­‰å¾…äº‹åŠ¡ç»“æŸçš„é€»è¾‘â€çš„ç¡®éœ€è¦åœ¨å®è·µä¸­æ‰èƒ½ç§¯ç´¯ã€‚


