[{"categories":null,"content":" 说明在上篇《TiProxy 尝鲜》中做了一些实验，比如加减tidb节点后tiproxy可以做到自动负载均衡，如果遇到会话有未提交的事务则等待事务结束才迁移。 本次主要研究这样的功能在tiproxy中是如何实现的，本次分享内容主要为以下几部分： tiproxy是怎么发现tidb？ tiproxy是在tidb节点间自动负载均衡的逻辑？ 在自动负载均衡时tiproxy是怎么做到优雅的session迁移、session上下文恢复？ tiproxy在自动负载均衡期间遇到处于未提交事务的session是怎么等待结束的？ ","date":"2023-07-30","objectID":"/tiproxy-yuan-li-he-shi-xian/:1:0","series":null,"tags":null,"title":"TiProxy 原理和实现","uri":"/tiproxy-yuan-li-he-shi-xian/#说明"},{"categories":null,"content":" Tiproxy 介绍tiproxy 在 2022年12月2日被operator支持 相关的设计文档可以从官方 README 和 goole doc 中查看 这个有个重要特性需要说明下： tiproxy组件不会保存账号的密码，因为这是不安全的行为，所以当进行会话迁移的时候使用的是 session token 认证方式(下文会提到这种方式的实现原理)。 ","date":"2023-07-30","objectID":"/tiproxy-yuan-li-he-shi-xian/:2:0","series":null,"tags":null,"title":"TiProxy 原理和实现","uri":"/tiproxy-yuan-li-he-shi-xian/#tiproxy-介绍"},{"categories":null,"content":" 声明目前tiproxy还处于实验阶段、功能还在持续开发中，本文讲述的内容跟日后GA版本可能存在差异，届时请各位看官留意。 另外本人能力有限，在阅读源码中难免有理解不到位的地方，如有发现欢迎在评论区指正，感谢。 开始发车 ","date":"2023-07-30","objectID":"/tiproxy-yuan-li-he-shi-xian/:3:0","series":null,"tags":null,"title":"TiProxy 原理和实现","uri":"/tiproxy-yuan-li-he-shi-xian/#声明"},{"categories":null,"content":" 原理分析","date":"2023-07-30","objectID":"/tiproxy-yuan-li-he-shi-xian/:4:0","series":null,"tags":null,"title":"TiProxy 原理和实现","uri":"/tiproxy-yuan-li-he-shi-xian/#原理分析"},{"categories":null,"content":" 1、tiproxy是怎么发现tidb？获取tidb拓扑最核心、简化后的代码如下，其实就是使用etcdCli.Get获取信息 // 从 etcd 获取 tidb 拓扑 路径 /topology/tidb/\u003cip:port\u003e/info /topology/tidb/\u003cip:port\u003e/ttl func (is *InfoSyncer) GetTiDBTopology(ctx context.Context) (map[string]*TiDBInfo, error) { res, err := is.etcdCli.Get(ctx, tidbinfo.TopologyInformationPath, clientv3.WithPrefix()) infos := make(map[string]*TiDBInfo, len(res.Kvs)/2) for _, kv := range res.Kvs { var ttl, addr string var topology *tidbinfo.TopologyInfo key := hack.String(kv.Key) switch { case strings.HasSuffix(key, ttlSuffix): addr = key[len(tidbinfo.TopologyInformationPath)+1 : len(key)-len(ttlSuffix)-1] ttl = hack.String(kv.Value) case strings.HasSuffix(key, infoSuffix): addr = key[len(tidbinfo.TopologyInformationPath)+1 : len(key)-len(infoSuffix)-1] json.Unmarshal(kv.Value, \u0026topology) default: continue } info := infos[addr] if len(ttl) \u003e 0 { info.TTL = hack.String(kv.Value) } else { info.TopologyInfo = topology } } return infos, nil } 这个函数是怎么被tiproxy用起来的呢？ 其实在每个proxy启动时后都会开启一个BackendObserver协程，这个协程会做三件事： func (bo *BackendObserver) observe(ctx context.Context) { for ctx.Err() == nil { // 获取 backendInfo, err := bo.fetcher.GetBackendList(ctx) // 检查 bhMap := bo.checkHealth(ctx, backendInfo) // 通知 bo.notifyIfChanged(bhMap) select { case \u003c-time.After(bo.healthCheckConfig.Interval): // 间隔3秒 case \u003c-bo.refreshChan: case \u003c-ctx.Done(): return } } } 第一步获取：从etcd获取tidb拓扑；代码见上； 第二步检查：判断获取到tidb节点是否可以连通、访问，给每个节点设置StatusHealthy或者StatusCannotConnect状态 func (bo *BackendObserver) checkHealth(ctx context.Context, backends map[string]*BackendInfo) map[string]*backendHealth { curBackendHealth := make(map[string]*backendHealth, len(backends)) for addr, info := range backends { bh := \u0026backendHealth{ status: StatusHealthy, } curBackendHealth[addr] = bh // http 服务检查 if info != nil \u0026\u0026 len(info.IP) \u003e 0 { schema := \"http\" httpCli := *bo.httpCli httpCli.Timeout = bo.healthCheckConfig.DialTimeout url := fmt.Sprintf(\"%s://%s:%d%s\", schema, info.IP, info.StatusPort, statusPathSuffix) resp, err := httpCli.Get(url) if err != nil { bh.status = StatusCannotConnect bh.pingErr = errors.Wrapf(err, \"connect status port failed\") continue } } // tcp 服务检查 conn, err := net.DialTimeout(\"tcp\", addr, bo.healthCheckConfig.DialTimeout) if err != nil { bh.status = StatusCannotConnect bh.pingErr = errors.Wrapf(err, \"connect sql port failed\") } } return curBackendHealth } 第三步通知：将检查后的 backends 列表跟内存中缓存的 backends 进行比较，将变动的 updatedBackends 进行通知 // notifyIfChanged 根据最新的 tidb 拓扑 bhMap 与之前的 tidb 拓扑 bo.curBackendInfo 进行比较 // - 在 bo.curBackendInfo 中但是不在 bhMap 中：说明 tidb 节点失联，需要记录下 // - 在 bo.curBackendInfo 中也在 bhMap 中，但是最新的状态不是 StatusHealthy：也需要记录下 // - 在 bhMap 中但是不在 bo.curBackendInfo 中：说明是新增 tidb 节点，需要记录下 func (bo *BackendObserver) notifyIfChanged(bhMap map[string]*backendHealth) { updatedBackends := make(map[string]*backendHealth) for addr, lastHealth := range bo.curBackendInfo { if lastHealth.status == StatusHealthy { if newHealth, ok := bhMap[addr]; !ok { updatedBackends[addr] = \u0026backendHealth{ status: StatusCannotConnect, pingErr: errors.New(\"removed from backend list\"), } updateBackendStatusMetrics(addr, lastHealth.status, StatusCannotConnect) } else if newHealth.status != StatusHealthy { updatedBackends[addr] = newHealth updateBackendStatusMetrics(addr, lastHealth.status, newHealth.status) } } } for addr, newHealth := range bhMap { if newHealth.status == StatusHealthy { lastHealth, ok := bo.curBackendInfo[addr] if !ok { lastHealth = \u0026backendHealth{ status: StatusCannotConnect, } } if lastHealth.status != StatusHealthy { updatedBackends[addr] = newHealth updateBackendStatusMetrics(addr, lastHealth.status, newHealth.status) } else if lastHealth.serverVersion != newHealth.serverVersion { // Not possible here: the backend finishes upgrading between two health checks. updatedBackends[addr] = newHealth } } } // Notify it even when the updatedBackends is empty, in order to clear the last error. bo.eventReceiver.OnBackendChanged(updatedBackends, nil) bo.curBackendInfo = bhMap } 通过上","date":"2023-07-30","objectID":"/tiproxy-yuan-li-he-shi-xian/:4:1","series":null,"tags":null,"title":"TiProxy 原理和实现","uri":"/tiproxy-yuan-li-he-shi-xian/#1tiproxy是怎么发现tidb"},{"categories":null,"content":" 1、tiproxy是怎么发现tidb？获取tidb拓扑最核心、简化后的代码如下，其实就是使用etcdCli.Get获取信息 // 从 etcd 获取 tidb 拓扑 路径 /topology/tidb//info /topology/tidb//ttl func (is *InfoSyncer) GetTiDBTopology(ctx context.Context) (map[string]*TiDBInfo, error) { res, err := is.etcdCli.Get(ctx, tidbinfo.TopologyInformationPath, clientv3.WithPrefix()) infos := make(map[string]*TiDBInfo, len(res.Kvs)/2) for _, kv := range res.Kvs { var ttl, addr string var topology *tidbinfo.TopologyInfo key := hack.String(kv.Key) switch { case strings.HasSuffix(key, ttlSuffix): addr = key[len(tidbinfo.TopologyInformationPath)+1 : len(key)-len(ttlSuffix)-1] ttl = hack.String(kv.Value) case strings.HasSuffix(key, infoSuffix): addr = key[len(tidbinfo.TopologyInformationPath)+1 : len(key)-len(infoSuffix)-1] json.Unmarshal(kv.Value, \u0026topology) default: continue } info := infos[addr] if len(ttl) \u003e 0 { info.TTL = hack.String(kv.Value) } else { info.TopologyInfo = topology } } return infos, nil } 这个函数是怎么被tiproxy用起来的呢？ 其实在每个proxy启动时后都会开启一个BackendObserver协程，这个协程会做三件事： func (bo *BackendObserver) observe(ctx context.Context) { for ctx.Err() == nil { // 获取 backendInfo, err := bo.fetcher.GetBackendList(ctx) // 检查 bhMap := bo.checkHealth(ctx, backendInfo) // 通知 bo.notifyIfChanged(bhMap) select { case \u003c-time.After(bo.healthCheckConfig.Interval): // 间隔3秒 case \u003c-bo.refreshChan: case \u003c-ctx.Done(): return } } } 第一步获取：从etcd获取tidb拓扑；代码见上； 第二步检查：判断获取到tidb节点是否可以连通、访问，给每个节点设置StatusHealthy或者StatusCannotConnect状态 func (bo *BackendObserver) checkHealth(ctx context.Context, backends map[string]*BackendInfo) map[string]*backendHealth { curBackendHealth := make(map[string]*backendHealth, len(backends)) for addr, info := range backends { bh := \u0026backendHealth{ status: StatusHealthy, } curBackendHealth[addr] = bh // http 服务检查 if info != nil \u0026\u0026 len(info.IP) \u003e 0 { schema := \"http\" httpCli := *bo.httpCli httpCli.Timeout = bo.healthCheckConfig.DialTimeout url := fmt.Sprintf(\"%s://%s:%d%s\", schema, info.IP, info.StatusPort, statusPathSuffix) resp, err := httpCli.Get(url) if err != nil { bh.status = StatusCannotConnect bh.pingErr = errors.Wrapf(err, \"connect status port failed\") continue } } // tcp 服务检查 conn, err := net.DialTimeout(\"tcp\", addr, bo.healthCheckConfig.DialTimeout) if err != nil { bh.status = StatusCannotConnect bh.pingErr = errors.Wrapf(err, \"connect sql port failed\") } } return curBackendHealth } 第三步通知：将检查后的 backends 列表跟内存中缓存的 backends 进行比较，将变动的 updatedBackends 进行通知 // notifyIfChanged 根据最新的 tidb 拓扑 bhMap 与之前的 tidb 拓扑 bo.curBackendInfo 进行比较 // - 在 bo.curBackendInfo 中但是不在 bhMap 中：说明 tidb 节点失联，需要记录下 // - 在 bo.curBackendInfo 中也在 bhMap 中，但是最新的状态不是 StatusHealthy：也需要记录下 // - 在 bhMap 中但是不在 bo.curBackendInfo 中：说明是新增 tidb 节点，需要记录下 func (bo *BackendObserver) notifyIfChanged(bhMap map[string]*backendHealth) { updatedBackends := make(map[string]*backendHealth) for addr, lastHealth := range bo.curBackendInfo { if lastHealth.status == StatusHealthy { if newHealth, ok := bhMap[addr]; !ok { updatedBackends[addr] = \u0026backendHealth{ status: StatusCannotConnect, pingErr: errors.New(\"removed from backend list\"), } updateBackendStatusMetrics(addr, lastHealth.status, StatusCannotConnect) } else if newHealth.status != StatusHealthy { updatedBackends[addr] = newHealth updateBackendStatusMetrics(addr, lastHealth.status, newHealth.status) } } } for addr, newHealth := range bhMap { if newHealth.status == StatusHealthy { lastHealth, ok := bo.curBackendInfo[addr] if !ok { lastHealth = \u0026backendHealth{ status: StatusCannotConnect, } } if lastHealth.status != StatusHealthy { updatedBackends[addr] = newHealth updateBackendStatusMetrics(addr, lastHealth.status, newHealth.status) } else if lastHealth.serverVersion != newHealth.serverVersion { // Not possible here: the backend finishes upgrading between two health checks. updatedBackends[addr] = newHealth } } } // Notify it even when the updatedBackends is empty, in order to clear the last error. bo.eventReceiver.OnBackendChanged(updatedBackends, nil) bo.curBackendInfo = bhMap } 通过上","date":"2023-07-30","objectID":"/tiproxy-yuan-li-he-shi-xian/:4:1","series":null,"tags":null,"title":"TiProxy 原理和实现","uri":"/tiproxy-yuan-li-he-shi-xian/#第一步获取"},{"categories":null,"content":" 1、tiproxy是怎么发现tidb？获取tidb拓扑最核心、简化后的代码如下，其实就是使用etcdCli.Get获取信息 // 从 etcd 获取 tidb 拓扑 路径 /topology/tidb//info /topology/tidb//ttl func (is *InfoSyncer) GetTiDBTopology(ctx context.Context) (map[string]*TiDBInfo, error) { res, err := is.etcdCli.Get(ctx, tidbinfo.TopologyInformationPath, clientv3.WithPrefix()) infos := make(map[string]*TiDBInfo, len(res.Kvs)/2) for _, kv := range res.Kvs { var ttl, addr string var topology *tidbinfo.TopologyInfo key := hack.String(kv.Key) switch { case strings.HasSuffix(key, ttlSuffix): addr = key[len(tidbinfo.TopologyInformationPath)+1 : len(key)-len(ttlSuffix)-1] ttl = hack.String(kv.Value) case strings.HasSuffix(key, infoSuffix): addr = key[len(tidbinfo.TopologyInformationPath)+1 : len(key)-len(infoSuffix)-1] json.Unmarshal(kv.Value, \u0026topology) default: continue } info := infos[addr] if len(ttl) \u003e 0 { info.TTL = hack.String(kv.Value) } else { info.TopologyInfo = topology } } return infos, nil } 这个函数是怎么被tiproxy用起来的呢？ 其实在每个proxy启动时后都会开启一个BackendObserver协程，这个协程会做三件事： func (bo *BackendObserver) observe(ctx context.Context) { for ctx.Err() == nil { // 获取 backendInfo, err := bo.fetcher.GetBackendList(ctx) // 检查 bhMap := bo.checkHealth(ctx, backendInfo) // 通知 bo.notifyIfChanged(bhMap) select { case \u003c-time.After(bo.healthCheckConfig.Interval): // 间隔3秒 case \u003c-bo.refreshChan: case \u003c-ctx.Done(): return } } } 第一步获取：从etcd获取tidb拓扑；代码见上； 第二步检查：判断获取到tidb节点是否可以连通、访问，给每个节点设置StatusHealthy或者StatusCannotConnect状态 func (bo *BackendObserver) checkHealth(ctx context.Context, backends map[string]*BackendInfo) map[string]*backendHealth { curBackendHealth := make(map[string]*backendHealth, len(backends)) for addr, info := range backends { bh := \u0026backendHealth{ status: StatusHealthy, } curBackendHealth[addr] = bh // http 服务检查 if info != nil \u0026\u0026 len(info.IP) \u003e 0 { schema := \"http\" httpCli := *bo.httpCli httpCli.Timeout = bo.healthCheckConfig.DialTimeout url := fmt.Sprintf(\"%s://%s:%d%s\", schema, info.IP, info.StatusPort, statusPathSuffix) resp, err := httpCli.Get(url) if err != nil { bh.status = StatusCannotConnect bh.pingErr = errors.Wrapf(err, \"connect status port failed\") continue } } // tcp 服务检查 conn, err := net.DialTimeout(\"tcp\", addr, bo.healthCheckConfig.DialTimeout) if err != nil { bh.status = StatusCannotConnect bh.pingErr = errors.Wrapf(err, \"connect sql port failed\") } } return curBackendHealth } 第三步通知：将检查后的 backends 列表跟内存中缓存的 backends 进行比较，将变动的 updatedBackends 进行通知 // notifyIfChanged 根据最新的 tidb 拓扑 bhMap 与之前的 tidb 拓扑 bo.curBackendInfo 进行比较 // - 在 bo.curBackendInfo 中但是不在 bhMap 中：说明 tidb 节点失联，需要记录下 // - 在 bo.curBackendInfo 中也在 bhMap 中，但是最新的状态不是 StatusHealthy：也需要记录下 // - 在 bhMap 中但是不在 bo.curBackendInfo 中：说明是新增 tidb 节点，需要记录下 func (bo *BackendObserver) notifyIfChanged(bhMap map[string]*backendHealth) { updatedBackends := make(map[string]*backendHealth) for addr, lastHealth := range bo.curBackendInfo { if lastHealth.status == StatusHealthy { if newHealth, ok := bhMap[addr]; !ok { updatedBackends[addr] = \u0026backendHealth{ status: StatusCannotConnect, pingErr: errors.New(\"removed from backend list\"), } updateBackendStatusMetrics(addr, lastHealth.status, StatusCannotConnect) } else if newHealth.status != StatusHealthy { updatedBackends[addr] = newHealth updateBackendStatusMetrics(addr, lastHealth.status, newHealth.status) } } } for addr, newHealth := range bhMap { if newHealth.status == StatusHealthy { lastHealth, ok := bo.curBackendInfo[addr] if !ok { lastHealth = \u0026backendHealth{ status: StatusCannotConnect, } } if lastHealth.status != StatusHealthy { updatedBackends[addr] = newHealth updateBackendStatusMetrics(addr, lastHealth.status, newHealth.status) } else if lastHealth.serverVersion != newHealth.serverVersion { // Not possible here: the backend finishes upgrading between two health checks. updatedBackends[addr] = newHealth } } } // Notify it even when the updatedBackends is empty, in order to clear the last error. bo.eventReceiver.OnBackendChanged(updatedBackends, nil) bo.curBackendInfo = bhMap } 通过上","date":"2023-07-30","objectID":"/tiproxy-yuan-li-he-shi-xian/:4:1","series":null,"tags":null,"title":"TiProxy 原理和实现","uri":"/tiproxy-yuan-li-he-shi-xian/#第二步检查"},{"categories":null,"content":" 1、tiproxy是怎么发现tidb？获取tidb拓扑最核心、简化后的代码如下，其实就是使用etcdCli.Get获取信息 // 从 etcd 获取 tidb 拓扑 路径 /topology/tidb//info /topology/tidb//ttl func (is *InfoSyncer) GetTiDBTopology(ctx context.Context) (map[string]*TiDBInfo, error) { res, err := is.etcdCli.Get(ctx, tidbinfo.TopologyInformationPath, clientv3.WithPrefix()) infos := make(map[string]*TiDBInfo, len(res.Kvs)/2) for _, kv := range res.Kvs { var ttl, addr string var topology *tidbinfo.TopologyInfo key := hack.String(kv.Key) switch { case strings.HasSuffix(key, ttlSuffix): addr = key[len(tidbinfo.TopologyInformationPath)+1 : len(key)-len(ttlSuffix)-1] ttl = hack.String(kv.Value) case strings.HasSuffix(key, infoSuffix): addr = key[len(tidbinfo.TopologyInformationPath)+1 : len(key)-len(infoSuffix)-1] json.Unmarshal(kv.Value, \u0026topology) default: continue } info := infos[addr] if len(ttl) \u003e 0 { info.TTL = hack.String(kv.Value) } else { info.TopologyInfo = topology } } return infos, nil } 这个函数是怎么被tiproxy用起来的呢？ 其实在每个proxy启动时后都会开启一个BackendObserver协程，这个协程会做三件事： func (bo *BackendObserver) observe(ctx context.Context) { for ctx.Err() == nil { // 获取 backendInfo, err := bo.fetcher.GetBackendList(ctx) // 检查 bhMap := bo.checkHealth(ctx, backendInfo) // 通知 bo.notifyIfChanged(bhMap) select { case \u003c-time.After(bo.healthCheckConfig.Interval): // 间隔3秒 case \u003c-bo.refreshChan: case \u003c-ctx.Done(): return } } } 第一步获取：从etcd获取tidb拓扑；代码见上； 第二步检查：判断获取到tidb节点是否可以连通、访问，给每个节点设置StatusHealthy或者StatusCannotConnect状态 func (bo *BackendObserver) checkHealth(ctx context.Context, backends map[string]*BackendInfo) map[string]*backendHealth { curBackendHealth := make(map[string]*backendHealth, len(backends)) for addr, info := range backends { bh := \u0026backendHealth{ status: StatusHealthy, } curBackendHealth[addr] = bh // http 服务检查 if info != nil \u0026\u0026 len(info.IP) \u003e 0 { schema := \"http\" httpCli := *bo.httpCli httpCli.Timeout = bo.healthCheckConfig.DialTimeout url := fmt.Sprintf(\"%s://%s:%d%s\", schema, info.IP, info.StatusPort, statusPathSuffix) resp, err := httpCli.Get(url) if err != nil { bh.status = StatusCannotConnect bh.pingErr = errors.Wrapf(err, \"connect status port failed\") continue } } // tcp 服务检查 conn, err := net.DialTimeout(\"tcp\", addr, bo.healthCheckConfig.DialTimeout) if err != nil { bh.status = StatusCannotConnect bh.pingErr = errors.Wrapf(err, \"connect sql port failed\") } } return curBackendHealth } 第三步通知：将检查后的 backends 列表跟内存中缓存的 backends 进行比较，将变动的 updatedBackends 进行通知 // notifyIfChanged 根据最新的 tidb 拓扑 bhMap 与之前的 tidb 拓扑 bo.curBackendInfo 进行比较 // - 在 bo.curBackendInfo 中但是不在 bhMap 中：说明 tidb 节点失联，需要记录下 // - 在 bo.curBackendInfo 中也在 bhMap 中，但是最新的状态不是 StatusHealthy：也需要记录下 // - 在 bhMap 中但是不在 bo.curBackendInfo 中：说明是新增 tidb 节点，需要记录下 func (bo *BackendObserver) notifyIfChanged(bhMap map[string]*backendHealth) { updatedBackends := make(map[string]*backendHealth) for addr, lastHealth := range bo.curBackendInfo { if lastHealth.status == StatusHealthy { if newHealth, ok := bhMap[addr]; !ok { updatedBackends[addr] = \u0026backendHealth{ status: StatusCannotConnect, pingErr: errors.New(\"removed from backend list\"), } updateBackendStatusMetrics(addr, lastHealth.status, StatusCannotConnect) } else if newHealth.status != StatusHealthy { updatedBackends[addr] = newHealth updateBackendStatusMetrics(addr, lastHealth.status, newHealth.status) } } } for addr, newHealth := range bhMap { if newHealth.status == StatusHealthy { lastHealth, ok := bo.curBackendInfo[addr] if !ok { lastHealth = \u0026backendHealth{ status: StatusCannotConnect, } } if lastHealth.status != StatusHealthy { updatedBackends[addr] = newHealth updateBackendStatusMetrics(addr, lastHealth.status, newHealth.status) } else if lastHealth.serverVersion != newHealth.serverVersion { // Not possible here: the backend finishes upgrading between two health checks. updatedBackends[addr] = newHealth } } } // Notify it even when the updatedBackends is empty, in order to clear the last error. bo.eventReceiver.OnBackendChanged(updatedBackends, nil) bo.curBackendInfo = bhMap } 通过上","date":"2023-07-30","objectID":"/tiproxy-yuan-li-he-shi-xian/:4:1","series":null,"tags":null,"title":"TiProxy 原理和实现","uri":"/tiproxy-yuan-li-he-shi-xian/#第三步通知"},{"categories":null,"content":" 2、tiproxy是在tidb节点间自动负载均衡的逻辑？此处自动负载的语义是：将哪个 backend 的哪个 connect 迁移到哪个 backend 上。这就要解决 backend 挑选和 connect 挑选问题。 这个问题的解决办法是在 ScoreBasedRouter 模块完成。这个模块有3个 func 和上述解释相关： type ScoreBasedRouter struct { sync.Mutex // A list of *backendWrapper. The backends are in descending order of scores. backends *glist.List[*backendWrapper] // ... } // 被 BackendObserver 调用，传来的 backends 会合并到 ScoreBasedRouter::backends 中 func (router *ScoreBasedRouter) OnBackendChanged(backends map[string]*backendHealth, err error) {} // 通过比较 backend 分数方式调整 ScoreBasedRouter::backends 中的位置 func (router *ScoreBasedRouter) adjustBackendList(be *glist.Element[*backendWrapper]) {} // 协程方式运行，做负载均衡处理 func (router *ScoreBasedRouter) rebalanceLoop(ctx context.Context) {} OnBackendChanged 是暴露给 BackendObserver 模块的一个接口， 用来同步从 etcd 发现的 tidb 信息，这个逻辑不复杂，详细可自行阅读源码。这个方法是问题一种提到的“通知”接收处。 adjustBackendList 本质就是调整 item 在双向链表中的位置，这个也不复杂。 下面重点说下 rebalanceLoop 的逻辑，这里涉及到\"将哪个 backend 的哪个 connect 迁移到哪个 backend 上\"的问题。 // rebalanceLoop 计算间隔是 10 ms，每次最多处理 10 个连接(防止后端出现抖动) // - backends 的变化是通过 OnBackendChanged 修改的，连接平衡是 rebalanceLoop 函数做的，两者为了保证并发使用了 sync.Mutex func (router *ScoreBasedRouter) rebalanceLoop(ctx context.Context) { for { router.rebalance(rebalanceConnsPerLoop) select { case \u003c-ctx.Done(): return case \u003c-time.After(rebalanceInterval): } } } // rebalance func (router *ScoreBasedRouter) rebalance(maxNum int) { curTime := time.Now() router.Lock() defer router.Unlock() for i := 0; i \u003c maxNum; i++ { var busiestEle *glist.Element[*backendWrapper] for be := router.backends.Front(); be != nil; be = be.Next() { backend := be.Value if backend.connList.Len() \u003e 0 { busiestEle = be break } } if busiestEle == nil { break } busiestBackend := busiestEle.Value idlestEle := router.backends.Back() idlestBackend := idlestEle.Value if float64(busiestBackend.score())/float64(idlestBackend.score()+1) \u003c rebalanceMaxScoreRatio { break } var ce *glist.Element[*connWrapper] for ele := busiestBackend.connList.Front(); ele != nil; ele = ele.Next() { conn := ele.Value switch conn.phase { case phaseRedirectNotify: continue case phaseRedirectFail: if conn.lastRedirect.Add(redirectFailMinInterval).After(curTime) { continue } } ce = ele break } if ce == nil { break } conn := ce.Value busiestBackend.connScore-- router.adjustBackendList(busiestEle) idlestBackend.connScore++ router.adjustBackendList(idlestEle) conn.phase = phaseRedirectNotify conn.lastRedirect = curTime conn.Redirect(idlestBackend.addr) } } rebalance 的逻辑 从前往后访问 backends list，找到 busiestBackend 在 backends list 最后找到 idlestBackend 比较两者 score， 如果差距在 20% 以内就不用处理了 否则在 busiestBackend 中取出一个 conn 给 idlestBackend 取出的逻辑很简单，就是从前到后遍历当前 backend 的 connList 因为session迁移要保证事务完成，所以迁移不是立刻执行的，这就得加个 phase 来跟进 处于 phaseRedirectNotify 阶段的不要再取出； 处于 phaseRedirectFail 但还没到超时时间的，也不要取出； 其他状态的 conn 可以被取出 因为有 conn 变动所以要调整下 busiestBackend 和 idlestBackend 在 backends list 中的位置 最后通过 channel 通知 BackendConnManager 做去session迁移，此时 conn 状态是 phaseRedirectNotify 给每个backend的打分逻辑如下，分数越大说明负载越大 func (b *backendWrapper) score() int { return b.status.ToScore() + b.connScore } // var statusScores = map[BackendStatus]int{ // StatusHealthy: 0, // StatusCannotConnect: 10000000, // StatusMemoryHigh: 5000, // StatusRunSlow: 5000, // StatusSchemaOutdated: 10000000, // } // connScore = connList.Len() + incoming connections - outgoing connections. ","date":"2023-07-30","objectID":"/tiproxy-yuan-li-he-shi-xian/:4:2","series":null,"tags":null,"title":"TiProxy 原理和实现","uri":"/tiproxy-yuan-li-he-shi-xian/#2tiproxy是在tidb节点间自动负载均衡的逻辑"},{"categories":null,"content":" 2、tiproxy是在tidb节点间自动负载均衡的逻辑？此处自动负载的语义是：将哪个 backend 的哪个 connect 迁移到哪个 backend 上。这就要解决 backend 挑选和 connect 挑选问题。 这个问题的解决办法是在 ScoreBasedRouter 模块完成。这个模块有3个 func 和上述解释相关： type ScoreBasedRouter struct { sync.Mutex // A list of *backendWrapper. The backends are in descending order of scores. backends *glist.List[*backendWrapper] // ... } // 被 BackendObserver 调用，传来的 backends 会合并到 ScoreBasedRouter::backends 中 func (router *ScoreBasedRouter) OnBackendChanged(backends map[string]*backendHealth, err error) {} // 通过比较 backend 分数方式调整 ScoreBasedRouter::backends 中的位置 func (router *ScoreBasedRouter) adjustBackendList(be *glist.Element[*backendWrapper]) {} // 协程方式运行，做负载均衡处理 func (router *ScoreBasedRouter) rebalanceLoop(ctx context.Context) {} OnBackendChanged 是暴露给 BackendObserver 模块的一个接口， 用来同步从 etcd 发现的 tidb 信息，这个逻辑不复杂，详细可自行阅读源码。这个方法是问题一种提到的“通知”接收处。 adjustBackendList 本质就是调整 item 在双向链表中的位置，这个也不复杂。 下面重点说下 rebalanceLoop 的逻辑，这里涉及到\"将哪个 backend 的哪个 connect 迁移到哪个 backend 上\"的问题。 // rebalanceLoop 计算间隔是 10 ms，每次最多处理 10 个连接(防止后端出现抖动) // - backends 的变化是通过 OnBackendChanged 修改的，连接平衡是 rebalanceLoop 函数做的，两者为了保证并发使用了 sync.Mutex func (router *ScoreBasedRouter) rebalanceLoop(ctx context.Context) { for { router.rebalance(rebalanceConnsPerLoop) select { case \u003c-ctx.Done(): return case \u003c-time.After(rebalanceInterval): } } } // rebalance func (router *ScoreBasedRouter) rebalance(maxNum int) { curTime := time.Now() router.Lock() defer router.Unlock() for i := 0; i \u003c maxNum; i++ { var busiestEle *glist.Element[*backendWrapper] for be := router.backends.Front(); be != nil; be = be.Next() { backend := be.Value if backend.connList.Len() \u003e 0 { busiestEle = be break } } if busiestEle == nil { break } busiestBackend := busiestEle.Value idlestEle := router.backends.Back() idlestBackend := idlestEle.Value if float64(busiestBackend.score())/float64(idlestBackend.score()+1) \u003c rebalanceMaxScoreRatio { break } var ce *glist.Element[*connWrapper] for ele := busiestBackend.connList.Front(); ele != nil; ele = ele.Next() { conn := ele.Value switch conn.phase { case phaseRedirectNotify: continue case phaseRedirectFail: if conn.lastRedirect.Add(redirectFailMinInterval).After(curTime) { continue } } ce = ele break } if ce == nil { break } conn := ce.Value busiestBackend.connScore-- router.adjustBackendList(busiestEle) idlestBackend.connScore++ router.adjustBackendList(idlestEle) conn.phase = phaseRedirectNotify conn.lastRedirect = curTime conn.Redirect(idlestBackend.addr) } } rebalance 的逻辑 从前往后访问 backends list，找到 busiestBackend 在 backends list 最后找到 idlestBackend 比较两者 score， 如果差距在 20% 以内就不用处理了 否则在 busiestBackend 中取出一个 conn 给 idlestBackend 取出的逻辑很简单，就是从前到后遍历当前 backend 的 connList 因为session迁移要保证事务完成，所以迁移不是立刻执行的，这就得加个 phase 来跟进 处于 phaseRedirectNotify 阶段的不要再取出； 处于 phaseRedirectFail 但还没到超时时间的，也不要取出； 其他状态的 conn 可以被取出 因为有 conn 变动所以要调整下 busiestBackend 和 idlestBackend 在 backends list 中的位置 最后通过 channel 通知 BackendConnManager 做去session迁移，此时 conn 状态是 phaseRedirectNotify 给每个backend的打分逻辑如下，分数越大说明负载越大 func (b *backendWrapper) score() int { return b.status.ToScore() + b.connScore } // var statusScores = map[BackendStatus]int{ // StatusHealthy: 0, // StatusCannotConnect: 10000000, // StatusMemoryHigh: 5000, // StatusRunSlow: 5000, // StatusSchemaOutdated: 10000000, // } // connScore = connList.Len() + incoming connections - outgoing connections. ","date":"2023-07-30","objectID":"/tiproxy-yuan-li-he-shi-xian/:4:2","series":null,"tags":null,"title":"TiProxy 原理和实现","uri":"/tiproxy-yuan-li-he-shi-xian/#rebalance-的逻辑"},{"categories":null,"content":" 3、在自动负载均衡时tiproxy是怎么做到优雅的session迁移、session上下文恢复？这个问题可以继续细分： 迁移消息接收 ScoreBasedRouter 模块计算出哪个 conn 从哪个 backend 迁移到哪个 backend 后，怎么通知给对应的 conn ？ 迁移任务执行 conn 接收到消息后要进行session迁移，那么如何解决迁移期间 client 可能存在访问的问题 ？ 因为tiproxy没有保存密码，那么基于session token的验证方式是怎么实现的？ 新的tidb节点登录成功后，session上下问题信息是怎么恢复的？ 以上的问题都可以在 BackendConnManager 模块找到答案： type BackendConnManager struct { // processLock makes redirecting and command processing exclusive. processLock sync.Mutex clientIO *pnet.PacketIO backendIO atomic.Pointer[pnet.PacketIO] authenticator *Authenticator } func (mgr *BackendConnManager) Redirect(newAddr string) bool {} func (mgr *BackendConnManager) processSignals(ctx context.Context) {} func (mgr *BackendConnManager) tryRedirect(ctx context.Context) {} func (mgr *BackendConnManager) querySessionStates(backendIO *pnet.PacketIO) (sessionStates, sessionToken string, err error) {} func (mgr *BackendConnManager) ExecuteCmd(ctx context.Context, request []byte) (err error) {} 迁移消息接收在前文的 rebalance 方法最后，有行这样的逻辑 conn.Redirect(idlestBackend.addr) 这就是 ScoreBasedRouter 的通知给对应 conn 的地方。 这里调用的是 BackendConnManager::Redirect， 具体执行逻辑 将目标 backend 存储到 redirectInfo 给 signalReceived channel 发 signalTypeRedirect 消息 func (mgr *BackendConnManager) Redirect(newAddr string) bool { // NOTE: BackendConnManager may be closing concurrently because of no lock. switch mgr.closeStatus.Load() { case statusNotifyClose, statusClosing, statusClosed: return false } mgr.redirectInfo.Store(\u0026signalRedirect{newAddr: newAddr}) // Generally, it won't wait because the caller won't send another signal before the previous one finishes. mgr.signalReceived \u003c- signalTypeRedirect return true } 该消息被 BackendConnManager::processSignals 协程接收 func (mgr *BackendConnManager) processSignals(ctx context.Context) { for { select { case s := \u003c-mgr.signalReceived: // Redirect the session immediately just in case the session is finishedTxn. mgr.processLock.Lock() switch s { case signalTypeGracefulClose: mgr.tryGracefulClose(ctx) case signalTypeRedirect: // \u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c mgr.tryRedirect(ctx) } mgr.processLock.Unlock() case rs := \u003c-mgr.redirectResCh: mgr.notifyRedirectResult(ctx, rs) case \u003c-mgr.checkBackendTicker.C: mgr.checkBackendActive() case \u003c-ctx.Done(): return } } } 这里补充下 processSignals 是怎么来的。正常情况下，client每发起一个连接，proxy就会起两个协程： 连接、转发 tcp 消息协程： 连接：SQLServer::Run 方法启动，也就是每连接每协程的意思。 转发：ClientConnection 模块调用 BackendConnManager::ExecuteCmd 实现消息转发 监听和执行 redirect 任务协程： BackendConnManager 模块启动 processSignals 协程处理 所以上文监听 signalTypeRedirect 消息的 processSignals 协程，在连接建立时就启动了，当收到消息后执行 tryRedirect 方法尝试执行迁移。 迁移任务执行tryRedirect 处理逻辑比较复杂，我们选取核心流程进行简述： func (mgr *BackendConnManager) tryRedirect(ctx context.Context) { // 获取目标 backend signal := mgr.redirectInfo.Load() // 处于事务中，先不做迁移 if !mgr.cmdProcessor.finishedTxn() { return } // 组装执行结果 rs := \u0026redirectResult{ from: mgr.ServerAddr(), to: signal.newAddr, } defer func() { // 不论执行成功与否都清空 redirectInfo， 并将 rs 结果发到 redirectResCh， redirectResCh 的处理逻辑还是在 processSignals 中处理 mgr.redirectInfo.Store(nil) mgr.redirectResCh \u003c- rs }() // 从源 backend 获取 sessionStates, sessionToken backendIO := mgr.backendIO.Load() sessionStates, sessionToken, rs.err := mgr.querySessionStates(backendIO) // 跟目标 backend 建立tcp连接 cn, rs.err := net.DialTimeout(\"tcp\", rs.to, DialTimeout) // 将 conn 包裹为 PacketIO newBackendIO := pnet.NewPacketIO(cn, mgr.logger, pnet.WithRemoteAddr(rs.to, cn.RemoteAddr()), pnet.WithWrapError(ErrBackendConn)) // 使用 session token方式跟目标 backend 进行鉴握手鉴权 mgr.authenticator.handshakeSecondTime(mgr.logger, mgr.clientIO, newBackendIO, mgr.backendTLS, sessionToken) // 登录目标 backend 进行鉴权 rs.err = mgr.initSessionStates(newBackendIO, sessionStates) // 将新的 PacketIO 存储到 BackendConnManager 的成员变量中，后续再有请求都是用此变量 mgr.backendIO.Store(newBackendIO) } 上面展示了 session token 的认证方式和上下文恢复的逻辑，对应 querySessionStates 、handshakeSecondTime 、initSessionStates 三个方法 querySessionStates: tiproxy 在 tidb a 上执行 SHOW SESSION_STATES 获取到 session_token session_state handshakeSecondTime: tiproxy 使用 session_token 认证方式登录到 tidb b initSessio","date":"2023-07-30","objectID":"/tiproxy-yuan-li-he-shi-xian/:4:3","series":null,"tags":null,"title":"TiProxy 原理和实现","uri":"/tiproxy-yuan-li-he-shi-xian/#3在自动负载均衡时tiproxy是怎么做到优雅的session迁移session上下文恢复"},{"categories":null,"content":" 3、在自动负载均衡时tiproxy是怎么做到优雅的session迁移、session上下文恢复？这个问题可以继续细分： 迁移消息接收 ScoreBasedRouter 模块计算出哪个 conn 从哪个 backend 迁移到哪个 backend 后，怎么通知给对应的 conn ？ 迁移任务执行 conn 接收到消息后要进行session迁移，那么如何解决迁移期间 client 可能存在访问的问题 ？ 因为tiproxy没有保存密码，那么基于session token的验证方式是怎么实现的？ 新的tidb节点登录成功后，session上下问题信息是怎么恢复的？ 以上的问题都可以在 BackendConnManager 模块找到答案： type BackendConnManager struct { // processLock makes redirecting and command processing exclusive. processLock sync.Mutex clientIO *pnet.PacketIO backendIO atomic.Pointer[pnet.PacketIO] authenticator *Authenticator } func (mgr *BackendConnManager) Redirect(newAddr string) bool {} func (mgr *BackendConnManager) processSignals(ctx context.Context) {} func (mgr *BackendConnManager) tryRedirect(ctx context.Context) {} func (mgr *BackendConnManager) querySessionStates(backendIO *pnet.PacketIO) (sessionStates, sessionToken string, err error) {} func (mgr *BackendConnManager) ExecuteCmd(ctx context.Context, request []byte) (err error) {} 迁移消息接收在前文的 rebalance 方法最后，有行这样的逻辑 conn.Redirect(idlestBackend.addr) 这就是 ScoreBasedRouter 的通知给对应 conn 的地方。 这里调用的是 BackendConnManager::Redirect， 具体执行逻辑 将目标 backend 存储到 redirectInfo 给 signalReceived channel 发 signalTypeRedirect 消息 func (mgr *BackendConnManager) Redirect(newAddr string) bool { // NOTE: BackendConnManager may be closing concurrently because of no lock. switch mgr.closeStatus.Load() { case statusNotifyClose, statusClosing, statusClosed: return false } mgr.redirectInfo.Store(\u0026signalRedirect{newAddr: newAddr}) // Generally, it won't wait because the caller won't send another signal before the previous one finishes. mgr.signalReceived \u003c- signalTypeRedirect return true } 该消息被 BackendConnManager::processSignals 协程接收 func (mgr *BackendConnManager) processSignals(ctx context.Context) { for { select { case s := \u003c-mgr.signalReceived: // Redirect the session immediately just in case the session is finishedTxn. mgr.processLock.Lock() switch s { case signalTypeGracefulClose: mgr.tryGracefulClose(ctx) case signalTypeRedirect: // \u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c mgr.tryRedirect(ctx) } mgr.processLock.Unlock() case rs := \u003c-mgr.redirectResCh: mgr.notifyRedirectResult(ctx, rs) case \u003c-mgr.checkBackendTicker.C: mgr.checkBackendActive() case \u003c-ctx.Done(): return } } } 这里补充下 processSignals 是怎么来的。正常情况下，client每发起一个连接，proxy就会起两个协程： 连接、转发 tcp 消息协程： 连接：SQLServer::Run 方法启动，也就是每连接每协程的意思。 转发：ClientConnection 模块调用 BackendConnManager::ExecuteCmd 实现消息转发 监听和执行 redirect 任务协程： BackendConnManager 模块启动 processSignals 协程处理 所以上文监听 signalTypeRedirect 消息的 processSignals 协程，在连接建立时就启动了，当收到消息后执行 tryRedirect 方法尝试执行迁移。 迁移任务执行tryRedirect 处理逻辑比较复杂，我们选取核心流程进行简述： func (mgr *BackendConnManager) tryRedirect(ctx context.Context) { // 获取目标 backend signal := mgr.redirectInfo.Load() // 处于事务中，先不做迁移 if !mgr.cmdProcessor.finishedTxn() { return } // 组装执行结果 rs := \u0026redirectResult{ from: mgr.ServerAddr(), to: signal.newAddr, } defer func() { // 不论执行成功与否都清空 redirectInfo， 并将 rs 结果发到 redirectResCh， redirectResCh 的处理逻辑还是在 processSignals 中处理 mgr.redirectInfo.Store(nil) mgr.redirectResCh \u003c- rs }() // 从源 backend 获取 sessionStates, sessionToken backendIO := mgr.backendIO.Load() sessionStates, sessionToken, rs.err := mgr.querySessionStates(backendIO) // 跟目标 backend 建立tcp连接 cn, rs.err := net.DialTimeout(\"tcp\", rs.to, DialTimeout) // 将 conn 包裹为 PacketIO newBackendIO := pnet.NewPacketIO(cn, mgr.logger, pnet.WithRemoteAddr(rs.to, cn.RemoteAddr()), pnet.WithWrapError(ErrBackendConn)) // 使用 session token方式跟目标 backend 进行鉴握手鉴权 mgr.authenticator.handshakeSecondTime(mgr.logger, mgr.clientIO, newBackendIO, mgr.backendTLS, sessionToken) // 登录目标 backend 进行鉴权 rs.err = mgr.initSessionStates(newBackendIO, sessionStates) // 将新的 PacketIO 存储到 BackendConnManager 的成员变量中，后续再有请求都是用此变量 mgr.backendIO.Store(newBackendIO) } 上面展示了 session token 的认证方式和上下文恢复的逻辑，对应 querySessionStates 、handshakeSecondTime 、initSessionStates 三个方法 querySessionStates: tiproxy 在 tidb a 上执行 SHOW SESSION_STATES 获取到 session_token session_state handshakeSecondTime: tiproxy 使用 session_token 认证方式登录到 tidb b initSessio","date":"2023-07-30","objectID":"/tiproxy-yuan-li-he-shi-xian/:4:3","series":null,"tags":null,"title":"TiProxy 原理和实现","uri":"/tiproxy-yuan-li-he-shi-xian/#迁移消息接收"},{"categories":null,"content":" 3、在自动负载均衡时tiproxy是怎么做到优雅的session迁移、session上下文恢复？这个问题可以继续细分： 迁移消息接收 ScoreBasedRouter 模块计算出哪个 conn 从哪个 backend 迁移到哪个 backend 后，怎么通知给对应的 conn ？ 迁移任务执行 conn 接收到消息后要进行session迁移，那么如何解决迁移期间 client 可能存在访问的问题 ？ 因为tiproxy没有保存密码，那么基于session token的验证方式是怎么实现的？ 新的tidb节点登录成功后，session上下问题信息是怎么恢复的？ 以上的问题都可以在 BackendConnManager 模块找到答案： type BackendConnManager struct { // processLock makes redirecting and command processing exclusive. processLock sync.Mutex clientIO *pnet.PacketIO backendIO atomic.Pointer[pnet.PacketIO] authenticator *Authenticator } func (mgr *BackendConnManager) Redirect(newAddr string) bool {} func (mgr *BackendConnManager) processSignals(ctx context.Context) {} func (mgr *BackendConnManager) tryRedirect(ctx context.Context) {} func (mgr *BackendConnManager) querySessionStates(backendIO *pnet.PacketIO) (sessionStates, sessionToken string, err error) {} func (mgr *BackendConnManager) ExecuteCmd(ctx context.Context, request []byte) (err error) {} 迁移消息接收在前文的 rebalance 方法最后，有行这样的逻辑 conn.Redirect(idlestBackend.addr) 这就是 ScoreBasedRouter 的通知给对应 conn 的地方。 这里调用的是 BackendConnManager::Redirect， 具体执行逻辑 将目标 backend 存储到 redirectInfo 给 signalReceived channel 发 signalTypeRedirect 消息 func (mgr *BackendConnManager) Redirect(newAddr string) bool { // NOTE: BackendConnManager may be closing concurrently because of no lock. switch mgr.closeStatus.Load() { case statusNotifyClose, statusClosing, statusClosed: return false } mgr.redirectInfo.Store(\u0026signalRedirect{newAddr: newAddr}) // Generally, it won't wait because the caller won't send another signal before the previous one finishes. mgr.signalReceived \u003c- signalTypeRedirect return true } 该消息被 BackendConnManager::processSignals 协程接收 func (mgr *BackendConnManager) processSignals(ctx context.Context) { for { select { case s := \u003c-mgr.signalReceived: // Redirect the session immediately just in case the session is finishedTxn. mgr.processLock.Lock() switch s { case signalTypeGracefulClose: mgr.tryGracefulClose(ctx) case signalTypeRedirect: // \u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c mgr.tryRedirect(ctx) } mgr.processLock.Unlock() case rs := \u003c-mgr.redirectResCh: mgr.notifyRedirectResult(ctx, rs) case \u003c-mgr.checkBackendTicker.C: mgr.checkBackendActive() case \u003c-ctx.Done(): return } } } 这里补充下 processSignals 是怎么来的。正常情况下，client每发起一个连接，proxy就会起两个协程： 连接、转发 tcp 消息协程： 连接：SQLServer::Run 方法启动，也就是每连接每协程的意思。 转发：ClientConnection 模块调用 BackendConnManager::ExecuteCmd 实现消息转发 监听和执行 redirect 任务协程： BackendConnManager 模块启动 processSignals 协程处理 所以上文监听 signalTypeRedirect 消息的 processSignals 协程，在连接建立时就启动了，当收到消息后执行 tryRedirect 方法尝试执行迁移。 迁移任务执行tryRedirect 处理逻辑比较复杂，我们选取核心流程进行简述： func (mgr *BackendConnManager) tryRedirect(ctx context.Context) { // 获取目标 backend signal := mgr.redirectInfo.Load() // 处于事务中，先不做迁移 if !mgr.cmdProcessor.finishedTxn() { return } // 组装执行结果 rs := \u0026redirectResult{ from: mgr.ServerAddr(), to: signal.newAddr, } defer func() { // 不论执行成功与否都清空 redirectInfo， 并将 rs 结果发到 redirectResCh， redirectResCh 的处理逻辑还是在 processSignals 中处理 mgr.redirectInfo.Store(nil) mgr.redirectResCh \u003c- rs }() // 从源 backend 获取 sessionStates, sessionToken backendIO := mgr.backendIO.Load() sessionStates, sessionToken, rs.err := mgr.querySessionStates(backendIO) // 跟目标 backend 建立tcp连接 cn, rs.err := net.DialTimeout(\"tcp\", rs.to, DialTimeout) // 将 conn 包裹为 PacketIO newBackendIO := pnet.NewPacketIO(cn, mgr.logger, pnet.WithRemoteAddr(rs.to, cn.RemoteAddr()), pnet.WithWrapError(ErrBackendConn)) // 使用 session token方式跟目标 backend 进行鉴握手鉴权 mgr.authenticator.handshakeSecondTime(mgr.logger, mgr.clientIO, newBackendIO, mgr.backendTLS, sessionToken) // 登录目标 backend 进行鉴权 rs.err = mgr.initSessionStates(newBackendIO, sessionStates) // 将新的 PacketIO 存储到 BackendConnManager 的成员变量中，后续再有请求都是用此变量 mgr.backendIO.Store(newBackendIO) } 上面展示了 session token 的认证方式和上下文恢复的逻辑，对应 querySessionStates 、handshakeSecondTime 、initSessionStates 三个方法 querySessionStates: tiproxy 在 tidb a 上执行 SHOW SESSION_STATES 获取到 session_token session_state handshakeSecondTime: tiproxy 使用 session_token 认证方式登录到 tidb b initSessio","date":"2023-07-30","objectID":"/tiproxy-yuan-li-he-shi-xian/:4:3","series":null,"tags":null,"title":"TiProxy 原理和实现","uri":"/tiproxy-yuan-li-he-shi-xian/#迁移任务执行"},{"categories":null,"content":" 4、tiproxy在自动负载均衡期间遇到处于未提交事务的session是怎么等待结束的？对于 tryRedirect 方法有两个地方被调用，即前文提到的 BackendConnManager::processSignals 和 BackendConnManager::ExecuteCmd BackendConnManager::processSignals 只有在收到channe消息后立即出发一次，如果有未完成的事务就不再执行了。 所以为了保证迁移任务可继续，在 BackendConnManager::ExecuteCmd 中每次执行完 executeCmd 后尝试迁移，这样就能保证事务结束后立刻迁移。 func (mgr *BackendConnManager) ExecuteCmd(ctx context.Context, request []byte) (err error) { // ... waitingRedirect := mgr.redirectInfo.Load() != nil // ... holdRequest, err = mgr.cmdProcessor.executeCmd(request, mgr.clientIO, mgr.backendIO.Load(), waitingRedirect) // ... if mgr.cmdProcessor.finishedTxn() { if waitingRedirect { mgr.tryRedirect(ctx) } // ... } // ... } 判断事务是否结束的 finishedTxn 方法逻辑：解析 client 的请求类型、解析 backend 的响应状态综合判断事务是否完成，此逻辑过于硬核，等以后研究明白后再分享吧。 有兴趣的读者可以分析下这段逻辑： func (cp *CmdProcessor) finishedTxn() bool { if cp.serverStatus\u0026(StatusInTrans|StatusQuit) \u003e 0 { return false } // If any result of the prepared statements is not fetched, we should wait. return !cp.hasPendingPreparedStmts() } func (cp *CmdProcessor) updatePrepStmtStatus(request []byte, serverStatus uint16) { var ( stmtID int prepStmtStatus uint32 ) cmd := pnet.Command(request[0]) switch cmd { case pnet.ComStmtSendLongData, pnet.ComStmtExecute, pnet.ComStmtFetch, pnet.ComStmtReset, pnet.ComStmtClose: stmtID = int(binary.LittleEndian.Uint32(request[1:5])) case pnet.ComResetConnection, pnet.ComChangeUser: cp.preparedStmtStatus = make(map[int]uint32) return default: return } switch cmd { case pnet.ComStmtSendLongData: prepStmtStatus = StatusPrepareWaitExecute case pnet.ComStmtExecute: if serverStatus\u0026mysql.ServerStatusCursorExists \u003e 0 { prepStmtStatus = StatusPrepareWaitFetch } case pnet.ComStmtFetch: if serverStatus\u0026mysql.ServerStatusLastRowSend == 0 { prepStmtStatus = StatusPrepareWaitFetch } } if prepStmtStatus \u003e 0 { cp.preparedStmtStatus[stmtID] = prepStmtStatus } else { delete(cp.preparedStmtStatus, stmtID) } } ","date":"2023-07-30","objectID":"/tiproxy-yuan-li-he-shi-xian/:4:4","series":null,"tags":null,"title":"TiProxy 原理和实现","uri":"/tiproxy-yuan-li-he-shi-xian/#4tiproxy在自动负载均衡期间遇到处于未提交事务的session是怎么等待结束的"},{"categories":null,"content":" 总结本文从4个疑惑入手，阅读了下tiproxy的代码实现，都找到了对应的处理逻辑。 对比于tidb、tikv、pd等组件代码，tiproxy实简单很多，推荐大家学习下。 ","date":"2023-07-30","objectID":"/tiproxy-yuan-li-he-shi-xian/:5:0","series":null,"tags":null,"title":"TiProxy 原理和实现","uri":"/tiproxy-yuan-li-he-shi-xian/#总结"},{"categories":null,"content":" 彩蛋在梳理上面4个问题的时，理清思路后，绘制了如下的内部交互图，有兴趣的可以自己研究下，下篇文章我们将对其进行说明。 ","date":"2023-07-30","objectID":"/tiproxy-yuan-li-he-shi-xian/:6:0","series":null,"tags":null,"title":"TiProxy 原理和实现","uri":"/tiproxy-yuan-li-he-shi-xian/#彩蛋"},{"categories":["tidb"],"content":" 说明最近发现 tidb 有个 https://github.com/pingcap/TiProxy 仓库，抱着好奇的心态想试试这个组件的使用效果。于是按照文档的介绍在本地环境使用tiup做了一些实验，现在将实验过程和实验结果分享给大家。 ","date":"2023-07-29","objectID":"/try-tiproxy/:1:0","series":null,"tags":["tidb","database"],"title":"TiProxy 尝鲜","uri":"/try-tiproxy/#说明"},{"categories":["tidb"],"content":" TiProxy介绍官方README介绍的已经很清楚了，最重要的特性是在TiDB升级、重启、扩缩节点时候可以保证连接不断。牛！ TiProxy is a database proxy that is based on TiDB. It keeps client connections alive while the TiDB server upgrades, restarts, scales in, and scales out. 此外还有一些特性 连接管理： 当tidb节点重启或者关机后，在这个节点上建立的连接会迁移到其他实例上，这个动作对client是透明的，client无感知 负载均衡： 新建连接会对后端tidb-server进行打分，然后进行多个tidb实例间的均衡 服务发现： TiProxy 通过跟pd交互获取最新的tidb实例信息，当有新的tidb启动时，proxy会自动发现并迁移连接至此。 ","date":"2023-07-29","objectID":"/try-tiproxy/:2:0","series":null,"tags":["tidb","database"],"title":"TiProxy 尝鲜","uri":"/try-tiproxy/#tiproxy介绍"},{"categories":["tidb"],"content":" 实验说明使用tiup搭建下测试环境，启动1个pd、1个tikv、1个tidb-server、1个tiproxy，通过tiproxy连接数据库，测试case如下： 启动两个终端连接数据库，然后加1个tidb-server节点，看看client无感的负责均衡效果 上一步完成后，我们有了2个tidb-server，那么缩掉一个，看看proxy是怎么做到会话迁移的 ","date":"2023-07-29","objectID":"/try-tiproxy/:3:0","series":null,"tags":["tidb","database"],"title":"TiProxy 尝鲜","uri":"/try-tiproxy/#实验说明"},{"categories":["tidb"],"content":" 启动集群查阅资料 发现TiProxy仅支持v6.4.0及以后版本，所以使用tiup启动这个版本的集群。 tidb 和 tiproxy 使用 toekn 认证方式，所以生成一个证书 openssl req -x509 -newkey rsa:4096 -sha256 -days 3650 -nodes -keyout key.pem -out cert.pem -subj \"/CN=example.com\" 准备配置文件 tidb.toml 和 tiproxy.yaml $ cat tidb.toml graceful-wait-before-shutdown=10 [security] auto-tls=true session-token-signing-cert='/tmp/tiup/tiproxy/cert.pem' session-token-signing-key='/tmp/tiup/tiproxy/key.pem' $ cat tiproxy.yaml [proxy] require-backend-tls = false 启动tidb tiup playground v6.4.0 --db 1 --kv 1 --pd 1 --tiflash 0 --without-monitor --db.config tidb.toml 启动tiproxy tiup tiproxy:nightly --config tiproxy.yaml ","date":"2023-07-29","objectID":"/try-tiproxy/:4:0","series":null,"tags":["tidb","database"],"title":"TiProxy 尝鲜","uri":"/try-tiproxy/#启动集群"},{"categories":["tidb"],"content":" 实验","date":"2023-07-29","objectID":"/try-tiproxy/:5:0","series":null,"tags":["tidb","database"],"title":"TiProxy 尝鲜","uri":"/try-tiproxy/#实验"},{"categories":["tidb"],"content":" 1、加节点自动负载均衡集群启动后，使用两个终端连接proxy，然后执行show processlist可以看到对方的会话，说明连接到了一个tidb节点上 执行tiup添加一个tidb-server节点 tiup playground scale-out --db 1 然后分别执行show processlit查询，发现每个终端看不到对方的会话了，说明各自连接到了一个tidb实例。 仔细查看发现其中一个连接的信息从127.0.0.1:53240变成了127.0.0.1:54328，也确实说明发生了重连接。 这里补充个说明：因为我测试的时候没有开启proxy协议，所以show processlist看到的host不是client真实的信息，是proxy和tidb建立连接的信息，tidb把proxy当成client出来了。 测试结果很好，负载均衡client无感。 ","date":"2023-07-29","objectID":"/try-tiproxy/:5:1","series":null,"tags":["tidb","database"],"title":"TiProxy 尝鲜","uri":"/try-tiproxy/#1加节点自动负载均衡"},{"categories":["tidb"],"content":" 2、缩节点会话自动迁移在这个基础上，执行tiup缩掉一个tidb-server tiup playground scale-in --pid 91609 然后执行show processlist，可以看到对方的会话，说明又连接到了同一个tidb节点上。 执行sql的时候没有报错，client无感知。 ","date":"2023-07-29","objectID":"/try-tiproxy/:5:2","series":null,"tags":["tidb","database"],"title":"TiProxy 尝鲜","uri":"/try-tiproxy/#2缩节点会话自动迁移"},{"categories":["tidb"],"content":" 加餐实验至此一切都丝般顺滑、符合预期。但是测试的场景未免有些简单。下面做个带有事务的case： 使用tiup搭建下测试环境，启动1个pd、1个tikv、1个tidb-server、1个tiproxy，通过tiproxy连接数据库，打开两个终端并显示执行一个begin，然后分别执行个写入操作，之后再添加1个tidb-server，看看会话是否会被迁移。 这说明在执行中的事务不会做迁移。在设计文档 中也的确有这样的描述 Transactions are hard to be restored, so Session Manager doesn’t support restoring a transaction. Session Manager must wait until the current transaction finishes or the TiDB instance exits due to shut down timeout. 符合预期。 ","date":"2023-07-29","objectID":"/try-tiproxy/:6:0","series":null,"tags":["tidb","database"],"title":"TiProxy 尝鲜","uri":"/try-tiproxy/#加餐"},{"categories":["tidb"],"content":" 总结本次基于v6.4.0版本做了3个简单的实验，对于tidb节点扩缩有会话自动迁移的能力的确很丝滑。 整个过程cleint无报错、无感知。被迁移的会话如果有未提的事务，则会等到事务结束后再迁移。 赞👍🏻 ","date":"2023-07-29","objectID":"/try-tiproxy/:7:0","series":null,"tags":["tidb","database"],"title":"TiProxy 尝鲜","uri":"/try-tiproxy/#总结"},{"categories":["mysql-operator"],"content":" mysql-operator chart 说明安装 mysql-operator 的操作命令在官方仓库和官网都有说明，具体安装命令如下： ## For Helm v3 helm repo add bitpoke https://helm-charts.bitpoke.io helm install mysql-operator bitpoke/mysql-operator 在安装的chart包中，values.yaml文件有些内容可以自定义配置。 如果想要在一个k8s集群里面启动多个mysql-operator，不想让不用operator互相干扰，就可以指定watchNamespace参数 如果k8s集群使用的storageClass不支持动态迁移，那么operator的replicaCount就得设置为3，否则节点挂掉后无法拉起新的pod 因为mysql高可用是基于orchestrator实现的，orchestrator又需要一个元数据库，元数据库的账号密码可以通过 orchestrator.topologyUser 和 orchestrator.topologyPassword 指定。topologyUser默认值是 orchestrator，如果密码不指定会随机生成 (其他参数以后用到再说明)… … ","date":"2023-03-12","objectID":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/:1:0","series":null,"tags":["mysql","operator"],"title":"bitpoke/mysql-operator 技术解读系列 - 部署","uri":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/#mysql-operator-chart-说明"},{"categories":["mysql-operator"],"content":" 在本地部署 mysql-operator 添加 repo $ helm repo add bitpoke https://helm-charts.bitpoke.io 安装 mysql-operator 安装之前先创建一个namespace，然后将mysql-operator安装到这个ns中 $ kubectl create namespace mysql namespace/mysql created $ helm install mysql-operator bitpoke/mysql-operator -n mysql NAME: mysql-operator LAST DEPLOYED: Fri Mar 17 18:53:26 2023 NAMESPACE: mysql STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: You can create a new cluster by issuing: cat \u003c\u003cEOF | kubectl apply -f- apiVersion: mysql.presslabs.org/v1alpha1 kind: MysqlCluster metadata: name: my-cluster spec: replicas: 1 secretName: my-cluster-secret --- apiVersion: v1 kind: Secret metadata: name: my-cluster-secret type: Opaque data: ROOT_PASSWORD: $(echo -n \"not-so-secure\" | base64) EOF 执行成功后，可以用kubectl工具查询pod状态，直到Running $ kubectl get all -n mysql NAME READY STATUS RESTARTS AGE pod/mysql-operator-0 2/2 Running 1 3m11s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/mysql-operator ClusterIP 10.102.117.142 \u003cnone\u003e 80/TCP,9125/TCP 9m1s service/mysql-operator-orc ClusterIP None \u003cnone\u003e 80/TCP,10008/TCP 9m1s NAME READY AGE statefulset.apps/mysql-operator 1/1 9m1s 至此 mysql-operator 就算安装完成了。 ","date":"2023-03-12","objectID":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/:2:0","series":null,"tags":["mysql","operator"],"title":"bitpoke/mysql-operator 技术解读系列 - 部署","uri":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/#在本地部署-mysql-operator"},{"categories":["mysql-operator"],"content":" mysql-operator 依赖资源解读上述的安装流程步骤很简单，但是运行起来的mysql-operator对我们来说就是一个黑盒，下面通过执行 helm template 对其被安装的内容进行梳理。 ","date":"2023-03-12","objectID":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/:3:0","series":null,"tags":["mysql","operator"],"title":"bitpoke/mysql-operator 技术解读系列 - 部署","uri":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/#mysql-operator-依赖资源解读"},{"categories":["mysql-operator"],"content":" CRD 资源CRD 全程叫做 CustomResourceDefinition，详细信息可以从官网获取，这里不进行解释。 $ helm template mysql-operator bitpoke/mysql-operator -n mysql --include-crds | grep -A 20 CustomResourceDefinition | grep -e kind kind: CustomResourceDefinition kind: MysqlBackup kind: CustomResourceDefinition kind: MysqlCluster kind: CustomResourceDefinition kind: MysqlDatabase kind: CustomResourceDefinition kind: MysqlUser 从上面可以看到，安装mysql-operator的时候会创建4个CRD资源。从名称可以看出是分别是关于backup、cluster、database、user 相关的，也是本系列在后续篇章中要重点讲解的内容。 ","date":"2023-03-12","objectID":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/:3:1","series":null,"tags":["mysql","operator"],"title":"bitpoke/mysql-operator 技术解读系列 - 部署","uri":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/#crd-资源"},{"categories":["mysql-operator"],"content":" 其他资源查看 mysql-operator 依赖的其他资源 $ helm template mysql-operator bitpoke/mysql-operator -n mysql | grep -e ^kind -e name | grep -A 1 ^kind kind: ServiceAccount name: mysql-operator -- kind: Secret name: mysql-operator-orc -- kind: ConfigMap name: mysql-operator-orc -- kind: ClusterRole name: mysql-operator -- kind: ClusterRoleBinding name: mysql-operator -- kind: Service name: mysql-operator-orc -- kind: Service name: mysql-operator -- kind: StatefulSet name: mysql-operator 从上可以看出，计算资源由StatefulSet管理，有2个Service资源，1个ConfigMap，1个Secret。其余3个是跟权限管理相关的配置。 接下来详细聊聊每个资源的作用。 ","date":"2023-03-12","objectID":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/:3:2","series":null,"tags":["mysql","operator"],"title":"bitpoke/mysql-operator 技术解读系列 - 部署","uri":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/#其他资源"},{"categories":["mysql-operator"],"content":" StatefulSethelm install 安装时的内容如下： # Source: mysql-operator/templates/statefulset.yaml apiVersion: apps/v1 kind: StatefulSet metadata: name: mysql-operator labels: helm.sh/chart: mysql-operator-0.6.2 app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator app.kubernetes.io/version: \"v0.6.2\" app.kubernetes.io/managed-by: Helm spec: replicas: 1 serviceName: mysql-operator-orc podManagementPolicy: Parallel selector: matchLabels: app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator template: metadata: annotations: checksum/orchestrator-config: 301f994fcecde72ab6be4371173a860c68b440504210a400a8105c833311443b checksum/orchestrator-secret: 20304c64003f30460df7e5cdcc078d3bc55b882af412ed1d43ced6a765f1c160 labels: app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator spec: serviceAccountName: mysql-operator securityContext: fsGroup: 65532 runAsGroup: 65532 runAsNonRoot: true runAsUser: 65532 containers: - name: operator securityContext: {} image: \"docker.io/bitpoke/mysql-operator:v0.6.2\" imagePullPolicy: IfNotPresent ports: - containerPort: 8080 name: prometheus protocol: TCP env: - name: ORC_TOPOLOGY_USER valueFrom: secretKeyRef: name: mysql-operator-orc key: TOPOLOGY_USER - name: ORC_TOPOLOGY_PASSWORD valueFrom: secretKeyRef: name: mysql-operator-orc key: TOPOLOGY_PASSWORD args: - --leader-election-namespace=mysql - --orchestrator-uri=http://mysql-operator.mysql/api - --sidecar-image=docker.io/bitpoke/mysql-operator-sidecar-5.7:v0.6.2 - --sidecar-mysql8-image=docker.io/bitpoke/mysql-operator-sidecar-8.0:v0.6.2 - --failover-before-shutdown=true livenessProbe: httpGet: path: /healthz port: 8081 readinessProbe: httpGet: path: /readyz port: 8081 resources: {} - name: orchestrator securityContext: {} image: docker.io/bitpoke/mysql-operator-orchestrator:v0.6.2 imagePullPolicy: IfNotPresent ports: - containerPort: 3000 name: http protocol: TCP - containerPort: 10008 name: raft protocol: TCP env: - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP envFrom: - prefix: ORC_ secretRef: name: mysql-operator-orc volumeMounts: - name: data mountPath: /var/lib/orchestrator - name: config mountPath: /usr/local/share/orchestrator/templates livenessProbe: timeoutSeconds: 10 initialDelaySeconds: 200 httpGet: path: /api/lb-check port: 3000 # https://github.com/github/orchestrator/blob/master/docs/raft.md#proxy-healthy-raft-nodes readinessProbe: timeoutSeconds: 10 httpGet: path: /api/raft-health port: 3000 resources: {} volumes: - name: config configMap: name: mysql-operator-orc volumeClaimTemplates: - metadata: name: data spec: accessModes: [ ReadWriteOnce ] resources: requests: storage: 1Gi 从描述中可以看到，一个pod里面会有两个container，分别启动 orchestrator 和 operator。 operator 容器 使用名字为 mysql-operator 的 serviceAccount 来做事件处理 声明 8080 的 prometheus 端口 在 livenessProbe 和 readinessProbe 里面出现了 8081 端口，但没有声明，所以只能 locahost 使用。 需要从 mysql-operator-orc secret 获取两个值当做自己的环境变量 ORC_TOPOLOGY_USER 和 ORC_TOPOLOGY_PASSWORD； 同时还有一些启动参数 - --leader-election-namespace=mysql # 如果安装时pod不是单副本，需要选主，这里是指定选主的ns - --orchestrator-uri=http://mysql-operator.mysql/api # operator 为了和 orchestrator 通信 - --sidecar-image=docker.io/bitpoke/mysql-operator-sidecar-5.7:v0.6.2 # 5.7 版本的 mysql sidecar 镜像 - --sidecar-mysql8-image=docker.io/bitpoke/mysql-operator-sidecar-8.0:v0.6.2 # 8.0 版本的 mysql sidecar 镜像。因为 mysql 版本差异导致备份恢复的 xtrabackup 软件版本不同。 - --failover-before-shutdown=true orchestrator 容器 声明 3000 的 http 服务端口 声明 10008 的 raft 消息同步端口 也需要从 mysql-operator-orc secret 获取值当做自己的环境变量 声明一个 volume 并挂载到 /var/lib/orchestrator(这里是为了存储 orchestrator 管理的元数据) 从 mysql-operator-orc configmap 获取配置并挂载到 /usr/local/share/orchestrator/templates(orchestrator启动的配置文件) ","date":"2023-03-12","objectID":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/:3:3","series":null,"tags":["mysql","operator"],"title":"bitpoke/mysql-operator 技术解读系列 - 部署","uri":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/#statefulset"},{"categories":["mysql-operator"],"content":" StatefulSethelm install 安装时的内容如下： # Source: mysql-operator/templates/statefulset.yaml apiVersion: apps/v1 kind: StatefulSet metadata: name: mysql-operator labels: helm.sh/chart: mysql-operator-0.6.2 app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator app.kubernetes.io/version: \"v0.6.2\" app.kubernetes.io/managed-by: Helm spec: replicas: 1 serviceName: mysql-operator-orc podManagementPolicy: Parallel selector: matchLabels: app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator template: metadata: annotations: checksum/orchestrator-config: 301f994fcecde72ab6be4371173a860c68b440504210a400a8105c833311443b checksum/orchestrator-secret: 20304c64003f30460df7e5cdcc078d3bc55b882af412ed1d43ced6a765f1c160 labels: app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator spec: serviceAccountName: mysql-operator securityContext: fsGroup: 65532 runAsGroup: 65532 runAsNonRoot: true runAsUser: 65532 containers: - name: operator securityContext: {} image: \"docker.io/bitpoke/mysql-operator:v0.6.2\" imagePullPolicy: IfNotPresent ports: - containerPort: 8080 name: prometheus protocol: TCP env: - name: ORC_TOPOLOGY_USER valueFrom: secretKeyRef: name: mysql-operator-orc key: TOPOLOGY_USER - name: ORC_TOPOLOGY_PASSWORD valueFrom: secretKeyRef: name: mysql-operator-orc key: TOPOLOGY_PASSWORD args: - --leader-election-namespace=mysql - --orchestrator-uri=http://mysql-operator.mysql/api - --sidecar-image=docker.io/bitpoke/mysql-operator-sidecar-5.7:v0.6.2 - --sidecar-mysql8-image=docker.io/bitpoke/mysql-operator-sidecar-8.0:v0.6.2 - --failover-before-shutdown=true livenessProbe: httpGet: path: /healthz port: 8081 readinessProbe: httpGet: path: /readyz port: 8081 resources: {} - name: orchestrator securityContext: {} image: docker.io/bitpoke/mysql-operator-orchestrator:v0.6.2 imagePullPolicy: IfNotPresent ports: - containerPort: 3000 name: http protocol: TCP - containerPort: 10008 name: raft protocol: TCP env: - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP envFrom: - prefix: ORC_ secretRef: name: mysql-operator-orc volumeMounts: - name: data mountPath: /var/lib/orchestrator - name: config mountPath: /usr/local/share/orchestrator/templates livenessProbe: timeoutSeconds: 10 initialDelaySeconds: 200 httpGet: path: /api/lb-check port: 3000 # https://github.com/github/orchestrator/blob/master/docs/raft.md#proxy-healthy-raft-nodes readinessProbe: timeoutSeconds: 10 httpGet: path: /api/raft-health port: 3000 resources: {} volumes: - name: config configMap: name: mysql-operator-orc volumeClaimTemplates: - metadata: name: data spec: accessModes: [ ReadWriteOnce ] resources: requests: storage: 1Gi 从描述中可以看到，一个pod里面会有两个container，分别启动 orchestrator 和 operator。 operator 容器 使用名字为 mysql-operator 的 serviceAccount 来做事件处理 声明 8080 的 prometheus 端口 在 livenessProbe 和 readinessProbe 里面出现了 8081 端口，但没有声明，所以只能 locahost 使用。 需要从 mysql-operator-orc secret 获取两个值当做自己的环境变量 ORC_TOPOLOGY_USER 和 ORC_TOPOLOGY_PASSWORD； 同时还有一些启动参数 - --leader-election-namespace=mysql # 如果安装时pod不是单副本，需要选主，这里是指定选主的ns - --orchestrator-uri=http://mysql-operator.mysql/api # operator 为了和 orchestrator 通信 - --sidecar-image=docker.io/bitpoke/mysql-operator-sidecar-5.7:v0.6.2 # 5.7 版本的 mysql sidecar 镜像 - --sidecar-mysql8-image=docker.io/bitpoke/mysql-operator-sidecar-8.0:v0.6.2 # 8.0 版本的 mysql sidecar 镜像。因为 mysql 版本差异导致备份恢复的 xtrabackup 软件版本不同。 - --failover-before-shutdown=true orchestrator 容器 声明 3000 的 http 服务端口 声明 10008 的 raft 消息同步端口 也需要从 mysql-operator-orc secret 获取值当做自己的环境变量 声明一个 volume 并挂载到 /var/lib/orchestrator(这里是为了存储 orchestrator 管理的元数据) 从 mysql-operator-orc configmap 获取配置并挂载到 /usr/local/share/orchestrator/templates(orchestrator启动的配置文件) ","date":"2023-03-12","objectID":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/:3:3","series":null,"tags":["mysql","operator"],"title":"bitpoke/mysql-operator 技术解读系列 - 部署","uri":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/#operator-容器"},{"categories":["mysql-operator"],"content":" StatefulSethelm install 安装时的内容如下： # Source: mysql-operator/templates/statefulset.yaml apiVersion: apps/v1 kind: StatefulSet metadata: name: mysql-operator labels: helm.sh/chart: mysql-operator-0.6.2 app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator app.kubernetes.io/version: \"v0.6.2\" app.kubernetes.io/managed-by: Helm spec: replicas: 1 serviceName: mysql-operator-orc podManagementPolicy: Parallel selector: matchLabels: app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator template: metadata: annotations: checksum/orchestrator-config: 301f994fcecde72ab6be4371173a860c68b440504210a400a8105c833311443b checksum/orchestrator-secret: 20304c64003f30460df7e5cdcc078d3bc55b882af412ed1d43ced6a765f1c160 labels: app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator spec: serviceAccountName: mysql-operator securityContext: fsGroup: 65532 runAsGroup: 65532 runAsNonRoot: true runAsUser: 65532 containers: - name: operator securityContext: {} image: \"docker.io/bitpoke/mysql-operator:v0.6.2\" imagePullPolicy: IfNotPresent ports: - containerPort: 8080 name: prometheus protocol: TCP env: - name: ORC_TOPOLOGY_USER valueFrom: secretKeyRef: name: mysql-operator-orc key: TOPOLOGY_USER - name: ORC_TOPOLOGY_PASSWORD valueFrom: secretKeyRef: name: mysql-operator-orc key: TOPOLOGY_PASSWORD args: - --leader-election-namespace=mysql - --orchestrator-uri=http://mysql-operator.mysql/api - --sidecar-image=docker.io/bitpoke/mysql-operator-sidecar-5.7:v0.6.2 - --sidecar-mysql8-image=docker.io/bitpoke/mysql-operator-sidecar-8.0:v0.6.2 - --failover-before-shutdown=true livenessProbe: httpGet: path: /healthz port: 8081 readinessProbe: httpGet: path: /readyz port: 8081 resources: {} - name: orchestrator securityContext: {} image: docker.io/bitpoke/mysql-operator-orchestrator:v0.6.2 imagePullPolicy: IfNotPresent ports: - containerPort: 3000 name: http protocol: TCP - containerPort: 10008 name: raft protocol: TCP env: - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP envFrom: - prefix: ORC_ secretRef: name: mysql-operator-orc volumeMounts: - name: data mountPath: /var/lib/orchestrator - name: config mountPath: /usr/local/share/orchestrator/templates livenessProbe: timeoutSeconds: 10 initialDelaySeconds: 200 httpGet: path: /api/lb-check port: 3000 # https://github.com/github/orchestrator/blob/master/docs/raft.md#proxy-healthy-raft-nodes readinessProbe: timeoutSeconds: 10 httpGet: path: /api/raft-health port: 3000 resources: {} volumes: - name: config configMap: name: mysql-operator-orc volumeClaimTemplates: - metadata: name: data spec: accessModes: [ ReadWriteOnce ] resources: requests: storage: 1Gi 从描述中可以看到，一个pod里面会有两个container，分别启动 orchestrator 和 operator。 operator 容器 使用名字为 mysql-operator 的 serviceAccount 来做事件处理 声明 8080 的 prometheus 端口 在 livenessProbe 和 readinessProbe 里面出现了 8081 端口，但没有声明，所以只能 locahost 使用。 需要从 mysql-operator-orc secret 获取两个值当做自己的环境变量 ORC_TOPOLOGY_USER 和 ORC_TOPOLOGY_PASSWORD； 同时还有一些启动参数 - --leader-election-namespace=mysql # 如果安装时pod不是单副本，需要选主，这里是指定选主的ns - --orchestrator-uri=http://mysql-operator.mysql/api # operator 为了和 orchestrator 通信 - --sidecar-image=docker.io/bitpoke/mysql-operator-sidecar-5.7:v0.6.2 # 5.7 版本的 mysql sidecar 镜像 - --sidecar-mysql8-image=docker.io/bitpoke/mysql-operator-sidecar-8.0:v0.6.2 # 8.0 版本的 mysql sidecar 镜像。因为 mysql 版本差异导致备份恢复的 xtrabackup 软件版本不同。 - --failover-before-shutdown=true orchestrator 容器 声明 3000 的 http 服务端口 声明 10008 的 raft 消息同步端口 也需要从 mysql-operator-orc secret 获取值当做自己的环境变量 声明一个 volume 并挂载到 /var/lib/orchestrator(这里是为了存储 orchestrator 管理的元数据) 从 mysql-operator-orc configmap 获取配置并挂载到 /usr/local/share/orchestrator/templates(orchestrator启动的配置文件) ","date":"2023-03-12","objectID":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/:3:3","series":null,"tags":["mysql","operator"],"title":"bitpoke/mysql-operator 技术解读系列 - 部署","uri":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/#orchestrator-容器"},{"categories":["mysql-operator"],"content":" ConfigMaphelm install 安装时的内容如下： # Source: mysql-operator/templates/orchestrator-config.yaml apiVersion: v1 kind: ConfigMap metadata: name: mysql-operator-orc labels: helm.sh/chart: mysql-operator-0.6.2 app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator app.kubernetes.io/version: \"v0.6.2\" app.kubernetes.io/managed-by: Helm data: orchestrator.conf.json: \"{..........}\" orc-topology.cnf: | [client] user = {{ .Env.ORC_TOPOLOGY_USER }} password = {{ .Env.ORC_TOPOLOGY_PASSWORD }} 在 StatefulSet 部分已经解释 orchestrator.conf.json 是 orchestrator 启动需要使用的配置文件。 orc-topology.cnf 是 orchestrator 访问数据库的账号密码。 配置信息如下（相关描述可以从官网查阅）： $ cat /usr/local/share/orchestrator/templates/orchestrator.conf.json { \"ApplyMySQLPromotionAfterMasterFailover\": true, \"BackendDB\": \"sqlite\", \"Debug\": false, \"DetachLostReplicasAfterMasterFailover\": true, \"DetectClusterAliasQuery\": \"SELECT CONCAT(SUBSTRING(@@hostname, 1, LENGTH(@@hostname) - 1 - LENGTH(SUBSTRING_INDEX(@@hostname,'-',-2))),'.',SUBSTRING_INDEX(@@report_host,'.',-1))\", \"DetectInstanceAliasQuery\": \"SELECT @@hostname\", \"DiscoverByShowSlaveHosts\": false, \"FailMasterPromotionIfSQLThreadNotUpToDate\": true, \"HTTPAdvertise\": \"http://{{ .Env.HOSTNAME }}.mysql-operator-orc:3000\", \"HostnameResolveMethod\": \"none\", \"InstancePollSeconds\": 5, \"ListenAddress\": \":3000\", \"MasterFailoverLostInstancesDowntimeMinutes\": 10, \"MySQLHostnameResolveMethod\": \"@@report_host\", \"MySQLTopologyCredentialsConfigFile\": \"/etc/orchestrator/orc-topology.cnf\", \"OnFailureDetectionProcesses\": [ \"/usr/local/bin/orc-helper event -w '{failureClusterAlias}' 'OrcFailureDetection' 'Failure: {failureType}, failed host: {failedHost}, lost replcas: {lostReplicas}' || true\", \"/usr/local/bin/orc-helper failover-in-progress '{failureClusterAlias}' '{failureDescription}' || true\" ], \"PostIntermediateMasterFailoverProcesses\": [ \"/usr/local/bin/orc-helper event '{failureClusterAlias}' 'OrcPostIntermediateMasterFailover' 'Failure type: {failureType}, failed hosts: {failedHost}, slaves: {countSlaves}' || true\" ], \"PostMasterFailoverProcesses\": [ \"/usr/local/bin/orc-helper event '{failureClusterAlias}' 'OrcPostMasterFailover' 'Failure type: {failureType}, new master: {successorHost}, slaves: {slaveHosts}' || true\" ], \"PostUnsuccessfulFailoverProcesses\": [ \"/usr/local/bin/orc-helper event -w '{failureClusterAlias}' 'OrcPostUnsuccessfulFailover' 'Failure: {failureType}, failed host: {failedHost} with {countSlaves} slaves' || true\" ], \"PreFailoverProcesses\": [ \"/usr/local/bin/orc-helper failover-in-progress '{failureClusterAlias}' '{failureDescription}' || true\" ], \"ProcessesShellCommand\": \"sh\", \"RaftAdvertise\": \"{{ .Env.HOSTNAME }}.mysql-operator-orc\", \"RaftBind\": \"{{ .Env.HOSTNAME }}\", \"RaftDataDir\": \"/var/lib/orchestrator\", \"RaftEnabled\": true, \"RaftNodes\": [], \"RecoverIntermediateMasterClusterFilters\": [ \".*\" ], \"RecoverMasterClusterFilters\": [ \".*\" ], \"RecoveryIgnoreHostnameFilters\": [], \"RecoveryPeriodBlockSeconds\": 300, \"RemoveTextFromHostnameDisplay\": \":3306\", \"SQLite3DataFile\": \"/var/lib/orchestrator/orc.db\", \"SlaveLagQuery\": \"SELECT TIMESTAMPDIFF(SECOND,ts,UTC_TIMESTAMP()) as drift FROM sys_operator.heartbeat ORDER BY drift ASC LIMIT 1\", \"UnseenInstanceForgetHours\": 1 } ","date":"2023-03-12","objectID":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/:3:4","series":null,"tags":["mysql","operator"],"title":"bitpoke/mysql-operator 技术解读系列 - 部署","uri":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/#configmap"},{"categories":["mysql-operator"],"content":" Secrethelm install 安装时的内容如下： # Source: mysql-operator/templates/orchestrator-secret.yaml apiVersion: v1 kind: Secret metadata: name: mysql-operator-orc labels: helm.sh/chart: mysql-operator-0.6.2 app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator app.kubernetes.io/version: \"v0.6.2\" app.kubernetes.io/managed-by: Helm data: TOPOLOGY_USER: \"b3JjaGVzdHJhdG9y\" TOPOLOGY_PASSWORD: \"aVdZZndhMzJHZw==\" 从 key 可以看出这就是 orchestrator 需要的元数据账号密码信息，使用base64解析出原文 $ echo 'b3JjaGVzdHJhdG9y' | base64 -d orchestrator $ echo 'aVdZZndhMzJHZw==' | base64 -d iWYfwa32Gg ","date":"2023-03-12","objectID":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/:3:5","series":null,"tags":["mysql","operator"],"title":"bitpoke/mysql-operator 技术解读系列 - 部署","uri":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/#secret"},{"categories":["mysql-operator"],"content":" Servicehelm install 安装时的内容如下： # Source: mysql-operator/templates/service.yaml apiVersion: v1 kind: Service metadata: name: mysql-operator labels: helm.sh/chart: mysql-operator-0.6.2 app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator app.kubernetes.io/version: \"v0.6.2\" app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: operator spec: type: ClusterIP ports: - port: 80 name: http protocol: TCP targetPort: http - port: 9125 name: prometheus protocol: TCP targetPort: prometheus selector: app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator --- # Source: mysql-operator/templates/orchestrator-raft-service.yaml apiVersion: v1 kind: Service metadata: name: mysql-operator-orc labels: app.kubernetes.io/component: orchestrator-raft helm.sh/chart: mysql-operator-0.6.2 app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator app.kubernetes.io/version: \"v0.6.2\" app.kubernetes.io/managed-by: Helm spec: type: ClusterIP clusterIP: None publishNotReadyAddresses: true ports: - name: http port: 80 targetPort: 3000 - name: raft port: 10008 targetPort: 10008 selector: app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator 以上有2个service，虽然 type 都是 ClusterIP，但是 mysql-operator-orc 还有个 clusterIP: None，这个配置表示该 service 是个 Headless Service，不需要额外的 ip。 mysql-operator-orc 有两个端口 80：targetPort 是 3000，orchestrator 的可视化页面和api服务接口 10008：targetPort 是 10008，这个端口是 orchestrator 多副本之前进行 raft 消息同步使用 mysql-operator 也有两个端口 80：targetPort 是 http，查询发现还是 orchestrator 的 3000 端口 9125：targetPort 是 prometheus，prometheus 在 StatefulSet中被定义到了 operator 容器的 8080 端口，通过 curl http://127.0.0.1:8080/metrics 可以查看到 operator 的指标数据 ","date":"2023-03-12","objectID":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/:3:6","series":null,"tags":["mysql","operator"],"title":"bitpoke/mysql-operator 技术解读系列 - 部署","uri":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/#service"},{"categories":["mysql-operator"],"content":" ServiceAccount、ClusterRole、ClusterRoleBindinghelm install 安装时的内容如下： # Source: mysql-operator/templates/serviceaccount.yaml apiVersion: v1 kind: ServiceAccount metadata: name: mysql-operator labels: helm.sh/chart: mysql-operator-0.6.2 app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator app.kubernetes.io/version: \"v0.6.2\" app.kubernetes.io/managed-by: Helm --- # Source: mysql-operator/templates/clusterrole.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: mysql-operator labels: helm.sh/chart: mysql-operator-0.6.2 app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator app.kubernetes.io/version: \"v0.6.2\" app.kubernetes.io/managed-by: Helm rules: - apiGroups: - apps resources: - statefulsets verbs: - create - delete - get - list - patch - update - watch - apiGroups: - batch resources: - jobs verbs: - create - delete - get - list - patch - update - watch - apiGroups: - coordination.k8s.io resources: - leases verbs: - create - delete - get - list - patch - update - watch - apiGroups: - \"\" resources: - configmaps - events - jobs - persistentvolumeclaims - pods - secrets - services verbs: - create - delete - get - list - patch - update - watch - apiGroups: - \"\" resources: - pods/status verbs: - create - delete - get - list - patch - update - watch - apiGroups: - mysql.presslabs.org resources: - mysqlbackups verbs: - create - delete - get - list - patch - update - watch - apiGroups: - mysql.presslabs.org resources: - mysqlclusters - mysqlclusters/status verbs: - create - delete - get - list - patch - update - watch - apiGroups: - mysql.presslabs.org resources: - mysqldatabases - mysqldatabases/status verbs: - create - delete - get - list - patch - update - watch - apiGroups: - mysql.presslabs.org resources: - mysqlusers - mysqlusers/status verbs: - create - delete - get - list - patch - update - watch - apiGroups: - policy resources: - poddisruptionbudgets verbs: - create - delete - get - list - patch - update - watch --- # Source: mysql-operator/templates/clusterrolebinding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: mysql-operator labels: helm.sh/chart: mysql-operator-0.6.2 app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator app.kubernetes.io/version: \"v0.6.2\" app.kubernetes.io/managed-by: Helm roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: mysql-operator subjects: - name: mysql-operator namespace: \"mysql\" kind: ServiceAccount 在这里要引入k8s一个重要的概念 RBAC(Role Base Access Control)，是一种权限控制机制，用于实现账户和权限的组合管理 通过 ClusterRole 来声明都能操作那些api； 通过 ServiceAccount 创建一个账户； 最后通过 ClusterRoleBinding 将两者绑定。 补充另外每次创建一个 sa 的时候都会生成一个对应的 token secret： $ kubectl create namespace sa-test namespace/sa-test created $ kubectl create serviceaccount test serviceaccount/test created $ kubectl get secret -n sa-test NAME TYPE DATA AGE default-token-6mdbt kubernetes.io/service-account-token 3 31s $ kubectl get secret -n sa-test -o yaml apiVersion: v1 items: - apiVersion: v1 data: ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJek1ETXhOekF5TlRrek1sb1hEVE16TURNeE5EQXlOVGt6TWxvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBT3lKCm1ScE51Q21tVGM5dXNKbVJGMEo0Umw1bElQeE1iM0psQ0dONGF5Ry9IZXFEcmJlZnFLMzZJWmFYZVhNbi9Zb0wKMkM3Mktmd1Z3UnJuUldmZkt6L3k0UFR1bkRVUHpNa1BOMloybVRmbENuRG1ENEVjdGk4SVVNeS81ZWtVZ0kvLwppaEh6MllrKzVLb1RISmozc1VrTFRyV2llc0E3WVV6WkZnTGRmZkU2Yjh1WVExSzZtTW1yeWtjSVdyTVJqNEMyCkZ1SjM2a1VqNEJWK1luRVRMNnAxb1FxZWJqa1I2dFZWajZvb25ZNmNHejBUc0JldE03M3FrRTBBUnBFVm1kb3cKSWY1MmVJN0U0WFBuNWtHQk9RZ3k1OE5tNHdzQmEvYTlIbEtCSUFMUm9vaDdyamhHbmpCT1ZtNnFMcDRjdUVkUwpqellONE9WdWtaQ2M1cGRLRVdNQ0F3RUFBYU5DTUVBd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZFME5mWDRsdnEyRjF3MXlVVElYSE5QNVRGSFVNQTBHQ1NxR1NJYjMKRFFFQkN3VUFBNElCQVFC","date":"2023-03-12","objectID":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/:3:7","series":null,"tags":["mysql","operator"],"title":"bitpoke/mysql-operator 技术解读系列 - 部署","uri":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/#serviceaccountclusterroleclusterrolebinding"},{"categories":["mysql-operator"],"content":" ServiceAccount、ClusterRole、ClusterRoleBindinghelm install 安装时的内容如下： # Source: mysql-operator/templates/serviceaccount.yaml apiVersion: v1 kind: ServiceAccount metadata: name: mysql-operator labels: helm.sh/chart: mysql-operator-0.6.2 app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator app.kubernetes.io/version: \"v0.6.2\" app.kubernetes.io/managed-by: Helm --- # Source: mysql-operator/templates/clusterrole.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: mysql-operator labels: helm.sh/chart: mysql-operator-0.6.2 app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator app.kubernetes.io/version: \"v0.6.2\" app.kubernetes.io/managed-by: Helm rules: - apiGroups: - apps resources: - statefulsets verbs: - create - delete - get - list - patch - update - watch - apiGroups: - batch resources: - jobs verbs: - create - delete - get - list - patch - update - watch - apiGroups: - coordination.k8s.io resources: - leases verbs: - create - delete - get - list - patch - update - watch - apiGroups: - \"\" resources: - configmaps - events - jobs - persistentvolumeclaims - pods - secrets - services verbs: - create - delete - get - list - patch - update - watch - apiGroups: - \"\" resources: - pods/status verbs: - create - delete - get - list - patch - update - watch - apiGroups: - mysql.presslabs.org resources: - mysqlbackups verbs: - create - delete - get - list - patch - update - watch - apiGroups: - mysql.presslabs.org resources: - mysqlclusters - mysqlclusters/status verbs: - create - delete - get - list - patch - update - watch - apiGroups: - mysql.presslabs.org resources: - mysqldatabases - mysqldatabases/status verbs: - create - delete - get - list - patch - update - watch - apiGroups: - mysql.presslabs.org resources: - mysqlusers - mysqlusers/status verbs: - create - delete - get - list - patch - update - watch - apiGroups: - policy resources: - poddisruptionbudgets verbs: - create - delete - get - list - patch - update - watch --- # Source: mysql-operator/templates/clusterrolebinding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: mysql-operator labels: helm.sh/chart: mysql-operator-0.6.2 app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator app.kubernetes.io/version: \"v0.6.2\" app.kubernetes.io/managed-by: Helm roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: mysql-operator subjects: - name: mysql-operator namespace: \"mysql\" kind: ServiceAccount 在这里要引入k8s一个重要的概念 RBAC(Role Base Access Control)，是一种权限控制机制，用于实现账户和权限的组合管理 通过 ClusterRole 来声明都能操作那些api； 通过 ServiceAccount 创建一个账户； 最后通过 ClusterRoleBinding 将两者绑定。 补充另外每次创建一个 sa 的时候都会生成一个对应的 token secret： $ kubectl create namespace sa-test namespace/sa-test created $ kubectl create serviceaccount test serviceaccount/test created $ kubectl get secret -n sa-test NAME TYPE DATA AGE default-token-6mdbt kubernetes.io/service-account-token 3 31s $ kubectl get secret -n sa-test -o yaml apiVersion: v1 items: - apiVersion: v1 data: ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJek1ETXhOekF5TlRrek1sb1hEVE16TURNeE5EQXlOVGt6TWxvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBT3lKCm1ScE51Q21tVGM5dXNKbVJGMEo0Umw1bElQeE1iM0psQ0dONGF5Ry9IZXFEcmJlZnFLMzZJWmFYZVhNbi9Zb0wKMkM3Mktmd1Z3UnJuUldmZkt6L3k0UFR1bkRVUHpNa1BOMloybVRmbENuRG1ENEVjdGk4SVVNeS81ZWtVZ0kvLwppaEh6MllrKzVLb1RISmozc1VrTFRyV2llc0E3WVV6WkZnTGRmZkU2Yjh1WVExSzZtTW1yeWtjSVdyTVJqNEMyCkZ1SjM2a1VqNEJWK1luRVRMNnAxb1FxZWJqa1I2dFZWajZvb25ZNmNHejBUc0JldE03M3FrRTBBUnBFVm1kb3cKSWY1MmVJN0U0WFBuNWtHQk9RZ3k1OE5tNHdzQmEvYTlIbEtCSUFMUm9vaDdyamhHbmpCT1ZtNnFMcDRjdUVkUwpqellONE9WdWtaQ2M1cGRLRVdNQ0F3RUFBYU5DTUVBd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZFME5mWDRsdnEyRjF3MXlVVElYSE5QNVRGSFVNQTBHQ1NxR1NJYjMKRFFFQkN3VUFBNElCQVFC","date":"2023-03-12","objectID":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/:3:7","series":null,"tags":["mysql","operator"],"title":"bitpoke/mysql-operator 技术解读系列 - 部署","uri":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/#补充"},{"categories":["mysql-operator"],"content":" 总结至此，通过安装mysql-operator和分析chart初步了解了资源组成，后续文章我们进入到mysql-operator内部，看看原理是如何实现的。 ","date":"2023-03-12","objectID":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/:4:0","series":null,"tags":["mysql","operator"],"title":"bitpoke/mysql-operator 技术解读系列 - 部署","uri":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/#总结"},{"categories":["mysql-operator"],"content":" 目的接下来的一段时间打算做个bitpoke/mysql-operator的技术解读。 出于两点考虑： 深度使用和改造过，熟悉这个项目 在 k8s 上部署 mysql-operator 的需求比较大，但是有没有比较好的原理解读 ","date":"2023-03-07","objectID":"/mysql-operator-0/:1:0","series":null,"tags":["mysql","operator"],"title":"bitpoke/mysql-operator 技术解读系列 - 序","uri":"/mysql-operator-0/#目的"},{"categories":["mysql-operator"],"content":" 项目 https://github.com/bitpoke/mysql-operator ","date":"2023-03-07","objectID":"/mysql-operator-0/:2:0","series":null,"tags":["mysql","operator"],"title":"bitpoke/mysql-operator 技术解读系列 - 序","uri":"/mysql-operator-0/#项目"},{"categories":["mysql-operator"],"content":" 范围包括但不限于： 创建集群 删除集群 参数修改 db user 操作 配置变更 备份数据 恢复数据 自动高可用 主备切换 … 如果有其他内容想要了解也可以留言~ ","date":"2023-03-07","objectID":"/mysql-operator-0/:3:0","series":null,"tags":["mysql","operator"],"title":"bitpoke/mysql-operator 技术解读系列 - 序","uri":"/mysql-operator-0/#范围"},{"categories":["Blog"],"content":" 安装最新版gohttps://www.runoob.com/go/go-environment.html ","date":"2023-03-04","objectID":"/how-to-run/:1:0","series":null,"tags":["Hugo"],"title":"How to Run","uri":"/how-to-run/#安装最新版go"},{"categories":["Blog"],"content":" 安装hugohttps://github.com/gohugoio/hugo CGO_ENABLED=1 go install --tags extended github.com/gohugoio/hugo@latest ","date":"2023-03-04","objectID":"/how-to-run/:2:0","series":null,"tags":["Hugo"],"title":"How to Run","uri":"/how-to-run/#安装hugo"},{"categories":["Blog"],"content":" 新建私有仓库并初始化新建私有仓库 https://github.com/xxx/xxx git clone https://github.com/xxxx/xxx cd xxxx hugo new site . --force ","date":"2023-03-04","objectID":"/how-to-run/:3:0","series":null,"tags":["Hugo"],"title":"How to Run","uri":"/how-to-run/#新建私有仓库并初始化"},{"categories":["Blog"],"content":" 更新主题 git submodule add https://github.com/HEIGE-PCloud/DoIt.git themes/DoIt echo 'theme = \"DoIt\"' \u003e\u003e config.toml ","date":"2023-03-04","objectID":"/how-to-run/:4:0","series":null,"tags":["Hugo"],"title":"How to Run","uri":"/how-to-run/#更新主题"},{"categories":["Blog"],"content":" 更多配置https://hugodoit.pages.dev/zh-cn/theme-documentation-basics/ (非必须)开启评论https://giscus.app/zh-CN [params.page.comment.giscus] enable = true # owner/repo dataRepo = \"xxxx/xxxx.github.io\" dataRepoId = \"xxxx\" dataCategory = \"Announcements\" dataCategoryId = \"xxxxx\" dataMapping = \"pathname\" dataReactionsEnabled = \"1\" dataEmitMetadata = \"0\" dataInputPosition = \"top\" lightTheme = \"light\" darkTheme = \"dark\" dataLang = \"en\" ","date":"2023-03-04","objectID":"/how-to-run/:4:1","series":null,"tags":["Hugo"],"title":"How to Run","uri":"/how-to-run/#更多配置"},{"categories":["Blog"],"content":" 更多配置https://hugodoit.pages.dev/zh-cn/theme-documentation-basics/ (非必须)开启评论https://giscus.app/zh-CN [params.page.comment.giscus] enable = true # owner/repo dataRepo = \"xxxx/xxxx.github.io\" dataRepoId = \"xxxx\" dataCategory = \"Announcements\" dataCategoryId = \"xxxxx\" dataMapping = \"pathname\" dataReactionsEnabled = \"1\" dataEmitMetadata = \"0\" dataInputPosition = \"top\" lightTheme = \"light\" darkTheme = \"dark\" dataLang = \"en\" ","date":"2023-03-04","objectID":"/how-to-run/:4:1","series":null,"tags":["Hugo"],"title":"How to Run","uri":"/how-to-run/#非必须开启评论"},{"categories":["Blog"],"content":" 新建文章 hugo new posts/how-to-run.md vim content/posts/how-to-run.md ","date":"2023-03-04","objectID":"/how-to-run/:5:0","series":null,"tags":["Hugo"],"title":"How to Run","uri":"/how-to-run/#新建文章"},{"categories":["Blog"],"content":" 运行看效果 默认情况下, 所有文章和页面均作为草稿创建. 如果想要渲染这些页面, 请从元数据中删除属性 draft: true, 设置属性 draft: false 或者为 hugo 命令添加 -D/–buildDrafts 参数. 由于本主题使用了 Hugo 中的 .Scratch 来实现一些特性, 非常建议你为 hugo server 命令添加 –disableFastRender 参数来实时预览你正在编辑的文章页面. hugo serve 的默认运行环境是 development, 而 hugo 的默认运行环境是 production. 由于本地 development 环境的限制, 评论系统, CDN 和 fingerprint 不会在 development 环境下启用. 你可以使用 hugo serve -e production 命令来开启这些特性. hugo server -D --disableFastRender -e production ","date":"2023-03-04","objectID":"/how-to-run/:6:0","series":null,"tags":["Hugo"],"title":"How to Run","uri":"/how-to-run/#运行看效果"},{"categories":["Blog"],"content":" 配置github action 实现提交同步到主页","date":"2023-03-04","objectID":"/how-to-run/:7:0","series":null,"tags":["Hugo"],"title":"How to Run","uri":"/how-to-run/#配置github-action-实现提交同步到主页"},{"categories":["Blog"],"content":" 配置 api tokenhttps://zhuanlan.zhihu.com/p/568764664 在私有仓库中配置 PERSONAL_TOKEN ","date":"2023-03-04","objectID":"/how-to-run/:7:1","series":null,"tags":["Hugo"],"title":"How to Run","uri":"/how-to-run/#配置-api-token"},{"categories":["Blog"],"content":" 新建个人主页https://github.com/xxxx/xxx.github.io ","date":"2023-03-04","objectID":"/how-to-run/:7:2","series":null,"tags":["Hugo"],"title":"How to Run","uri":"/how-to-run/#新建个人主页"},{"categories":["Blog"],"content":" 添加 workflowvim .github/workflows/gh-pages.yml name: GitHub Pages on: push: branches: - main # Set a branch to deploy pull_request: jobs: deploy: runs-on: ubuntu-latest concurrency: group: ${{ github.workflow }}-${{ github.ref }} steps: - uses: actions/checkout@v3 with: submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: '0.111.1' extended: true - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 if: ${{ github.ref == 'refs/heads/main' }} with: external_repository: xxx/xxx.github.io publish_dir: ./public publish_branch: gh-page personal_token: ${{ secrets.PERSONAL_TOKEN }} commit_message: ${{ github.event.head_commit.message }} ","date":"2023-03-04","objectID":"/how-to-run/:7:3","series":null,"tags":["Hugo"],"title":"How to Run","uri":"/how-to-run/#添加-workflow"},{"categories":["Blog"],"content":" 推送代码等待效果 git add . git commit -m \"add workflow\" git push ","date":"2023-03-04","objectID":"/how-to-run/:7:4","series":null,"tags":["Hugo"],"title":"How to Run","uri":"/how-to-run/#推送代码等待效果"}]