[{"categories":["tidb"],"content":" èƒŒæ™¯ä»Šå¤©ä½¿ç”¨tiupåšå®éªŒçš„äº‹åï¼Œå°†tidbèŠ‚ç‚¹ä»2ä¸ªç¼©åˆ°1ä¸ªï¼Œå‘ç°tiupè¿”å›æˆåŠŸä½†æ˜¯tidb-serverè¿›ç¨‹è¿˜åœ¨ã€‚ è¿™å°±å¼•å‘çš„æˆ‘çš„å¥½å¥‡å¿ƒï¼Œwhyï¼Ÿ ","date":"2023-07-31","objectID":"/tidb-you-ya-guan-bi/:1:0","series":null,"tags":["tidb","database"],"title":"TiDB ä¼˜é›…å…³é—­","uri":"/tidb-you-ya-guan-bi/#èƒŒæ™¯"},{"categories":["tidb"],"content":" å®éªŒå¤ç°","date":"2023-07-31","objectID":"/tidb-you-ya-guan-bi/:2:0","series":null,"tags":["tidb","database"],"title":"TiDB ä¼˜é›…å…³é—­","uri":"/tidb-you-ya-guan-bi/#å®éªŒå¤ç°"},{"categories":["tidb"],"content":" å¯åŠ¨é›†ç¾¤ #( 07/31/23@ 8:32ä¸‹åˆ )( happy@ZBMAC-f298743e3 ):~/docker/tiup/tiproxy tiup playground v6.4.0 --db 2 --kv 1 --pd 1 --tiflash 0 --without-monitor --db.config tidb.toml tiup is checking updates for component playground ... Starting component `playground`: /Users/happy/.tiup/components/playground/v1.12.5/tiup-playground v6.4.0 --db 2 --kv 1 --pd 1 --tiflash 0 --without-monitor --db.config tidb.toml Start pd instance:v6.4.0 Start tikv instance:v6.4.0 Start tidb instance:v6.4.0 Start tidb instance:v6.4.0 Waiting for tidb instances ready 127.0.0.1:4000 ... Done 127.0.0.1:4001 ... Done ğŸ‰ TiDB Playground Cluster is started, enjoy! Connect TiDB: mysql --comments --host 127.0.0.1 --port 4000 -u root Connect TiDB: mysql --comments --host 127.0.0.1 --port 4001 -u root TiDB Dashboard: http://127.0.0.1:2379/dashboard ","date":"2023-07-31","objectID":"/tidb-you-ya-guan-bi/:2:1","series":null,"tags":["tidb","database"],"title":"TiDB ä¼˜é›…å…³é—­","uri":"/tidb-you-ya-guan-bi/#å¯åŠ¨é›†ç¾¤"},{"categories":["tidb"],"content":" æŸ¥çœ‹èŠ‚ç‚¹ä¿¡æ¯ #( 07/31/23@ 8:32ä¸‹åˆ )( happy@ZBMAC-f298743e3 ):~ tiup playground display tiup is checking updates for component playground ... Starting component `playground`: /Users/happy/.tiup/components/playground/v1.12.5/tiup-playground display Pid Role Uptime --- ---- ------ 10113 pd 49.376485092s 10114 tikv 49.32262974s 10115 tidb 49.283144092s 10116 tidb 49.245069308s ","date":"2023-07-31","objectID":"/tidb-you-ya-guan-bi/:2:2","series":null,"tags":["tidb","database"],"title":"TiDB ä¼˜é›…å…³é—­","uri":"/tidb-you-ya-guan-bi/#æŸ¥çœ‹èŠ‚ç‚¹ä¿¡æ¯"},{"categories":["tidb"],"content":" ç¼©æ‰ä¸€ä¸ªtidbèŠ‚ç‚¹ #( 07/31/23@ 8:34ä¸‹åˆ )( happy@ZBMAC-f298743e3 ):~ tiup playground scale-in --pid 10115 tiup is checking updates for component playground ... Starting component `playground`: /Users/happy/.tiup/components/playground/v1.12.5/tiup-playground scale-in --pid 10115 scale in tidb success è¿™é‡Œå¯ä»¥çœ‹åˆ°å·²ç»è¿”å›äº† scale in tidb success ","date":"2023-07-31","objectID":"/tidb-you-ya-guan-bi/:2:3","series":null,"tags":["tidb","database"],"title":"TiDB ä¼˜é›…å…³é—­","uri":"/tidb-you-ya-guan-bi/#ç¼©æ‰ä¸€ä¸ªtidbèŠ‚ç‚¹"},{"categories":["tidb"],"content":" æŸ¥çœ‹è¿›ç¨‹ #( 07/31/23@ 8:34ä¸‹åˆ )( happy@ZBMAC-f298743e3 ):~ ps -ef | grep 10115 502 11371 99718 0 8:34ä¸‹åˆ ttys001 0:00.00 grep --color=auto --exclude-dir=.bzr --exclude-dir=CVS --exclude-dir=.git --exclude-dir=.hg --exclude-dir=.svn --exclude-dir=.idea --exclude-dir=.tox 10115 502 10115 10111 0 8:32ä¸‹åˆ ttys005 0:04.29 /Users/happy/.tiup/components/tidb/v6.4.0/tidb-server -P 4000 --store=tikv --host=127.0.0.1 --status=10080 --path=127.0.0.1:2379 --log-file=/Users/happy/.tiup/data/TlaeoSj/tidb-0/tidb.log --config=/Users/happy/.tiup/data/TlaeoSj/tidb-0/tidb.toml è¿›ç¨‹è¿˜æ˜¯å­˜åœ¨ ","date":"2023-07-31","objectID":"/tidb-you-ya-guan-bi/:2:4","series":null,"tags":["tidb","database"],"title":"TiDB ä¼˜é›…å…³é—­","uri":"/tidb-you-ya-guan-bi/#æŸ¥çœ‹è¿›ç¨‹"},{"categories":["tidb"],"content":" åˆ†æäºæ˜¯æŸ¥çœ‹äº† v6.4.0 çš„ tidb-server ä»£ç ã€‚é¦–å…ˆæƒ³åˆ°å»mainå‡½æ•°çœ‹ä¸‹closeçš„æµç¨‹ ","date":"2023-07-31","objectID":"/tidb-you-ya-guan-bi/:3:0","series":null,"tags":["tidb","database"],"title":"TiDB ä¼˜é›…å…³é—­","uri":"/tidb-you-ya-guan-bi/#åˆ†æ"},{"categories":["tidb"],"content":" main func main() { //.. signal.SetupSignalHandler(func(graceful bool) { svr.Close() cleanup(svr, storage, dom, graceful) cpuprofile.StopCPUProfiler() close(exited) }) // ... } åœ¨è¿™é‡Œå‘ç°ä¸¤ä¸ªé‡è¦çš„é€»è¾‘ svr.Close()ï¼Œcleanup(svr, storage, dom, graceful) ","date":"2023-07-31","objectID":"/tidb-you-ya-guan-bi/:3:1","series":null,"tags":["tidb","database"],"title":"TiDB ä¼˜é›…å…³é—­","uri":"/tidb-you-ya-guan-bi/#main"},{"categories":["tidb"],"content":" svr.Close() // Close closes the server. func (s *Server) Close() { s.startShutdown() s.rwlock.Lock() // prevent new connections defer s.rwlock.Unlock() if s.listener != nil { err := s.listener.Close() terror.Log(errors.Trace(err)) s.listener = nil } if s.socket != nil { err := s.socket.Close() terror.Log(errors.Trace(err)) s.socket = nil } if s.statusServer != nil { err := s.statusServer.Close() terror.Log(errors.Trace(err)) s.statusServer = nil } if s.grpcServer != nil { s.grpcServer.Stop() s.grpcServer = nil } if s.autoIDService != nil { s.autoIDService.Close() } if s.authTokenCancelFunc != nil { s.authTokenCancelFunc() } s.wg.Wait() metrics.ServerEventCounter.WithLabelValues(metrics.EventClose).Inc() } func (s *Server) startShutdown() { s.rwlock.RLock() logutil.BgLogger().Info(\"setting tidb-server to report unhealthy (shutting-down)\") s.inShutdownMode = true s.rwlock.RUnlock() // give the load balancer a chance to receive a few unhealthy health reports // before acquiring the s.rwlock and blocking connections. waitTime := time.Duration(s.cfg.GracefulWaitBeforeShutdown) * time.Second if waitTime \u003e 0 { logutil.BgLogger().Info(\"waiting for stray connections before starting shutdown process\", zap.Duration(\"waitTime\", waitTime)) time.Sleep(waitTime) } } ä»ä¸Šé¢çš„é€»è¾‘å¯ä»¥çœ‹åˆ°ï¼Œcloseçš„æ—¶å€™å…ˆstartShutdownå†è¿›è¡Œèµ„æºå›æ”¶ã€‚è€Œåœ¨æ‰§è¡ŒstartShutdownçš„æ—¶å€™ï¼Œå±…ç„¶æœ‰ä¸ªtime.Sleep(waitTime)ã€‚ ç„¶åç ”ç©¶ä¸‹ graceful-wait-before-shutdown å‚æ•°ï¼Œå‘ç°å‚æ•°æ˜¯0ï¼Œä¸æ˜¯æ­¤å¤„å¯¼è‡´çš„ã€‚ åœ¨ TiDB ç­‰å¾…æœåŠ¡å™¨å…³é—­æœŸé—´ï¼ŒHTTP çŠ¶æ€ä¼šæ˜¾ç¤ºå¤±è´¥ï¼Œä½¿å¾—è´Ÿè½½å‡è¡¡å™¨å¯ä»¥é‡æ–°è·¯ç”±æµé‡ é»˜è®¤å€¼ï¼š0 æŒ‡å®šå…³é—­æœåŠ¡å™¨æ—¶ TiDB ç­‰å¾…çš„ç§’æ•°ï¼Œä½¿å¾—å®¢æˆ·ç«¯æœ‰æ—¶é—´æ–­å¼€è¿æ¥ã€‚ ","date":"2023-07-31","objectID":"/tidb-you-ya-guan-bi/:3:2","series":null,"tags":["tidb","database"],"title":"TiDB ä¼˜é›…å…³é—­","uri":"/tidb-you-ya-guan-bi/#svrclose"},{"categories":["tidb"],"content":" cleanup()åœ¨ cleanup ä¸­çœ‹åˆ°äº† GracefulDown å’Œ TryGracefulDown ä¸¤ä¸ªæ–¹æ³• func cleanup(svr *server.Server, storage kv.Storage, dom *domain.Domain, graceful bool) { if graceful { done := make(chan struct{}) svr.GracefulDown(context.Background(), done) } else { svr.TryGracefulDown() } plugin.Shutdown(context.Background()) closeDomainAndStorage(storage, dom) disk.CleanUp() topsql.Close() } TryGracefulDownç ”ç©¶å‘ç°ä½¿ç”¨ SIGHUP ç»ˆæ­¢è¿›ç¨‹æ—¶ä½¿ç”¨ TryGracefulDown æ–¹æ³•ï¼Œå…¶ä»–æ—¶å€™ä½¿ç”¨ GracefulDownã€‚å¯¹æ¯” TryGracefulDown å’Œ GracefulDown å®ç°ï¼Œ TryGracefulDown åªæ˜¯å¤šä¸ª15sçš„è¶…æ—¶å¤„ç†ï¼Œåº•å±‚é€»è¾‘è¿˜æ˜¯ GracefulDown var gracefulCloseConnectionsTimeout = 15 * time.Second // TryGracefulDown will try to gracefully close all connection first with timeout. if timeout, will close all connection directly. func (s *Server) TryGracefulDown() { ctx, cancel := context.WithTimeout(context.Background(), gracefulCloseConnectionsTimeout) defer cancel() done := make(chan struct{}) go func() { s.GracefulDown(ctx, done) }() select { case \u003c-ctx.Done(): s.KillAllConnections() case \u003c-done: return } } GracefulDownä¸‹é¢æ˜¯ GracefulDown å®ç°ï¼ŒåŸæ¥åœ¨è¿™é‡Œä¼šé—´éš”1sï¼Œä¸€ç›´åˆ¤æ–­å®¢æˆ·ç«¯è¿æ¥æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨æ‰é€€å‡ºã€‚ // GracefulDown waits all clients to close. func (s *Server) GracefulDown(ctx context.Context, done chan struct{}) { logutil.Logger(ctx).Info(\"[server] graceful shutdown.\") metrics.ServerEventCounter.WithLabelValues(metrics.EventGracefulDown).Inc() count := s.ConnectionCount() for i := 0; count \u003e 0; i++ { s.kickIdleConnection() count = s.ConnectionCount() if count == 0 { break } // Print information for every 30s. if i%30 == 0 { logutil.Logger(ctx).Info(\"graceful shutdown...\", zap.Int(\"conn count\", count)) } ticker := time.After(time.Second) select { case \u003c-ctx.Done(): return case \u003c-ticker: } } close(done) } ConnectionCountåˆ¤æ–­è¿æ¥ä¸ªæ•°çš„é€»è¾‘ä¹Ÿå¾ˆç®€å•ï¼Œå°±æ˜¯å¯¹ç®—ä¸‹ s.clients çš„ length // ConnectionCount gets current connection count. func (s *Server) ConnectionCount() int { s.rwlock.RLock() cnt := len(s.clients) s.rwlock.RUnlock() return cnt } å…¶ä¸­è¿˜æœ‰ä¸€ä¸ªå¥‡æ€ªçš„å‡½æ•° kickIdleConnectionï¼Œè¿™ä¸ªæ˜¯åšä»€ä¹ˆçš„ï¼Ÿ kickIdleConnectionçœ‹é€»è¾‘æ˜¯æ”¶é›†å¯ä»¥è¢«closeçš„ä¼šè¯ç„¶åcloseæ‰ã€‚ func (s *Server) kickIdleConnection() { var conns []*clientConn s.rwlock.RLock() for _, cc := range s.clients { if cc.ShutdownOrNotify() { // Shutdowned conn will be closed by us, and notified conn will exist themselves. conns = append(conns, cc) } } s.rwlock.RUnlock() for _, cc := range conns { err := cc.Close() if err != nil { logutil.BgLogger().Error(\"close connection\", zap.Error(err)) } } } é‚£ä¹ˆä»€ä¹ˆæ ·çš„ä¼šè¯å¯ä»¥è¢«closeå‘¢ï¼Ÿ ShutdownOrNotifyæœ‰ä¸‰ç±»ï¼š client çŠ¶æ€å¤„äº ServerStatusInTransï¼› çŠ¶æ€å¤„äº connStatusReading ä»¥åŠå¤„äº connStatusDispatching åœ¨ clientConn.Run æ–¹æ³•ä¸­è¢«å›æ”¶ // ShutdownOrNotify will Shutdown this client connection, or do its best to notify. func (cc *clientConn) ShutdownOrNotify() bool { if (cc.ctx.Status() \u0026 mysql.ServerStatusInTrans) \u003e 0 { return false } // If the client connection status is reading, it's safe to shutdown it. if atomic.CompareAndSwapInt32(\u0026cc.status, connStatusReading, connStatusShutdown) { return true } // If the client connection status is dispatching, we can't shutdown it immediately, // so set the status to WaitShutdown as a notification, the loop in clientConn.Run // will detect it and then exit. atomic.StoreInt32(\u0026cc.status, connStatusWaitShutdown) return false } const ( connStatusDispatching int32 = iota connStatusReading connStatusShutdown // Closed by server. connStatusWaitShutdown // Notified by server to close. ) ","date":"2023-07-31","objectID":"/tidb-you-ya-guan-bi/:3:3","series":null,"tags":["tidb","database"],"title":"TiDB ä¼˜é›…å…³é—­","uri":"/tidb-you-ya-guan-bi/#cleanup"},{"categories":["tidb"],"content":" cleanup()åœ¨ cleanup ä¸­çœ‹åˆ°äº† GracefulDown å’Œ TryGracefulDown ä¸¤ä¸ªæ–¹æ³• func cleanup(svr *server.Server, storage kv.Storage, dom *domain.Domain, graceful bool) { if graceful { done := make(chan struct{}) svr.GracefulDown(context.Background(), done) } else { svr.TryGracefulDown() } plugin.Shutdown(context.Background()) closeDomainAndStorage(storage, dom) disk.CleanUp() topsql.Close() } TryGracefulDownç ”ç©¶å‘ç°ä½¿ç”¨ SIGHUP ç»ˆæ­¢è¿›ç¨‹æ—¶ä½¿ç”¨ TryGracefulDown æ–¹æ³•ï¼Œå…¶ä»–æ—¶å€™ä½¿ç”¨ GracefulDownã€‚å¯¹æ¯” TryGracefulDown å’Œ GracefulDown å®ç°ï¼Œ TryGracefulDown åªæ˜¯å¤šä¸ª15sçš„è¶…æ—¶å¤„ç†ï¼Œåº•å±‚é€»è¾‘è¿˜æ˜¯ GracefulDown var gracefulCloseConnectionsTimeout = 15 * time.Second // TryGracefulDown will try to gracefully close all connection first with timeout. if timeout, will close all connection directly. func (s *Server) TryGracefulDown() { ctx, cancel := context.WithTimeout(context.Background(), gracefulCloseConnectionsTimeout) defer cancel() done := make(chan struct{}) go func() { s.GracefulDown(ctx, done) }() select { case \u003c-ctx.Done(): s.KillAllConnections() case \u003c-done: return } } GracefulDownä¸‹é¢æ˜¯ GracefulDown å®ç°ï¼ŒåŸæ¥åœ¨è¿™é‡Œä¼šé—´éš”1sï¼Œä¸€ç›´åˆ¤æ–­å®¢æˆ·ç«¯è¿æ¥æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨æ‰é€€å‡ºã€‚ // GracefulDown waits all clients to close. func (s *Server) GracefulDown(ctx context.Context, done chan struct{}) { logutil.Logger(ctx).Info(\"[server] graceful shutdown.\") metrics.ServerEventCounter.WithLabelValues(metrics.EventGracefulDown).Inc() count := s.ConnectionCount() for i := 0; count \u003e 0; i++ { s.kickIdleConnection() count = s.ConnectionCount() if count == 0 { break } // Print information for every 30s. if i%30 == 0 { logutil.Logger(ctx).Info(\"graceful shutdown...\", zap.Int(\"conn count\", count)) } ticker := time.After(time.Second) select { case \u003c-ctx.Done(): return case \u003c-ticker: } } close(done) } ConnectionCountåˆ¤æ–­è¿æ¥ä¸ªæ•°çš„é€»è¾‘ä¹Ÿå¾ˆç®€å•ï¼Œå°±æ˜¯å¯¹ç®—ä¸‹ s.clients çš„ length // ConnectionCount gets current connection count. func (s *Server) ConnectionCount() int { s.rwlock.RLock() cnt := len(s.clients) s.rwlock.RUnlock() return cnt } å…¶ä¸­è¿˜æœ‰ä¸€ä¸ªå¥‡æ€ªçš„å‡½æ•° kickIdleConnectionï¼Œè¿™ä¸ªæ˜¯åšä»€ä¹ˆçš„ï¼Ÿ kickIdleConnectionçœ‹é€»è¾‘æ˜¯æ”¶é›†å¯ä»¥è¢«closeçš„ä¼šè¯ç„¶åcloseæ‰ã€‚ func (s *Server) kickIdleConnection() { var conns []*clientConn s.rwlock.RLock() for _, cc := range s.clients { if cc.ShutdownOrNotify() { // Shutdowned conn will be closed by us, and notified conn will exist themselves. conns = append(conns, cc) } } s.rwlock.RUnlock() for _, cc := range conns { err := cc.Close() if err != nil { logutil.BgLogger().Error(\"close connection\", zap.Error(err)) } } } é‚£ä¹ˆä»€ä¹ˆæ ·çš„ä¼šè¯å¯ä»¥è¢«closeå‘¢ï¼Ÿ ShutdownOrNotifyæœ‰ä¸‰ç±»ï¼š client çŠ¶æ€å¤„äº ServerStatusInTransï¼› çŠ¶æ€å¤„äº connStatusReading ä»¥åŠå¤„äº connStatusDispatching åœ¨ clientConn.Run æ–¹æ³•ä¸­è¢«å›æ”¶ // ShutdownOrNotify will Shutdown this client connection, or do its best to notify. func (cc *clientConn) ShutdownOrNotify() bool { if (cc.ctx.Status() \u0026 mysql.ServerStatusInTrans) \u003e 0 { return false } // If the client connection status is reading, it's safe to shutdown it. if atomic.CompareAndSwapInt32(\u0026cc.status, connStatusReading, connStatusShutdown) { return true } // If the client connection status is dispatching, we can't shutdown it immediately, // so set the status to WaitShutdown as a notification, the loop in clientConn.Run // will detect it and then exit. atomic.StoreInt32(\u0026cc.status, connStatusWaitShutdown) return false } const ( connStatusDispatching int32 = iota connStatusReading connStatusShutdown // Closed by server. connStatusWaitShutdown // Notified by server to close. ) ","date":"2023-07-31","objectID":"/tidb-you-ya-guan-bi/:3:3","series":null,"tags":["tidb","database"],"title":"TiDB ä¼˜é›…å…³é—­","uri":"/tidb-you-ya-guan-bi/#trygracefuldown"},{"categories":["tidb"],"content":" cleanup()åœ¨ cleanup ä¸­çœ‹åˆ°äº† GracefulDown å’Œ TryGracefulDown ä¸¤ä¸ªæ–¹æ³• func cleanup(svr *server.Server, storage kv.Storage, dom *domain.Domain, graceful bool) { if graceful { done := make(chan struct{}) svr.GracefulDown(context.Background(), done) } else { svr.TryGracefulDown() } plugin.Shutdown(context.Background()) closeDomainAndStorage(storage, dom) disk.CleanUp() topsql.Close() } TryGracefulDownç ”ç©¶å‘ç°ä½¿ç”¨ SIGHUP ç»ˆæ­¢è¿›ç¨‹æ—¶ä½¿ç”¨ TryGracefulDown æ–¹æ³•ï¼Œå…¶ä»–æ—¶å€™ä½¿ç”¨ GracefulDownã€‚å¯¹æ¯” TryGracefulDown å’Œ GracefulDown å®ç°ï¼Œ TryGracefulDown åªæ˜¯å¤šä¸ª15sçš„è¶…æ—¶å¤„ç†ï¼Œåº•å±‚é€»è¾‘è¿˜æ˜¯ GracefulDown var gracefulCloseConnectionsTimeout = 15 * time.Second // TryGracefulDown will try to gracefully close all connection first with timeout. if timeout, will close all connection directly. func (s *Server) TryGracefulDown() { ctx, cancel := context.WithTimeout(context.Background(), gracefulCloseConnectionsTimeout) defer cancel() done := make(chan struct{}) go func() { s.GracefulDown(ctx, done) }() select { case \u003c-ctx.Done(): s.KillAllConnections() case \u003c-done: return } } GracefulDownä¸‹é¢æ˜¯ GracefulDown å®ç°ï¼ŒåŸæ¥åœ¨è¿™é‡Œä¼šé—´éš”1sï¼Œä¸€ç›´åˆ¤æ–­å®¢æˆ·ç«¯è¿æ¥æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨æ‰é€€å‡ºã€‚ // GracefulDown waits all clients to close. func (s *Server) GracefulDown(ctx context.Context, done chan struct{}) { logutil.Logger(ctx).Info(\"[server] graceful shutdown.\") metrics.ServerEventCounter.WithLabelValues(metrics.EventGracefulDown).Inc() count := s.ConnectionCount() for i := 0; count \u003e 0; i++ { s.kickIdleConnection() count = s.ConnectionCount() if count == 0 { break } // Print information for every 30s. if i%30 == 0 { logutil.Logger(ctx).Info(\"graceful shutdown...\", zap.Int(\"conn count\", count)) } ticker := time.After(time.Second) select { case \u003c-ctx.Done(): return case \u003c-ticker: } } close(done) } ConnectionCountåˆ¤æ–­è¿æ¥ä¸ªæ•°çš„é€»è¾‘ä¹Ÿå¾ˆç®€å•ï¼Œå°±æ˜¯å¯¹ç®—ä¸‹ s.clients çš„ length // ConnectionCount gets current connection count. func (s *Server) ConnectionCount() int { s.rwlock.RLock() cnt := len(s.clients) s.rwlock.RUnlock() return cnt } å…¶ä¸­è¿˜æœ‰ä¸€ä¸ªå¥‡æ€ªçš„å‡½æ•° kickIdleConnectionï¼Œè¿™ä¸ªæ˜¯åšä»€ä¹ˆçš„ï¼Ÿ kickIdleConnectionçœ‹é€»è¾‘æ˜¯æ”¶é›†å¯ä»¥è¢«closeçš„ä¼šè¯ç„¶åcloseæ‰ã€‚ func (s *Server) kickIdleConnection() { var conns []*clientConn s.rwlock.RLock() for _, cc := range s.clients { if cc.ShutdownOrNotify() { // Shutdowned conn will be closed by us, and notified conn will exist themselves. conns = append(conns, cc) } } s.rwlock.RUnlock() for _, cc := range conns { err := cc.Close() if err != nil { logutil.BgLogger().Error(\"close connection\", zap.Error(err)) } } } é‚£ä¹ˆä»€ä¹ˆæ ·çš„ä¼šè¯å¯ä»¥è¢«closeå‘¢ï¼Ÿ ShutdownOrNotifyæœ‰ä¸‰ç±»ï¼š client çŠ¶æ€å¤„äº ServerStatusInTransï¼› çŠ¶æ€å¤„äº connStatusReading ä»¥åŠå¤„äº connStatusDispatching åœ¨ clientConn.Run æ–¹æ³•ä¸­è¢«å›æ”¶ // ShutdownOrNotify will Shutdown this client connection, or do its best to notify. func (cc *clientConn) ShutdownOrNotify() bool { if (cc.ctx.Status() \u0026 mysql.ServerStatusInTrans) \u003e 0 { return false } // If the client connection status is reading, it's safe to shutdown it. if atomic.CompareAndSwapInt32(\u0026cc.status, connStatusReading, connStatusShutdown) { return true } // If the client connection status is dispatching, we can't shutdown it immediately, // so set the status to WaitShutdown as a notification, the loop in clientConn.Run // will detect it and then exit. atomic.StoreInt32(\u0026cc.status, connStatusWaitShutdown) return false } const ( connStatusDispatching int32 = iota connStatusReading connStatusShutdown // Closed by server. connStatusWaitShutdown // Notified by server to close. ) ","date":"2023-07-31","objectID":"/tidb-you-ya-guan-bi/:3:3","series":null,"tags":["tidb","database"],"title":"TiDB ä¼˜é›…å…³é—­","uri":"/tidb-you-ya-guan-bi/#gracefuldown"},{"categories":["tidb"],"content":" cleanup()åœ¨ cleanup ä¸­çœ‹åˆ°äº† GracefulDown å’Œ TryGracefulDown ä¸¤ä¸ªæ–¹æ³• func cleanup(svr *server.Server, storage kv.Storage, dom *domain.Domain, graceful bool) { if graceful { done := make(chan struct{}) svr.GracefulDown(context.Background(), done) } else { svr.TryGracefulDown() } plugin.Shutdown(context.Background()) closeDomainAndStorage(storage, dom) disk.CleanUp() topsql.Close() } TryGracefulDownç ”ç©¶å‘ç°ä½¿ç”¨ SIGHUP ç»ˆæ­¢è¿›ç¨‹æ—¶ä½¿ç”¨ TryGracefulDown æ–¹æ³•ï¼Œå…¶ä»–æ—¶å€™ä½¿ç”¨ GracefulDownã€‚å¯¹æ¯” TryGracefulDown å’Œ GracefulDown å®ç°ï¼Œ TryGracefulDown åªæ˜¯å¤šä¸ª15sçš„è¶…æ—¶å¤„ç†ï¼Œåº•å±‚é€»è¾‘è¿˜æ˜¯ GracefulDown var gracefulCloseConnectionsTimeout = 15 * time.Second // TryGracefulDown will try to gracefully close all connection first with timeout. if timeout, will close all connection directly. func (s *Server) TryGracefulDown() { ctx, cancel := context.WithTimeout(context.Background(), gracefulCloseConnectionsTimeout) defer cancel() done := make(chan struct{}) go func() { s.GracefulDown(ctx, done) }() select { case \u003c-ctx.Done(): s.KillAllConnections() case \u003c-done: return } } GracefulDownä¸‹é¢æ˜¯ GracefulDown å®ç°ï¼ŒåŸæ¥åœ¨è¿™é‡Œä¼šé—´éš”1sï¼Œä¸€ç›´åˆ¤æ–­å®¢æˆ·ç«¯è¿æ¥æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨æ‰é€€å‡ºã€‚ // GracefulDown waits all clients to close. func (s *Server) GracefulDown(ctx context.Context, done chan struct{}) { logutil.Logger(ctx).Info(\"[server] graceful shutdown.\") metrics.ServerEventCounter.WithLabelValues(metrics.EventGracefulDown).Inc() count := s.ConnectionCount() for i := 0; count \u003e 0; i++ { s.kickIdleConnection() count = s.ConnectionCount() if count == 0 { break } // Print information for every 30s. if i%30 == 0 { logutil.Logger(ctx).Info(\"graceful shutdown...\", zap.Int(\"conn count\", count)) } ticker := time.After(time.Second) select { case \u003c-ctx.Done(): return case \u003c-ticker: } } close(done) } ConnectionCountåˆ¤æ–­è¿æ¥ä¸ªæ•°çš„é€»è¾‘ä¹Ÿå¾ˆç®€å•ï¼Œå°±æ˜¯å¯¹ç®—ä¸‹ s.clients çš„ length // ConnectionCount gets current connection count. func (s *Server) ConnectionCount() int { s.rwlock.RLock() cnt := len(s.clients) s.rwlock.RUnlock() return cnt } å…¶ä¸­è¿˜æœ‰ä¸€ä¸ªå¥‡æ€ªçš„å‡½æ•° kickIdleConnectionï¼Œè¿™ä¸ªæ˜¯åšä»€ä¹ˆçš„ï¼Ÿ kickIdleConnectionçœ‹é€»è¾‘æ˜¯æ”¶é›†å¯ä»¥è¢«closeçš„ä¼šè¯ç„¶åcloseæ‰ã€‚ func (s *Server) kickIdleConnection() { var conns []*clientConn s.rwlock.RLock() for _, cc := range s.clients { if cc.ShutdownOrNotify() { // Shutdowned conn will be closed by us, and notified conn will exist themselves. conns = append(conns, cc) } } s.rwlock.RUnlock() for _, cc := range conns { err := cc.Close() if err != nil { logutil.BgLogger().Error(\"close connection\", zap.Error(err)) } } } é‚£ä¹ˆä»€ä¹ˆæ ·çš„ä¼šè¯å¯ä»¥è¢«closeå‘¢ï¼Ÿ ShutdownOrNotifyæœ‰ä¸‰ç±»ï¼š client çŠ¶æ€å¤„äº ServerStatusInTransï¼› çŠ¶æ€å¤„äº connStatusReading ä»¥åŠå¤„äº connStatusDispatching åœ¨ clientConn.Run æ–¹æ³•ä¸­è¢«å›æ”¶ // ShutdownOrNotify will Shutdown this client connection, or do its best to notify. func (cc *clientConn) ShutdownOrNotify() bool { if (cc.ctx.Status() \u0026 mysql.ServerStatusInTrans) \u003e 0 { return false } // If the client connection status is reading, it's safe to shutdown it. if atomic.CompareAndSwapInt32(\u0026cc.status, connStatusReading, connStatusShutdown) { return true } // If the client connection status is dispatching, we can't shutdown it immediately, // so set the status to WaitShutdown as a notification, the loop in clientConn.Run // will detect it and then exit. atomic.StoreInt32(\u0026cc.status, connStatusWaitShutdown) return false } const ( connStatusDispatching int32 = iota connStatusReading connStatusShutdown // Closed by server. connStatusWaitShutdown // Notified by server to close. ) ","date":"2023-07-31","objectID":"/tidb-you-ya-guan-bi/:3:3","series":null,"tags":["tidb","database"],"title":"TiDB ä¼˜é›…å…³é—­","uri":"/tidb-you-ya-guan-bi/#connectioncount"},{"categories":["tidb"],"content":" cleanup()åœ¨ cleanup ä¸­çœ‹åˆ°äº† GracefulDown å’Œ TryGracefulDown ä¸¤ä¸ªæ–¹æ³• func cleanup(svr *server.Server, storage kv.Storage, dom *domain.Domain, graceful bool) { if graceful { done := make(chan struct{}) svr.GracefulDown(context.Background(), done) } else { svr.TryGracefulDown() } plugin.Shutdown(context.Background()) closeDomainAndStorage(storage, dom) disk.CleanUp() topsql.Close() } TryGracefulDownç ”ç©¶å‘ç°ä½¿ç”¨ SIGHUP ç»ˆæ­¢è¿›ç¨‹æ—¶ä½¿ç”¨ TryGracefulDown æ–¹æ³•ï¼Œå…¶ä»–æ—¶å€™ä½¿ç”¨ GracefulDownã€‚å¯¹æ¯” TryGracefulDown å’Œ GracefulDown å®ç°ï¼Œ TryGracefulDown åªæ˜¯å¤šä¸ª15sçš„è¶…æ—¶å¤„ç†ï¼Œåº•å±‚é€»è¾‘è¿˜æ˜¯ GracefulDown var gracefulCloseConnectionsTimeout = 15 * time.Second // TryGracefulDown will try to gracefully close all connection first with timeout. if timeout, will close all connection directly. func (s *Server) TryGracefulDown() { ctx, cancel := context.WithTimeout(context.Background(), gracefulCloseConnectionsTimeout) defer cancel() done := make(chan struct{}) go func() { s.GracefulDown(ctx, done) }() select { case \u003c-ctx.Done(): s.KillAllConnections() case \u003c-done: return } } GracefulDownä¸‹é¢æ˜¯ GracefulDown å®ç°ï¼ŒåŸæ¥åœ¨è¿™é‡Œä¼šé—´éš”1sï¼Œä¸€ç›´åˆ¤æ–­å®¢æˆ·ç«¯è¿æ¥æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨æ‰é€€å‡ºã€‚ // GracefulDown waits all clients to close. func (s *Server) GracefulDown(ctx context.Context, done chan struct{}) { logutil.Logger(ctx).Info(\"[server] graceful shutdown.\") metrics.ServerEventCounter.WithLabelValues(metrics.EventGracefulDown).Inc() count := s.ConnectionCount() for i := 0; count \u003e 0; i++ { s.kickIdleConnection() count = s.ConnectionCount() if count == 0 { break } // Print information for every 30s. if i%30 == 0 { logutil.Logger(ctx).Info(\"graceful shutdown...\", zap.Int(\"conn count\", count)) } ticker := time.After(time.Second) select { case \u003c-ctx.Done(): return case \u003c-ticker: } } close(done) } ConnectionCountåˆ¤æ–­è¿æ¥ä¸ªæ•°çš„é€»è¾‘ä¹Ÿå¾ˆç®€å•ï¼Œå°±æ˜¯å¯¹ç®—ä¸‹ s.clients çš„ length // ConnectionCount gets current connection count. func (s *Server) ConnectionCount() int { s.rwlock.RLock() cnt := len(s.clients) s.rwlock.RUnlock() return cnt } å…¶ä¸­è¿˜æœ‰ä¸€ä¸ªå¥‡æ€ªçš„å‡½æ•° kickIdleConnectionï¼Œè¿™ä¸ªæ˜¯åšä»€ä¹ˆçš„ï¼Ÿ kickIdleConnectionçœ‹é€»è¾‘æ˜¯æ”¶é›†å¯ä»¥è¢«closeçš„ä¼šè¯ç„¶åcloseæ‰ã€‚ func (s *Server) kickIdleConnection() { var conns []*clientConn s.rwlock.RLock() for _, cc := range s.clients { if cc.ShutdownOrNotify() { // Shutdowned conn will be closed by us, and notified conn will exist themselves. conns = append(conns, cc) } } s.rwlock.RUnlock() for _, cc := range conns { err := cc.Close() if err != nil { logutil.BgLogger().Error(\"close connection\", zap.Error(err)) } } } é‚£ä¹ˆä»€ä¹ˆæ ·çš„ä¼šè¯å¯ä»¥è¢«closeå‘¢ï¼Ÿ ShutdownOrNotifyæœ‰ä¸‰ç±»ï¼š client çŠ¶æ€å¤„äº ServerStatusInTransï¼› çŠ¶æ€å¤„äº connStatusReading ä»¥åŠå¤„äº connStatusDispatching åœ¨ clientConn.Run æ–¹æ³•ä¸­è¢«å›æ”¶ // ShutdownOrNotify will Shutdown this client connection, or do its best to notify. func (cc *clientConn) ShutdownOrNotify() bool { if (cc.ctx.Status() \u0026 mysql.ServerStatusInTrans) \u003e 0 { return false } // If the client connection status is reading, it's safe to shutdown it. if atomic.CompareAndSwapInt32(\u0026cc.status, connStatusReading, connStatusShutdown) { return true } // If the client connection status is dispatching, we can't shutdown it immediately, // so set the status to WaitShutdown as a notification, the loop in clientConn.Run // will detect it and then exit. atomic.StoreInt32(\u0026cc.status, connStatusWaitShutdown) return false } const ( connStatusDispatching int32 = iota connStatusReading connStatusShutdown // Closed by server. connStatusWaitShutdown // Notified by server to close. ) ","date":"2023-07-31","objectID":"/tidb-you-ya-guan-bi/:3:3","series":null,"tags":["tidb","database"],"title":"TiDB ä¼˜é›…å…³é—­","uri":"/tidb-you-ya-guan-bi/#kickidleconnection"},{"categories":["tidb"],"content":" cleanup()åœ¨ cleanup ä¸­çœ‹åˆ°äº† GracefulDown å’Œ TryGracefulDown ä¸¤ä¸ªæ–¹æ³• func cleanup(svr *server.Server, storage kv.Storage, dom *domain.Domain, graceful bool) { if graceful { done := make(chan struct{}) svr.GracefulDown(context.Background(), done) } else { svr.TryGracefulDown() } plugin.Shutdown(context.Background()) closeDomainAndStorage(storage, dom) disk.CleanUp() topsql.Close() } TryGracefulDownç ”ç©¶å‘ç°ä½¿ç”¨ SIGHUP ç»ˆæ­¢è¿›ç¨‹æ—¶ä½¿ç”¨ TryGracefulDown æ–¹æ³•ï¼Œå…¶ä»–æ—¶å€™ä½¿ç”¨ GracefulDownã€‚å¯¹æ¯” TryGracefulDown å’Œ GracefulDown å®ç°ï¼Œ TryGracefulDown åªæ˜¯å¤šä¸ª15sçš„è¶…æ—¶å¤„ç†ï¼Œåº•å±‚é€»è¾‘è¿˜æ˜¯ GracefulDown var gracefulCloseConnectionsTimeout = 15 * time.Second // TryGracefulDown will try to gracefully close all connection first with timeout. if timeout, will close all connection directly. func (s *Server) TryGracefulDown() { ctx, cancel := context.WithTimeout(context.Background(), gracefulCloseConnectionsTimeout) defer cancel() done := make(chan struct{}) go func() { s.GracefulDown(ctx, done) }() select { case \u003c-ctx.Done(): s.KillAllConnections() case \u003c-done: return } } GracefulDownä¸‹é¢æ˜¯ GracefulDown å®ç°ï¼ŒåŸæ¥åœ¨è¿™é‡Œä¼šé—´éš”1sï¼Œä¸€ç›´åˆ¤æ–­å®¢æˆ·ç«¯è¿æ¥æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨æ‰é€€å‡ºã€‚ // GracefulDown waits all clients to close. func (s *Server) GracefulDown(ctx context.Context, done chan struct{}) { logutil.Logger(ctx).Info(\"[server] graceful shutdown.\") metrics.ServerEventCounter.WithLabelValues(metrics.EventGracefulDown).Inc() count := s.ConnectionCount() for i := 0; count \u003e 0; i++ { s.kickIdleConnection() count = s.ConnectionCount() if count == 0 { break } // Print information for every 30s. if i%30 == 0 { logutil.Logger(ctx).Info(\"graceful shutdown...\", zap.Int(\"conn count\", count)) } ticker := time.After(time.Second) select { case \u003c-ctx.Done(): return case \u003c-ticker: } } close(done) } ConnectionCountåˆ¤æ–­è¿æ¥ä¸ªæ•°çš„é€»è¾‘ä¹Ÿå¾ˆç®€å•ï¼Œå°±æ˜¯å¯¹ç®—ä¸‹ s.clients çš„ length // ConnectionCount gets current connection count. func (s *Server) ConnectionCount() int { s.rwlock.RLock() cnt := len(s.clients) s.rwlock.RUnlock() return cnt } å…¶ä¸­è¿˜æœ‰ä¸€ä¸ªå¥‡æ€ªçš„å‡½æ•° kickIdleConnectionï¼Œè¿™ä¸ªæ˜¯åšä»€ä¹ˆçš„ï¼Ÿ kickIdleConnectionçœ‹é€»è¾‘æ˜¯æ”¶é›†å¯ä»¥è¢«closeçš„ä¼šè¯ç„¶åcloseæ‰ã€‚ func (s *Server) kickIdleConnection() { var conns []*clientConn s.rwlock.RLock() for _, cc := range s.clients { if cc.ShutdownOrNotify() { // Shutdowned conn will be closed by us, and notified conn will exist themselves. conns = append(conns, cc) } } s.rwlock.RUnlock() for _, cc := range conns { err := cc.Close() if err != nil { logutil.BgLogger().Error(\"close connection\", zap.Error(err)) } } } é‚£ä¹ˆä»€ä¹ˆæ ·çš„ä¼šè¯å¯ä»¥è¢«closeå‘¢ï¼Ÿ ShutdownOrNotifyæœ‰ä¸‰ç±»ï¼š client çŠ¶æ€å¤„äº ServerStatusInTransï¼› çŠ¶æ€å¤„äº connStatusReading ä»¥åŠå¤„äº connStatusDispatching åœ¨ clientConn.Run æ–¹æ³•ä¸­è¢«å›æ”¶ // ShutdownOrNotify will Shutdown this client connection, or do its best to notify. func (cc *clientConn) ShutdownOrNotify() bool { if (cc.ctx.Status() \u0026 mysql.ServerStatusInTrans) \u003e 0 { return false } // If the client connection status is reading, it's safe to shutdown it. if atomic.CompareAndSwapInt32(\u0026cc.status, connStatusReading, connStatusShutdown) { return true } // If the client connection status is dispatching, we can't shutdown it immediately, // so set the status to WaitShutdown as a notification, the loop in clientConn.Run // will detect it and then exit. atomic.StoreInt32(\u0026cc.status, connStatusWaitShutdown) return false } const ( connStatusDispatching int32 = iota connStatusReading connStatusShutdown // Closed by server. connStatusWaitShutdown // Notified by server to close. ) ","date":"2023-07-31","objectID":"/tidb-you-ya-guan-bi/:3:3","series":null,"tags":["tidb","database"],"title":"TiDB ä¼˜é›…å…³é—­","uri":"/tidb-you-ya-guan-bi/#shutdownornotify"},{"categories":["tidb"],"content":" ç ´æ¡ˆé€šè¿‡ä¸Šé¢çš„åˆ†æï¼Œæˆ‘ä»¬æ³¨æ„åˆ°äº†å¤„äº ServerStatusInTrans çŠ¶æ€çš„è¿æ¥ä¸ä¼šè¢«å…³é—­ï¼Œç„¶åè¿æ¥è¯¥èŠ‚ç‚¹æ‰§è¡Œshow processlistå‘ç°çš„ç¡®æœ‰ä¸ªå¤„äºäº‹åŠ¡ä¸­çš„ä¼šè¯ mysql\u003e show processlist; +---------------------+------+-----------------+------+---------+------+----------------------------+------------------+ | Id | User | Host | db | Command | Time | State | Info | +---------------------+------+-----------------+------+---------+------+----------------------------+------------------+ | 7794237818187809175 | root | 127.0.0.1:61293 | a | Query | 0 | in transaction; autocommit | show processlist | +---------------------+------+-----------------+------+---------+------+----------------------------+------------------+ 1 row in set (0.00 sec) å¹³æ—¶mysqlä½¿ç”¨çš„å¤šï¼Œmysqlåœ¨å…³é—­çš„æ—¶å€™ä¸ç®¡ä¼šè¯å¤„äºä»€ä¹ˆé˜¶æ®µï¼Œä¸ç®¡ä¸é¡¾ç›´æ¥åœæœï¼Œè€Œtidbçš„è¿™æ ·å¤„ç†ç€å®è®©æˆ‘æƒ³ä¸åˆ°ã€‚ ","date":"2023-07-31","objectID":"/tidb-you-ya-guan-bi/:4:0","series":null,"tags":["tidb","database"],"title":"TiDB ä¼˜é›…å…³é—­","uri":"/tidb-you-ya-guan-bi/#ç ´æ¡ˆ"},{"categories":["tidb"],"content":" æ€»ç»“æœ¬æ–‡ç®€çŸ­çš„åˆ†æäº†ä¸‹ tidb è¿›ç¨‹å…³é—­çš„å¤„ç†æµç¨‹ï¼Œæœ€ç»ˆå®šä½åˆ°è¿›ç¨‹æ²¡æœ‰åŠæ—¶å…³é—­çš„åŸå› ã€‚ å¯¹æ¯”äºmysqlçš„åœæœè¡Œä¸ºï¼Œè®©æˆ‘ä»¬å¯¹tidbçš„å¤„ç†æ–¹å¼æœ‰äº†ä¸ä¸€æ ·çš„ç†è§£ã€‚ å¯¹äº â€œgraceful-wait-before-shutdown å‚æ•°â€ã€â€œåœæœæ—¶ç­‰å¾…äº‹åŠ¡ç»“æŸçš„é€»è¾‘â€çš„ç¡®éœ€è¦åœ¨å®è·µä¸­æ‰èƒ½ç§¯ç´¯ã€‚ ","date":"2023-07-31","objectID":"/tidb-you-ya-guan-bi/:5:0","series":null,"tags":["tidb","database"],"title":"TiDB ä¼˜é›…å…³é—­","uri":"/tidb-you-ya-guan-bi/#æ€»ç»“"},{"categories":["tidb"],"content":" è¯´æ˜åœ¨ä¸Šç¯‡ã€ŠTiProxy å°é²œã€‹ä¸­åšäº†ä¸€äº›å®éªŒï¼Œæ¯”å¦‚åŠ å‡tidbèŠ‚ç‚¹åtiproxyå¯ä»¥åšåˆ°è‡ªåŠ¨è´Ÿè½½å‡è¡¡ï¼Œå¦‚æœé‡åˆ°ä¼šè¯æœ‰æœªæäº¤çš„äº‹åŠ¡åˆ™ç­‰å¾…äº‹åŠ¡ç»“æŸæ‰è¿ç§»ã€‚ æœ¬æ¬¡ä¸»è¦ç ”ç©¶è¿™æ ·çš„åŠŸèƒ½åœ¨tiproxyä¸­æ˜¯å¦‚ä½•å®ç°çš„ï¼Œæœ¬æ¬¡åˆ†äº«å†…å®¹ä¸»è¦ä¸ºä»¥ä¸‹å‡ éƒ¨åˆ†ï¼š tiproxyæ˜¯æ€ä¹ˆå‘ç°tidbï¼Ÿ tiproxyæ˜¯åœ¨tidbèŠ‚ç‚¹é—´è‡ªåŠ¨è´Ÿè½½å‡è¡¡çš„é€»è¾‘ï¼Ÿ åœ¨è‡ªåŠ¨è´Ÿè½½å‡è¡¡æ—¶tiproxyæ˜¯æ€ä¹ˆåšåˆ°ä¼˜é›…çš„sessionè¿ç§»ã€sessionä¸Šä¸‹æ–‡æ¢å¤ï¼Ÿ tiproxyåœ¨è‡ªåŠ¨è´Ÿè½½å‡è¡¡æœŸé—´é‡åˆ°å¤„äºæœªæäº¤äº‹åŠ¡çš„sessionæ˜¯æ€ä¹ˆç­‰å¾…ç»“æŸçš„ï¼Ÿ ","date":"2023-07-30","objectID":"/tiproxy-yuan-li-he-shi-xian/:1:0","series":null,"tags":["tidb","database","tiproxy"],"title":"TiProxy åŸç†å’Œå®ç°","uri":"/tiproxy-yuan-li-he-shi-xian/#è¯´æ˜"},{"categories":["tidb"],"content":" Tiproxy ä»‹ç»tiproxy åœ¨ 2022å¹´12æœˆ2æ—¥è¢«operatoræ”¯æŒ ç›¸å…³çš„è®¾è®¡æ–‡æ¡£å¯ä»¥ä»å®˜æ–¹ README å’Œ goole doc ä¸­æŸ¥çœ‹ è¿™ä¸ªæœ‰ä¸ªé‡è¦ç‰¹æ€§éœ€è¦è¯´æ˜ä¸‹ï¼š tiproxyç»„ä»¶ä¸ä¼šä¿å­˜è´¦å·çš„å¯†ç ï¼Œå› ä¸ºè¿™æ˜¯ä¸å®‰å…¨çš„è¡Œä¸ºï¼Œæ‰€ä»¥å½“è¿›è¡Œä¼šè¯è¿ç§»çš„æ—¶å€™ä½¿ç”¨çš„æ˜¯ session token è®¤è¯æ–¹å¼(ä¸‹æ–‡ä¼šæåˆ°è¿™ç§æ–¹å¼çš„å®ç°åŸç†)ã€‚ ","date":"2023-07-30","objectID":"/tiproxy-yuan-li-he-shi-xian/:2:0","series":null,"tags":["tidb","database","tiproxy"],"title":"TiProxy åŸç†å’Œå®ç°","uri":"/tiproxy-yuan-li-he-shi-xian/#tiproxy-ä»‹ç»"},{"categories":["tidb"],"content":" å£°æ˜ç›®å‰tiproxyè¿˜å¤„äºå®éªŒé˜¶æ®µã€åŠŸèƒ½è¿˜åœ¨æŒç»­å¼€å‘ä¸­ï¼Œæœ¬æ–‡è®²è¿°çš„å†…å®¹è·Ÿæ—¥åGAç‰ˆæœ¬å¯èƒ½å­˜åœ¨å·®å¼‚ï¼Œå±Šæ—¶è¯·å„ä½çœ‹å®˜ç•™æ„ã€‚ å¦å¤–æœ¬äººèƒ½åŠ›æœ‰é™ï¼Œåœ¨é˜…è¯»æºç ä¸­éš¾å…æœ‰ç†è§£ä¸åˆ°ä½çš„åœ°æ–¹ï¼Œå¦‚æœ‰å‘ç°æ¬¢è¿åœ¨è¯„è®ºåŒºæŒ‡æ­£ï¼Œæ„Ÿè°¢ã€‚ å¼€å§‹å‘è½¦ ","date":"2023-07-30","objectID":"/tiproxy-yuan-li-he-shi-xian/:3:0","series":null,"tags":["tidb","database","tiproxy"],"title":"TiProxy åŸç†å’Œå®ç°","uri":"/tiproxy-yuan-li-he-shi-xian/#å£°æ˜"},{"categories":["tidb"],"content":" åŸç†åˆ†æ","date":"2023-07-30","objectID":"/tiproxy-yuan-li-he-shi-xian/:4:0","series":null,"tags":["tidb","database","tiproxy"],"title":"TiProxy åŸç†å’Œå®ç°","uri":"/tiproxy-yuan-li-he-shi-xian/#åŸç†åˆ†æ"},{"categories":["tidb"],"content":" 1ã€tiproxyæ˜¯æ€ä¹ˆå‘ç°tidbï¼Ÿè·å–tidbæ‹“æ‰‘æœ€æ ¸å¿ƒã€ç®€åŒ–åçš„ä»£ç å¦‚ä¸‹ï¼Œå…¶å®å°±æ˜¯ä½¿ç”¨etcdCli.Getè·å–ä¿¡æ¯ // ä» etcd è·å– tidb æ‹“æ‰‘ è·¯å¾„ /topology/tidb/\u003cip:port\u003e/info /topology/tidb/\u003cip:port\u003e/ttl func (is *InfoSyncer) GetTiDBTopology(ctx context.Context) (map[string]*TiDBInfo, error) { res, err := is.etcdCli.Get(ctx, tidbinfo.TopologyInformationPath, clientv3.WithPrefix()) infos := make(map[string]*TiDBInfo, len(res.Kvs)/2) for _, kv := range res.Kvs { var ttl, addr string var topology *tidbinfo.TopologyInfo key := hack.String(kv.Key) switch { case strings.HasSuffix(key, ttlSuffix): addr = key[len(tidbinfo.TopologyInformationPath)+1 : len(key)-len(ttlSuffix)-1] ttl = hack.String(kv.Value) case strings.HasSuffix(key, infoSuffix): addr = key[len(tidbinfo.TopologyInformationPath)+1 : len(key)-len(infoSuffix)-1] json.Unmarshal(kv.Value, \u0026topology) default: continue } info := infos[addr] if len(ttl) \u003e 0 { info.TTL = hack.String(kv.Value) } else { info.TopologyInfo = topology } } return infos, nil } è¿™ä¸ªå‡½æ•°æ˜¯æ€ä¹ˆè¢«tiproxyç”¨èµ·æ¥çš„å‘¢ï¼Ÿ å…¶å®åœ¨æ¯ä¸ªproxyå¯åŠ¨æ—¶åéƒ½ä¼šå¼€å¯ä¸€ä¸ªBackendObserveråç¨‹ï¼Œè¿™ä¸ªåç¨‹ä¼šåšä¸‰ä»¶äº‹ï¼š func (bo *BackendObserver) observe(ctx context.Context) { for ctx.Err() == nil { // è·å– backendInfo, err := bo.fetcher.GetBackendList(ctx) // æ£€æŸ¥ bhMap := bo.checkHealth(ctx, backendInfo) // é€šçŸ¥ bo.notifyIfChanged(bhMap) select { case \u003c-time.After(bo.healthCheckConfig.Interval): // é—´éš”3ç§’ case \u003c-bo.refreshChan: case \u003c-ctx.Done(): return } } } ç¬¬ä¸€æ­¥è·å–ï¼šä»etcdè·å–tidbæ‹“æ‰‘ï¼›ä»£ç è§ä¸Šï¼› ç¬¬äºŒæ­¥æ£€æŸ¥ï¼šåˆ¤æ–­è·å–åˆ°tidbèŠ‚ç‚¹æ˜¯å¦å¯ä»¥è¿é€šã€è®¿é—®ï¼Œç»™æ¯ä¸ªèŠ‚ç‚¹è®¾ç½®StatusHealthyæˆ–è€…StatusCannotConnectçŠ¶æ€ func (bo *BackendObserver) checkHealth(ctx context.Context, backends map[string]*BackendInfo) map[string]*backendHealth { curBackendHealth := make(map[string]*backendHealth, len(backends)) for addr, info := range backends { bh := \u0026backendHealth{ status: StatusHealthy, } curBackendHealth[addr] = bh // http æœåŠ¡æ£€æŸ¥ if info != nil \u0026\u0026 len(info.IP) \u003e 0 { schema := \"http\" httpCli := *bo.httpCli httpCli.Timeout = bo.healthCheckConfig.DialTimeout url := fmt.Sprintf(\"%s://%s:%d%s\", schema, info.IP, info.StatusPort, statusPathSuffix) resp, err := httpCli.Get(url) if err != nil { bh.status = StatusCannotConnect bh.pingErr = errors.Wrapf(err, \"connect status port failed\") continue } } // tcp æœåŠ¡æ£€æŸ¥ conn, err := net.DialTimeout(\"tcp\", addr, bo.healthCheckConfig.DialTimeout) if err != nil { bh.status = StatusCannotConnect bh.pingErr = errors.Wrapf(err, \"connect sql port failed\") } } return curBackendHealth } ç¬¬ä¸‰æ­¥é€šçŸ¥ï¼šå°†æ£€æŸ¥åçš„ backends åˆ—è¡¨è·Ÿå†…å­˜ä¸­ç¼“å­˜çš„ backends è¿›è¡Œæ¯”è¾ƒï¼Œå°†å˜åŠ¨çš„ updatedBackends è¿›è¡Œé€šçŸ¥ // notifyIfChanged æ ¹æ®æœ€æ–°çš„ tidb æ‹“æ‰‘ bhMap ä¸ä¹‹å‰çš„ tidb æ‹“æ‰‘ bo.curBackendInfo è¿›è¡Œæ¯”è¾ƒ // - åœ¨ bo.curBackendInfo ä¸­ä½†æ˜¯ä¸åœ¨ bhMap ä¸­ï¼šè¯´æ˜ tidb èŠ‚ç‚¹å¤±è”ï¼Œéœ€è¦è®°å½•ä¸‹ // - åœ¨ bo.curBackendInfo ä¸­ä¹Ÿåœ¨ bhMap ä¸­ï¼Œä½†æ˜¯æœ€æ–°çš„çŠ¶æ€ä¸æ˜¯ StatusHealthyï¼šä¹Ÿéœ€è¦è®°å½•ä¸‹ // - åœ¨ bhMap ä¸­ä½†æ˜¯ä¸åœ¨ bo.curBackendInfo ä¸­ï¼šè¯´æ˜æ˜¯æ–°å¢ tidb èŠ‚ç‚¹ï¼Œéœ€è¦è®°å½•ä¸‹ func (bo *BackendObserver) notifyIfChanged(bhMap map[string]*backendHealth) { updatedBackends := make(map[string]*backendHealth) for addr, lastHealth := range bo.curBackendInfo { if lastHealth.status == StatusHealthy { if newHealth, ok := bhMap[addr]; !ok { updatedBackends[addr] = \u0026backendHealth{ status: StatusCannotConnect, pingErr: errors.New(\"removed from backend list\"), } updateBackendStatusMetrics(addr, lastHealth.status, StatusCannotConnect) } else if newHealth.status != StatusHealthy { updatedBackends[addr] = newHealth updateBackendStatusMetrics(addr, lastHealth.status, newHealth.status) } } } for addr, newHealth := range bhMap { if newHealth.status == StatusHealthy { lastHealth, ok := bo.curBackendInfo[addr] if !ok { lastHealth = \u0026backendHealth{ status: StatusCannotConnect, } } if lastHealth.status != StatusHealthy { updatedBackends[addr] = newHealth updateBackendStatusMetrics(addr, lastHealth.status, newHealth.status) } else if lastHealth.serverVersion != newHealth.serverVersion { // Not possible here: the backend finishes upgrading between two health checks. updatedBackends[addr] = newHealth } } } // Notify it even when the updatedBackends is empty, in order to clear the last error. bo.eventReceiver.OnBackendChanged(updatedBackends, nil) bo.curBackendInfo = bhMap } é€šè¿‡ä¸Š","date":"2023-07-30","objectID":"/tiproxy-yuan-li-he-shi-xian/:4:1","series":null,"tags":["tidb","database","tiproxy"],"title":"TiProxy åŸç†å’Œå®ç°","uri":"/tiproxy-yuan-li-he-shi-xian/#1tiproxyæ˜¯æ€ä¹ˆå‘ç°tidb"},{"categories":["tidb"],"content":" 1ã€tiproxyæ˜¯æ€ä¹ˆå‘ç°tidbï¼Ÿè·å–tidbæ‹“æ‰‘æœ€æ ¸å¿ƒã€ç®€åŒ–åçš„ä»£ç å¦‚ä¸‹ï¼Œå…¶å®å°±æ˜¯ä½¿ç”¨etcdCli.Getè·å–ä¿¡æ¯ // ä» etcd è·å– tidb æ‹“æ‰‘ è·¯å¾„ /topology/tidb//info /topology/tidb//ttl func (is *InfoSyncer) GetTiDBTopology(ctx context.Context) (map[string]*TiDBInfo, error) { res, err := is.etcdCli.Get(ctx, tidbinfo.TopologyInformationPath, clientv3.WithPrefix()) infos := make(map[string]*TiDBInfo, len(res.Kvs)/2) for _, kv := range res.Kvs { var ttl, addr string var topology *tidbinfo.TopologyInfo key := hack.String(kv.Key) switch { case strings.HasSuffix(key, ttlSuffix): addr = key[len(tidbinfo.TopologyInformationPath)+1 : len(key)-len(ttlSuffix)-1] ttl = hack.String(kv.Value) case strings.HasSuffix(key, infoSuffix): addr = key[len(tidbinfo.TopologyInformationPath)+1 : len(key)-len(infoSuffix)-1] json.Unmarshal(kv.Value, \u0026topology) default: continue } info := infos[addr] if len(ttl) \u003e 0 { info.TTL = hack.String(kv.Value) } else { info.TopologyInfo = topology } } return infos, nil } è¿™ä¸ªå‡½æ•°æ˜¯æ€ä¹ˆè¢«tiproxyç”¨èµ·æ¥çš„å‘¢ï¼Ÿ å…¶å®åœ¨æ¯ä¸ªproxyå¯åŠ¨æ—¶åéƒ½ä¼šå¼€å¯ä¸€ä¸ªBackendObserveråç¨‹ï¼Œè¿™ä¸ªåç¨‹ä¼šåšä¸‰ä»¶äº‹ï¼š func (bo *BackendObserver) observe(ctx context.Context) { for ctx.Err() == nil { // è·å– backendInfo, err := bo.fetcher.GetBackendList(ctx) // æ£€æŸ¥ bhMap := bo.checkHealth(ctx, backendInfo) // é€šçŸ¥ bo.notifyIfChanged(bhMap) select { case \u003c-time.After(bo.healthCheckConfig.Interval): // é—´éš”3ç§’ case \u003c-bo.refreshChan: case \u003c-ctx.Done(): return } } } ç¬¬ä¸€æ­¥è·å–ï¼šä»etcdè·å–tidbæ‹“æ‰‘ï¼›ä»£ç è§ä¸Šï¼› ç¬¬äºŒæ­¥æ£€æŸ¥ï¼šåˆ¤æ–­è·å–åˆ°tidbèŠ‚ç‚¹æ˜¯å¦å¯ä»¥è¿é€šã€è®¿é—®ï¼Œç»™æ¯ä¸ªèŠ‚ç‚¹è®¾ç½®StatusHealthyæˆ–è€…StatusCannotConnectçŠ¶æ€ func (bo *BackendObserver) checkHealth(ctx context.Context, backends map[string]*BackendInfo) map[string]*backendHealth { curBackendHealth := make(map[string]*backendHealth, len(backends)) for addr, info := range backends { bh := \u0026backendHealth{ status: StatusHealthy, } curBackendHealth[addr] = bh // http æœåŠ¡æ£€æŸ¥ if info != nil \u0026\u0026 len(info.IP) \u003e 0 { schema := \"http\" httpCli := *bo.httpCli httpCli.Timeout = bo.healthCheckConfig.DialTimeout url := fmt.Sprintf(\"%s://%s:%d%s\", schema, info.IP, info.StatusPort, statusPathSuffix) resp, err := httpCli.Get(url) if err != nil { bh.status = StatusCannotConnect bh.pingErr = errors.Wrapf(err, \"connect status port failed\") continue } } // tcp æœåŠ¡æ£€æŸ¥ conn, err := net.DialTimeout(\"tcp\", addr, bo.healthCheckConfig.DialTimeout) if err != nil { bh.status = StatusCannotConnect bh.pingErr = errors.Wrapf(err, \"connect sql port failed\") } } return curBackendHealth } ç¬¬ä¸‰æ­¥é€šçŸ¥ï¼šå°†æ£€æŸ¥åçš„ backends åˆ—è¡¨è·Ÿå†…å­˜ä¸­ç¼“å­˜çš„ backends è¿›è¡Œæ¯”è¾ƒï¼Œå°†å˜åŠ¨çš„ updatedBackends è¿›è¡Œé€šçŸ¥ // notifyIfChanged æ ¹æ®æœ€æ–°çš„ tidb æ‹“æ‰‘ bhMap ä¸ä¹‹å‰çš„ tidb æ‹“æ‰‘ bo.curBackendInfo è¿›è¡Œæ¯”è¾ƒ // - åœ¨ bo.curBackendInfo ä¸­ä½†æ˜¯ä¸åœ¨ bhMap ä¸­ï¼šè¯´æ˜ tidb èŠ‚ç‚¹å¤±è”ï¼Œéœ€è¦è®°å½•ä¸‹ // - åœ¨ bo.curBackendInfo ä¸­ä¹Ÿåœ¨ bhMap ä¸­ï¼Œä½†æ˜¯æœ€æ–°çš„çŠ¶æ€ä¸æ˜¯ StatusHealthyï¼šä¹Ÿéœ€è¦è®°å½•ä¸‹ // - åœ¨ bhMap ä¸­ä½†æ˜¯ä¸åœ¨ bo.curBackendInfo ä¸­ï¼šè¯´æ˜æ˜¯æ–°å¢ tidb èŠ‚ç‚¹ï¼Œéœ€è¦è®°å½•ä¸‹ func (bo *BackendObserver) notifyIfChanged(bhMap map[string]*backendHealth) { updatedBackends := make(map[string]*backendHealth) for addr, lastHealth := range bo.curBackendInfo { if lastHealth.status == StatusHealthy { if newHealth, ok := bhMap[addr]; !ok { updatedBackends[addr] = \u0026backendHealth{ status: StatusCannotConnect, pingErr: errors.New(\"removed from backend list\"), } updateBackendStatusMetrics(addr, lastHealth.status, StatusCannotConnect) } else if newHealth.status != StatusHealthy { updatedBackends[addr] = newHealth updateBackendStatusMetrics(addr, lastHealth.status, newHealth.status) } } } for addr, newHealth := range bhMap { if newHealth.status == StatusHealthy { lastHealth, ok := bo.curBackendInfo[addr] if !ok { lastHealth = \u0026backendHealth{ status: StatusCannotConnect, } } if lastHealth.status != StatusHealthy { updatedBackends[addr] = newHealth updateBackendStatusMetrics(addr, lastHealth.status, newHealth.status) } else if lastHealth.serverVersion != newHealth.serverVersion { // Not possible here: the backend finishes upgrading between two health checks. updatedBackends[addr] = newHealth } } } // Notify it even when the updatedBackends is empty, in order to clear the last error. bo.eventReceiver.OnBackendChanged(updatedBackends, nil) bo.curBackendInfo = bhMap } é€šè¿‡ä¸Š","date":"2023-07-30","objectID":"/tiproxy-yuan-li-he-shi-xian/:4:1","series":null,"tags":["tidb","database","tiproxy"],"title":"TiProxy åŸç†å’Œå®ç°","uri":"/tiproxy-yuan-li-he-shi-xian/#ç¬¬ä¸€æ­¥è·å–"},{"categories":["tidb"],"content":" 1ã€tiproxyæ˜¯æ€ä¹ˆå‘ç°tidbï¼Ÿè·å–tidbæ‹“æ‰‘æœ€æ ¸å¿ƒã€ç®€åŒ–åçš„ä»£ç å¦‚ä¸‹ï¼Œå…¶å®å°±æ˜¯ä½¿ç”¨etcdCli.Getè·å–ä¿¡æ¯ // ä» etcd è·å– tidb æ‹“æ‰‘ è·¯å¾„ /topology/tidb//info /topology/tidb//ttl func (is *InfoSyncer) GetTiDBTopology(ctx context.Context) (map[string]*TiDBInfo, error) { res, err := is.etcdCli.Get(ctx, tidbinfo.TopologyInformationPath, clientv3.WithPrefix()) infos := make(map[string]*TiDBInfo, len(res.Kvs)/2) for _, kv := range res.Kvs { var ttl, addr string var topology *tidbinfo.TopologyInfo key := hack.String(kv.Key) switch { case strings.HasSuffix(key, ttlSuffix): addr = key[len(tidbinfo.TopologyInformationPath)+1 : len(key)-len(ttlSuffix)-1] ttl = hack.String(kv.Value) case strings.HasSuffix(key, infoSuffix): addr = key[len(tidbinfo.TopologyInformationPath)+1 : len(key)-len(infoSuffix)-1] json.Unmarshal(kv.Value, \u0026topology) default: continue } info := infos[addr] if len(ttl) \u003e 0 { info.TTL = hack.String(kv.Value) } else { info.TopologyInfo = topology } } return infos, nil } è¿™ä¸ªå‡½æ•°æ˜¯æ€ä¹ˆè¢«tiproxyç”¨èµ·æ¥çš„å‘¢ï¼Ÿ å…¶å®åœ¨æ¯ä¸ªproxyå¯åŠ¨æ—¶åéƒ½ä¼šå¼€å¯ä¸€ä¸ªBackendObserveråç¨‹ï¼Œè¿™ä¸ªåç¨‹ä¼šåšä¸‰ä»¶äº‹ï¼š func (bo *BackendObserver) observe(ctx context.Context) { for ctx.Err() == nil { // è·å– backendInfo, err := bo.fetcher.GetBackendList(ctx) // æ£€æŸ¥ bhMap := bo.checkHealth(ctx, backendInfo) // é€šçŸ¥ bo.notifyIfChanged(bhMap) select { case \u003c-time.After(bo.healthCheckConfig.Interval): // é—´éš”3ç§’ case \u003c-bo.refreshChan: case \u003c-ctx.Done(): return } } } ç¬¬ä¸€æ­¥è·å–ï¼šä»etcdè·å–tidbæ‹“æ‰‘ï¼›ä»£ç è§ä¸Šï¼› ç¬¬äºŒæ­¥æ£€æŸ¥ï¼šåˆ¤æ–­è·å–åˆ°tidbèŠ‚ç‚¹æ˜¯å¦å¯ä»¥è¿é€šã€è®¿é—®ï¼Œç»™æ¯ä¸ªèŠ‚ç‚¹è®¾ç½®StatusHealthyæˆ–è€…StatusCannotConnectçŠ¶æ€ func (bo *BackendObserver) checkHealth(ctx context.Context, backends map[string]*BackendInfo) map[string]*backendHealth { curBackendHealth := make(map[string]*backendHealth, len(backends)) for addr, info := range backends { bh := \u0026backendHealth{ status: StatusHealthy, } curBackendHealth[addr] = bh // http æœåŠ¡æ£€æŸ¥ if info != nil \u0026\u0026 len(info.IP) \u003e 0 { schema := \"http\" httpCli := *bo.httpCli httpCli.Timeout = bo.healthCheckConfig.DialTimeout url := fmt.Sprintf(\"%s://%s:%d%s\", schema, info.IP, info.StatusPort, statusPathSuffix) resp, err := httpCli.Get(url) if err != nil { bh.status = StatusCannotConnect bh.pingErr = errors.Wrapf(err, \"connect status port failed\") continue } } // tcp æœåŠ¡æ£€æŸ¥ conn, err := net.DialTimeout(\"tcp\", addr, bo.healthCheckConfig.DialTimeout) if err != nil { bh.status = StatusCannotConnect bh.pingErr = errors.Wrapf(err, \"connect sql port failed\") } } return curBackendHealth } ç¬¬ä¸‰æ­¥é€šçŸ¥ï¼šå°†æ£€æŸ¥åçš„ backends åˆ—è¡¨è·Ÿå†…å­˜ä¸­ç¼“å­˜çš„ backends è¿›è¡Œæ¯”è¾ƒï¼Œå°†å˜åŠ¨çš„ updatedBackends è¿›è¡Œé€šçŸ¥ // notifyIfChanged æ ¹æ®æœ€æ–°çš„ tidb æ‹“æ‰‘ bhMap ä¸ä¹‹å‰çš„ tidb æ‹“æ‰‘ bo.curBackendInfo è¿›è¡Œæ¯”è¾ƒ // - åœ¨ bo.curBackendInfo ä¸­ä½†æ˜¯ä¸åœ¨ bhMap ä¸­ï¼šè¯´æ˜ tidb èŠ‚ç‚¹å¤±è”ï¼Œéœ€è¦è®°å½•ä¸‹ // - åœ¨ bo.curBackendInfo ä¸­ä¹Ÿåœ¨ bhMap ä¸­ï¼Œä½†æ˜¯æœ€æ–°çš„çŠ¶æ€ä¸æ˜¯ StatusHealthyï¼šä¹Ÿéœ€è¦è®°å½•ä¸‹ // - åœ¨ bhMap ä¸­ä½†æ˜¯ä¸åœ¨ bo.curBackendInfo ä¸­ï¼šè¯´æ˜æ˜¯æ–°å¢ tidb èŠ‚ç‚¹ï¼Œéœ€è¦è®°å½•ä¸‹ func (bo *BackendObserver) notifyIfChanged(bhMap map[string]*backendHealth) { updatedBackends := make(map[string]*backendHealth) for addr, lastHealth := range bo.curBackendInfo { if lastHealth.status == StatusHealthy { if newHealth, ok := bhMap[addr]; !ok { updatedBackends[addr] = \u0026backendHealth{ status: StatusCannotConnect, pingErr: errors.New(\"removed from backend list\"), } updateBackendStatusMetrics(addr, lastHealth.status, StatusCannotConnect) } else if newHealth.status != StatusHealthy { updatedBackends[addr] = newHealth updateBackendStatusMetrics(addr, lastHealth.status, newHealth.status) } } } for addr, newHealth := range bhMap { if newHealth.status == StatusHealthy { lastHealth, ok := bo.curBackendInfo[addr] if !ok { lastHealth = \u0026backendHealth{ status: StatusCannotConnect, } } if lastHealth.status != StatusHealthy { updatedBackends[addr] = newHealth updateBackendStatusMetrics(addr, lastHealth.status, newHealth.status) } else if lastHealth.serverVersion != newHealth.serverVersion { // Not possible here: the backend finishes upgrading between two health checks. updatedBackends[addr] = newHealth } } } // Notify it even when the updatedBackends is empty, in order to clear the last error. bo.eventReceiver.OnBackendChanged(updatedBackends, nil) bo.curBackendInfo = bhMap } é€šè¿‡ä¸Š","date":"2023-07-30","objectID":"/tiproxy-yuan-li-he-shi-xian/:4:1","series":null,"tags":["tidb","database","tiproxy"],"title":"TiProxy åŸç†å’Œå®ç°","uri":"/tiproxy-yuan-li-he-shi-xian/#ç¬¬äºŒæ­¥æ£€æŸ¥"},{"categories":["tidb"],"content":" 1ã€tiproxyæ˜¯æ€ä¹ˆå‘ç°tidbï¼Ÿè·å–tidbæ‹“æ‰‘æœ€æ ¸å¿ƒã€ç®€åŒ–åçš„ä»£ç å¦‚ä¸‹ï¼Œå…¶å®å°±æ˜¯ä½¿ç”¨etcdCli.Getè·å–ä¿¡æ¯ // ä» etcd è·å– tidb æ‹“æ‰‘ è·¯å¾„ /topology/tidb//info /topology/tidb//ttl func (is *InfoSyncer) GetTiDBTopology(ctx context.Context) (map[string]*TiDBInfo, error) { res, err := is.etcdCli.Get(ctx, tidbinfo.TopologyInformationPath, clientv3.WithPrefix()) infos := make(map[string]*TiDBInfo, len(res.Kvs)/2) for _, kv := range res.Kvs { var ttl, addr string var topology *tidbinfo.TopologyInfo key := hack.String(kv.Key) switch { case strings.HasSuffix(key, ttlSuffix): addr = key[len(tidbinfo.TopologyInformationPath)+1 : len(key)-len(ttlSuffix)-1] ttl = hack.String(kv.Value) case strings.HasSuffix(key, infoSuffix): addr = key[len(tidbinfo.TopologyInformationPath)+1 : len(key)-len(infoSuffix)-1] json.Unmarshal(kv.Value, \u0026topology) default: continue } info := infos[addr] if len(ttl) \u003e 0 { info.TTL = hack.String(kv.Value) } else { info.TopologyInfo = topology } } return infos, nil } è¿™ä¸ªå‡½æ•°æ˜¯æ€ä¹ˆè¢«tiproxyç”¨èµ·æ¥çš„å‘¢ï¼Ÿ å…¶å®åœ¨æ¯ä¸ªproxyå¯åŠ¨æ—¶åéƒ½ä¼šå¼€å¯ä¸€ä¸ªBackendObserveråç¨‹ï¼Œè¿™ä¸ªåç¨‹ä¼šåšä¸‰ä»¶äº‹ï¼š func (bo *BackendObserver) observe(ctx context.Context) { for ctx.Err() == nil { // è·å– backendInfo, err := bo.fetcher.GetBackendList(ctx) // æ£€æŸ¥ bhMap := bo.checkHealth(ctx, backendInfo) // é€šçŸ¥ bo.notifyIfChanged(bhMap) select { case \u003c-time.After(bo.healthCheckConfig.Interval): // é—´éš”3ç§’ case \u003c-bo.refreshChan: case \u003c-ctx.Done(): return } } } ç¬¬ä¸€æ­¥è·å–ï¼šä»etcdè·å–tidbæ‹“æ‰‘ï¼›ä»£ç è§ä¸Šï¼› ç¬¬äºŒæ­¥æ£€æŸ¥ï¼šåˆ¤æ–­è·å–åˆ°tidbèŠ‚ç‚¹æ˜¯å¦å¯ä»¥è¿é€šã€è®¿é—®ï¼Œç»™æ¯ä¸ªèŠ‚ç‚¹è®¾ç½®StatusHealthyæˆ–è€…StatusCannotConnectçŠ¶æ€ func (bo *BackendObserver) checkHealth(ctx context.Context, backends map[string]*BackendInfo) map[string]*backendHealth { curBackendHealth := make(map[string]*backendHealth, len(backends)) for addr, info := range backends { bh := \u0026backendHealth{ status: StatusHealthy, } curBackendHealth[addr] = bh // http æœåŠ¡æ£€æŸ¥ if info != nil \u0026\u0026 len(info.IP) \u003e 0 { schema := \"http\" httpCli := *bo.httpCli httpCli.Timeout = bo.healthCheckConfig.DialTimeout url := fmt.Sprintf(\"%s://%s:%d%s\", schema, info.IP, info.StatusPort, statusPathSuffix) resp, err := httpCli.Get(url) if err != nil { bh.status = StatusCannotConnect bh.pingErr = errors.Wrapf(err, \"connect status port failed\") continue } } // tcp æœåŠ¡æ£€æŸ¥ conn, err := net.DialTimeout(\"tcp\", addr, bo.healthCheckConfig.DialTimeout) if err != nil { bh.status = StatusCannotConnect bh.pingErr = errors.Wrapf(err, \"connect sql port failed\") } } return curBackendHealth } ç¬¬ä¸‰æ­¥é€šçŸ¥ï¼šå°†æ£€æŸ¥åçš„ backends åˆ—è¡¨è·Ÿå†…å­˜ä¸­ç¼“å­˜çš„ backends è¿›è¡Œæ¯”è¾ƒï¼Œå°†å˜åŠ¨çš„ updatedBackends è¿›è¡Œé€šçŸ¥ // notifyIfChanged æ ¹æ®æœ€æ–°çš„ tidb æ‹“æ‰‘ bhMap ä¸ä¹‹å‰çš„ tidb æ‹“æ‰‘ bo.curBackendInfo è¿›è¡Œæ¯”è¾ƒ // - åœ¨ bo.curBackendInfo ä¸­ä½†æ˜¯ä¸åœ¨ bhMap ä¸­ï¼šè¯´æ˜ tidb èŠ‚ç‚¹å¤±è”ï¼Œéœ€è¦è®°å½•ä¸‹ // - åœ¨ bo.curBackendInfo ä¸­ä¹Ÿåœ¨ bhMap ä¸­ï¼Œä½†æ˜¯æœ€æ–°çš„çŠ¶æ€ä¸æ˜¯ StatusHealthyï¼šä¹Ÿéœ€è¦è®°å½•ä¸‹ // - åœ¨ bhMap ä¸­ä½†æ˜¯ä¸åœ¨ bo.curBackendInfo ä¸­ï¼šè¯´æ˜æ˜¯æ–°å¢ tidb èŠ‚ç‚¹ï¼Œéœ€è¦è®°å½•ä¸‹ func (bo *BackendObserver) notifyIfChanged(bhMap map[string]*backendHealth) { updatedBackends := make(map[string]*backendHealth) for addr, lastHealth := range bo.curBackendInfo { if lastHealth.status == StatusHealthy { if newHealth, ok := bhMap[addr]; !ok { updatedBackends[addr] = \u0026backendHealth{ status: StatusCannotConnect, pingErr: errors.New(\"removed from backend list\"), } updateBackendStatusMetrics(addr, lastHealth.status, StatusCannotConnect) } else if newHealth.status != StatusHealthy { updatedBackends[addr] = newHealth updateBackendStatusMetrics(addr, lastHealth.status, newHealth.status) } } } for addr, newHealth := range bhMap { if newHealth.status == StatusHealthy { lastHealth, ok := bo.curBackendInfo[addr] if !ok { lastHealth = \u0026backendHealth{ status: StatusCannotConnect, } } if lastHealth.status != StatusHealthy { updatedBackends[addr] = newHealth updateBackendStatusMetrics(addr, lastHealth.status, newHealth.status) } else if lastHealth.serverVersion != newHealth.serverVersion { // Not possible here: the backend finishes upgrading between two health checks. updatedBackends[addr] = newHealth } } } // Notify it even when the updatedBackends is empty, in order to clear the last error. bo.eventReceiver.OnBackendChanged(updatedBackends, nil) bo.curBackendInfo = bhMap } é€šè¿‡ä¸Š","date":"2023-07-30","objectID":"/tiproxy-yuan-li-he-shi-xian/:4:1","series":null,"tags":["tidb","database","tiproxy"],"title":"TiProxy åŸç†å’Œå®ç°","uri":"/tiproxy-yuan-li-he-shi-xian/#ç¬¬ä¸‰æ­¥é€šçŸ¥"},{"categories":["tidb"],"content":" 2ã€tiproxyæ˜¯åœ¨tidbèŠ‚ç‚¹é—´è‡ªåŠ¨è´Ÿè½½å‡è¡¡çš„é€»è¾‘ï¼Ÿæ­¤å¤„è‡ªåŠ¨è´Ÿè½½çš„è¯­ä¹‰æ˜¯ï¼šå°†å“ªä¸ª backend çš„å“ªä¸ª connect è¿ç§»åˆ°å“ªä¸ª backend ä¸Šã€‚è¿™å°±è¦è§£å†³ backend æŒ‘é€‰å’Œ connect æŒ‘é€‰é—®é¢˜ã€‚ è¿™ä¸ªé—®é¢˜çš„è§£å†³åŠæ³•æ˜¯åœ¨ ScoreBasedRouter æ¨¡å—å®Œæˆã€‚è¿™ä¸ªæ¨¡å—æœ‰3ä¸ª func å’Œä¸Šè¿°è§£é‡Šç›¸å…³ï¼š type ScoreBasedRouter struct { sync.Mutex // A list of *backendWrapper. The backends are in descending order of scores. backends *glist.List[*backendWrapper] // ... } // è¢« BackendObserver è°ƒç”¨ï¼Œä¼ æ¥çš„ backends ä¼šåˆå¹¶åˆ° ScoreBasedRouter::backends ä¸­ func (router *ScoreBasedRouter) OnBackendChanged(backends map[string]*backendHealth, err error) {} // é€šè¿‡æ¯”è¾ƒ backend åˆ†æ•°æ–¹å¼è°ƒæ•´ ScoreBasedRouter::backends ä¸­çš„ä½ç½® func (router *ScoreBasedRouter) adjustBackendList(be *glist.Element[*backendWrapper]) {} // åç¨‹æ–¹å¼è¿è¡Œï¼Œåšè´Ÿè½½å‡è¡¡å¤„ç† func (router *ScoreBasedRouter) rebalanceLoop(ctx context.Context) {} OnBackendChanged æ˜¯æš´éœ²ç»™ BackendObserver æ¨¡å—çš„ä¸€ä¸ªæ¥å£ï¼Œ ç”¨æ¥åŒæ­¥ä» etcd å‘ç°çš„ tidb ä¿¡æ¯ï¼Œè¿™ä¸ªé€»è¾‘ä¸å¤æ‚ï¼Œè¯¦ç»†å¯è‡ªè¡Œé˜…è¯»æºç ã€‚è¿™ä¸ªæ–¹æ³•æ˜¯é—®é¢˜ä¸€ç§æåˆ°çš„â€œé€šçŸ¥â€æ¥æ”¶å¤„ã€‚ adjustBackendList æœ¬è´¨å°±æ˜¯è°ƒæ•´ item åœ¨åŒå‘é“¾è¡¨ä¸­çš„ä½ç½®ï¼Œè¿™ä¸ªä¹Ÿä¸å¤æ‚ã€‚ ä¸‹é¢é‡ç‚¹è¯´ä¸‹ rebalanceLoop çš„é€»è¾‘ï¼Œè¿™é‡Œæ¶‰åŠåˆ°\"å°†å“ªä¸ª backend çš„å“ªä¸ª connect è¿ç§»åˆ°å“ªä¸ª backend ä¸Š\"çš„é—®é¢˜ã€‚ // rebalanceLoop è®¡ç®—é—´éš”æ˜¯ 10 msï¼Œæ¯æ¬¡æœ€å¤šå¤„ç† 10 ä¸ªè¿æ¥(é˜²æ­¢åç«¯å‡ºç°æŠ–åŠ¨) // - backends çš„å˜åŒ–æ˜¯é€šè¿‡ OnBackendChanged ä¿®æ”¹çš„ï¼Œè¿æ¥å¹³è¡¡æ˜¯ rebalanceLoop å‡½æ•°åšçš„ï¼Œä¸¤è€…ä¸ºäº†ä¿è¯å¹¶å‘ä½¿ç”¨äº† sync.Mutex func (router *ScoreBasedRouter) rebalanceLoop(ctx context.Context) { for { router.rebalance(rebalanceConnsPerLoop) select { case \u003c-ctx.Done(): return case \u003c-time.After(rebalanceInterval): } } } // rebalance func (router *ScoreBasedRouter) rebalance(maxNum int) { curTime := time.Now() router.Lock() defer router.Unlock() for i := 0; i \u003c maxNum; i++ { var busiestEle *glist.Element[*backendWrapper] for be := router.backends.Front(); be != nil; be = be.Next() { backend := be.Value if backend.connList.Len() \u003e 0 { busiestEle = be break } } if busiestEle == nil { break } busiestBackend := busiestEle.Value idlestEle := router.backends.Back() idlestBackend := idlestEle.Value if float64(busiestBackend.score())/float64(idlestBackend.score()+1) \u003c rebalanceMaxScoreRatio { break } var ce *glist.Element[*connWrapper] for ele := busiestBackend.connList.Front(); ele != nil; ele = ele.Next() { conn := ele.Value switch conn.phase { case phaseRedirectNotify: continue case phaseRedirectFail: if conn.lastRedirect.Add(redirectFailMinInterval).After(curTime) { continue } } ce = ele break } if ce == nil { break } conn := ce.Value busiestBackend.connScore-- router.adjustBackendList(busiestEle) idlestBackend.connScore++ router.adjustBackendList(idlestEle) conn.phase = phaseRedirectNotify conn.lastRedirect = curTime conn.Redirect(idlestBackend.addr) } } rebalance çš„é€»è¾‘ ä»å‰å¾€åè®¿é—® backends listï¼Œæ‰¾åˆ° busiestBackend åœ¨ backends list æœ€åæ‰¾åˆ° idlestBackend æ¯”è¾ƒä¸¤è€… scoreï¼Œ å¦‚æœå·®è·åœ¨ 20% ä»¥å†…å°±ä¸ç”¨å¤„ç†äº† å¦åˆ™åœ¨ busiestBackend ä¸­å–å‡ºä¸€ä¸ª conn ç»™ idlestBackend å–å‡ºçš„é€»è¾‘å¾ˆç®€å•ï¼Œå°±æ˜¯ä»å‰åˆ°åéå†å½“å‰ backend çš„ connList å› ä¸ºsessionè¿ç§»è¦ä¿è¯äº‹åŠ¡å®Œæˆï¼Œæ‰€ä»¥è¿ç§»ä¸æ˜¯ç«‹åˆ»æ‰§è¡Œçš„ï¼Œè¿™å°±å¾—åŠ ä¸ª phase æ¥è·Ÿè¿› å¤„äº phaseRedirectNotify é˜¶æ®µçš„ä¸è¦å†å–å‡ºï¼› å¤„äº phaseRedirectFail ä½†è¿˜æ²¡åˆ°è¶…æ—¶æ—¶é—´çš„ï¼Œä¹Ÿä¸è¦å–å‡ºï¼› å…¶ä»–çŠ¶æ€çš„ conn å¯ä»¥è¢«å–å‡º å› ä¸ºæœ‰ conn å˜åŠ¨æ‰€ä»¥è¦è°ƒæ•´ä¸‹ busiestBackend å’Œ idlestBackend åœ¨ backends list ä¸­çš„ä½ç½® æœ€åé€šè¿‡ channel é€šçŸ¥ BackendConnManager åšå»sessionè¿ç§»ï¼Œæ­¤æ—¶ conn çŠ¶æ€æ˜¯ phaseRedirectNotify ç»™æ¯ä¸ªbackendçš„æ‰“åˆ†é€»è¾‘å¦‚ä¸‹ï¼Œåˆ†æ•°è¶Šå¤§è¯´æ˜è´Ÿè½½è¶Šå¤§ func (b *backendWrapper) score() int { return b.status.ToScore() + b.connScore } // var statusScores = map[BackendStatus]int{ // StatusHealthy: 0, // StatusCannotConnect: 10000000, // StatusMemoryHigh: 5000, // StatusRunSlow: 5000, // StatusSchemaOutdated: 10000000, // } // connScore = connList.Len() + incoming connections - outgoing connections. ","date":"2023-07-30","objectID":"/tiproxy-yuan-li-he-shi-xian/:4:2","series":null,"tags":["tidb","database","tiproxy"],"title":"TiProxy åŸç†å’Œå®ç°","uri":"/tiproxy-yuan-li-he-shi-xian/#2tiproxyæ˜¯åœ¨tidbèŠ‚ç‚¹é—´è‡ªåŠ¨è´Ÿè½½å‡è¡¡çš„é€»è¾‘"},{"categories":["tidb"],"content":" 2ã€tiproxyæ˜¯åœ¨tidbèŠ‚ç‚¹é—´è‡ªåŠ¨è´Ÿè½½å‡è¡¡çš„é€»è¾‘ï¼Ÿæ­¤å¤„è‡ªåŠ¨è´Ÿè½½çš„è¯­ä¹‰æ˜¯ï¼šå°†å“ªä¸ª backend çš„å“ªä¸ª connect è¿ç§»åˆ°å“ªä¸ª backend ä¸Šã€‚è¿™å°±è¦è§£å†³ backend æŒ‘é€‰å’Œ connect æŒ‘é€‰é—®é¢˜ã€‚ è¿™ä¸ªé—®é¢˜çš„è§£å†³åŠæ³•æ˜¯åœ¨ ScoreBasedRouter æ¨¡å—å®Œæˆã€‚è¿™ä¸ªæ¨¡å—æœ‰3ä¸ª func å’Œä¸Šè¿°è§£é‡Šç›¸å…³ï¼š type ScoreBasedRouter struct { sync.Mutex // A list of *backendWrapper. The backends are in descending order of scores. backends *glist.List[*backendWrapper] // ... } // è¢« BackendObserver è°ƒç”¨ï¼Œä¼ æ¥çš„ backends ä¼šåˆå¹¶åˆ° ScoreBasedRouter::backends ä¸­ func (router *ScoreBasedRouter) OnBackendChanged(backends map[string]*backendHealth, err error) {} // é€šè¿‡æ¯”è¾ƒ backend åˆ†æ•°æ–¹å¼è°ƒæ•´ ScoreBasedRouter::backends ä¸­çš„ä½ç½® func (router *ScoreBasedRouter) adjustBackendList(be *glist.Element[*backendWrapper]) {} // åç¨‹æ–¹å¼è¿è¡Œï¼Œåšè´Ÿè½½å‡è¡¡å¤„ç† func (router *ScoreBasedRouter) rebalanceLoop(ctx context.Context) {} OnBackendChanged æ˜¯æš´éœ²ç»™ BackendObserver æ¨¡å—çš„ä¸€ä¸ªæ¥å£ï¼Œ ç”¨æ¥åŒæ­¥ä» etcd å‘ç°çš„ tidb ä¿¡æ¯ï¼Œè¿™ä¸ªé€»è¾‘ä¸å¤æ‚ï¼Œè¯¦ç»†å¯è‡ªè¡Œé˜…è¯»æºç ã€‚è¿™ä¸ªæ–¹æ³•æ˜¯é—®é¢˜ä¸€ç§æåˆ°çš„â€œé€šçŸ¥â€æ¥æ”¶å¤„ã€‚ adjustBackendList æœ¬è´¨å°±æ˜¯è°ƒæ•´ item åœ¨åŒå‘é“¾è¡¨ä¸­çš„ä½ç½®ï¼Œè¿™ä¸ªä¹Ÿä¸å¤æ‚ã€‚ ä¸‹é¢é‡ç‚¹è¯´ä¸‹ rebalanceLoop çš„é€»è¾‘ï¼Œè¿™é‡Œæ¶‰åŠåˆ°\"å°†å“ªä¸ª backend çš„å“ªä¸ª connect è¿ç§»åˆ°å“ªä¸ª backend ä¸Š\"çš„é—®é¢˜ã€‚ // rebalanceLoop è®¡ç®—é—´éš”æ˜¯ 10 msï¼Œæ¯æ¬¡æœ€å¤šå¤„ç† 10 ä¸ªè¿æ¥(é˜²æ­¢åç«¯å‡ºç°æŠ–åŠ¨) // - backends çš„å˜åŒ–æ˜¯é€šè¿‡ OnBackendChanged ä¿®æ”¹çš„ï¼Œè¿æ¥å¹³è¡¡æ˜¯ rebalanceLoop å‡½æ•°åšçš„ï¼Œä¸¤è€…ä¸ºäº†ä¿è¯å¹¶å‘ä½¿ç”¨äº† sync.Mutex func (router *ScoreBasedRouter) rebalanceLoop(ctx context.Context) { for { router.rebalance(rebalanceConnsPerLoop) select { case \u003c-ctx.Done(): return case \u003c-time.After(rebalanceInterval): } } } // rebalance func (router *ScoreBasedRouter) rebalance(maxNum int) { curTime := time.Now() router.Lock() defer router.Unlock() for i := 0; i \u003c maxNum; i++ { var busiestEle *glist.Element[*backendWrapper] for be := router.backends.Front(); be != nil; be = be.Next() { backend := be.Value if backend.connList.Len() \u003e 0 { busiestEle = be break } } if busiestEle == nil { break } busiestBackend := busiestEle.Value idlestEle := router.backends.Back() idlestBackend := idlestEle.Value if float64(busiestBackend.score())/float64(idlestBackend.score()+1) \u003c rebalanceMaxScoreRatio { break } var ce *glist.Element[*connWrapper] for ele := busiestBackend.connList.Front(); ele != nil; ele = ele.Next() { conn := ele.Value switch conn.phase { case phaseRedirectNotify: continue case phaseRedirectFail: if conn.lastRedirect.Add(redirectFailMinInterval).After(curTime) { continue } } ce = ele break } if ce == nil { break } conn := ce.Value busiestBackend.connScore-- router.adjustBackendList(busiestEle) idlestBackend.connScore++ router.adjustBackendList(idlestEle) conn.phase = phaseRedirectNotify conn.lastRedirect = curTime conn.Redirect(idlestBackend.addr) } } rebalance çš„é€»è¾‘ ä»å‰å¾€åè®¿é—® backends listï¼Œæ‰¾åˆ° busiestBackend åœ¨ backends list æœ€åæ‰¾åˆ° idlestBackend æ¯”è¾ƒä¸¤è€… scoreï¼Œ å¦‚æœå·®è·åœ¨ 20% ä»¥å†…å°±ä¸ç”¨å¤„ç†äº† å¦åˆ™åœ¨ busiestBackend ä¸­å–å‡ºä¸€ä¸ª conn ç»™ idlestBackend å–å‡ºçš„é€»è¾‘å¾ˆç®€å•ï¼Œå°±æ˜¯ä»å‰åˆ°åéå†å½“å‰ backend çš„ connList å› ä¸ºsessionè¿ç§»è¦ä¿è¯äº‹åŠ¡å®Œæˆï¼Œæ‰€ä»¥è¿ç§»ä¸æ˜¯ç«‹åˆ»æ‰§è¡Œçš„ï¼Œè¿™å°±å¾—åŠ ä¸ª phase æ¥è·Ÿè¿› å¤„äº phaseRedirectNotify é˜¶æ®µçš„ä¸è¦å†å–å‡ºï¼› å¤„äº phaseRedirectFail ä½†è¿˜æ²¡åˆ°è¶…æ—¶æ—¶é—´çš„ï¼Œä¹Ÿä¸è¦å–å‡ºï¼› å…¶ä»–çŠ¶æ€çš„ conn å¯ä»¥è¢«å–å‡º å› ä¸ºæœ‰ conn å˜åŠ¨æ‰€ä»¥è¦è°ƒæ•´ä¸‹ busiestBackend å’Œ idlestBackend åœ¨ backends list ä¸­çš„ä½ç½® æœ€åé€šè¿‡ channel é€šçŸ¥ BackendConnManager åšå»sessionè¿ç§»ï¼Œæ­¤æ—¶ conn çŠ¶æ€æ˜¯ phaseRedirectNotify ç»™æ¯ä¸ªbackendçš„æ‰“åˆ†é€»è¾‘å¦‚ä¸‹ï¼Œåˆ†æ•°è¶Šå¤§è¯´æ˜è´Ÿè½½è¶Šå¤§ func (b *backendWrapper) score() int { return b.status.ToScore() + b.connScore } // var statusScores = map[BackendStatus]int{ // StatusHealthy: 0, // StatusCannotConnect: 10000000, // StatusMemoryHigh: 5000, // StatusRunSlow: 5000, // StatusSchemaOutdated: 10000000, // } // connScore = connList.Len() + incoming connections - outgoing connections. ","date":"2023-07-30","objectID":"/tiproxy-yuan-li-he-shi-xian/:4:2","series":null,"tags":["tidb","database","tiproxy"],"title":"TiProxy åŸç†å’Œå®ç°","uri":"/tiproxy-yuan-li-he-shi-xian/#rebalance-çš„é€»è¾‘"},{"categories":["tidb"],"content":" 3ã€åœ¨è‡ªåŠ¨è´Ÿè½½å‡è¡¡æ—¶tiproxyæ˜¯æ€ä¹ˆåšåˆ°ä¼˜é›…çš„sessionè¿ç§»ã€sessionä¸Šä¸‹æ–‡æ¢å¤ï¼Ÿè¿™ä¸ªé—®é¢˜å¯ä»¥ç»§ç»­ç»†åˆ†ï¼š è¿ç§»æ¶ˆæ¯æ¥æ”¶ ScoreBasedRouter æ¨¡å—è®¡ç®—å‡ºå“ªä¸ª conn ä»å“ªä¸ª backend è¿ç§»åˆ°å“ªä¸ª backend åï¼Œæ€ä¹ˆé€šçŸ¥ç»™å¯¹åº”çš„ conn ï¼Ÿ è¿ç§»ä»»åŠ¡æ‰§è¡Œ conn æ¥æ”¶åˆ°æ¶ˆæ¯åè¦è¿›è¡Œsessionè¿ç§»ï¼Œé‚£ä¹ˆå¦‚ä½•è§£å†³è¿ç§»æœŸé—´ client å¯èƒ½å­˜åœ¨è®¿é—®çš„é—®é¢˜ ï¼Ÿ å› ä¸ºtiproxyæ²¡æœ‰ä¿å­˜å¯†ç ï¼Œé‚£ä¹ˆåŸºäºsession tokençš„éªŒè¯æ–¹å¼æ˜¯æ€ä¹ˆå®ç°çš„ï¼Ÿ æ–°çš„tidbèŠ‚ç‚¹ç™»å½•æˆåŠŸåï¼Œsessionä¸Šä¸‹é—®é¢˜ä¿¡æ¯æ˜¯æ€ä¹ˆæ¢å¤çš„ï¼Ÿ ä»¥ä¸Šçš„é—®é¢˜éƒ½å¯ä»¥åœ¨ BackendConnManager æ¨¡å—æ‰¾åˆ°ç­”æ¡ˆï¼š type BackendConnManager struct { // processLock makes redirecting and command processing exclusive. processLock sync.Mutex clientIO *pnet.PacketIO backendIO atomic.Pointer[pnet.PacketIO] authenticator *Authenticator } func (mgr *BackendConnManager) Redirect(newAddr string) bool {} func (mgr *BackendConnManager) processSignals(ctx context.Context) {} func (mgr *BackendConnManager) tryRedirect(ctx context.Context) {} func (mgr *BackendConnManager) querySessionStates(backendIO *pnet.PacketIO) (sessionStates, sessionToken string, err error) {} func (mgr *BackendConnManager) ExecuteCmd(ctx context.Context, request []byte) (err error) {} è¿ç§»æ¶ˆæ¯æ¥æ”¶åœ¨å‰æ–‡çš„ rebalance æ–¹æ³•æœ€åï¼Œæœ‰è¡Œè¿™æ ·çš„é€»è¾‘ conn.Redirect(idlestBackend.addr) è¿™å°±æ˜¯ ScoreBasedRouter çš„é€šçŸ¥ç»™å¯¹åº” conn çš„åœ°æ–¹ã€‚ è¿™é‡Œè°ƒç”¨çš„æ˜¯ BackendConnManager::Redirectï¼Œ å…·ä½“æ‰§è¡Œé€»è¾‘ å°†ç›®æ ‡ backend å­˜å‚¨åˆ° redirectInfo ç»™ signalReceived channel å‘ signalTypeRedirect æ¶ˆæ¯ func (mgr *BackendConnManager) Redirect(newAddr string) bool { // NOTE: BackendConnManager may be closing concurrently because of no lock. switch mgr.closeStatus.Load() { case statusNotifyClose, statusClosing, statusClosed: return false } mgr.redirectInfo.Store(\u0026signalRedirect{newAddr: newAddr}) // Generally, it won't wait because the caller won't send another signal before the previous one finishes. mgr.signalReceived \u003c- signalTypeRedirect return true } è¯¥æ¶ˆæ¯è¢« BackendConnManager::processSignals åç¨‹æ¥æ”¶ func (mgr *BackendConnManager) processSignals(ctx context.Context) { for { select { case s := \u003c-mgr.signalReceived: // Redirect the session immediately just in case the session is finishedTxn. mgr.processLock.Lock() switch s { case signalTypeGracefulClose: mgr.tryGracefulClose(ctx) case signalTypeRedirect: // \u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c mgr.tryRedirect(ctx) } mgr.processLock.Unlock() case rs := \u003c-mgr.redirectResCh: mgr.notifyRedirectResult(ctx, rs) case \u003c-mgr.checkBackendTicker.C: mgr.checkBackendActive() case \u003c-ctx.Done(): return } } } è¿™é‡Œè¡¥å……ä¸‹ processSignals æ˜¯æ€ä¹ˆæ¥çš„ã€‚æ­£å¸¸æƒ…å†µä¸‹ï¼Œclientæ¯å‘èµ·ä¸€ä¸ªè¿æ¥ï¼Œproxyå°±ä¼šèµ·ä¸¤ä¸ªåç¨‹ï¼š è¿æ¥ã€è½¬å‘ tcp æ¶ˆæ¯åç¨‹ï¼š è¿æ¥ï¼šSQLServer::Run æ–¹æ³•å¯åŠ¨ï¼Œä¹Ÿå°±æ˜¯æ¯è¿æ¥æ¯åç¨‹çš„æ„æ€ã€‚ è½¬å‘ï¼šClientConnection æ¨¡å—è°ƒç”¨ BackendConnManager::ExecuteCmd å®ç°æ¶ˆæ¯è½¬å‘ ç›‘å¬å’Œæ‰§è¡Œ redirect ä»»åŠ¡åç¨‹ï¼š BackendConnManager æ¨¡å—å¯åŠ¨ processSignals åç¨‹å¤„ç† æ‰€ä»¥ä¸Šæ–‡ç›‘å¬ signalTypeRedirect æ¶ˆæ¯çš„ processSignals åç¨‹ï¼Œåœ¨è¿æ¥å»ºç«‹æ—¶å°±å¯åŠ¨äº†ï¼Œå½“æ”¶åˆ°æ¶ˆæ¯åæ‰§è¡Œ tryRedirect æ–¹æ³•å°è¯•æ‰§è¡Œè¿ç§»ã€‚ è¿ç§»ä»»åŠ¡æ‰§è¡ŒtryRedirect å¤„ç†é€»è¾‘æ¯”è¾ƒå¤æ‚ï¼Œæˆ‘ä»¬é€‰å–æ ¸å¿ƒæµç¨‹è¿›è¡Œç®€è¿°ï¼š func (mgr *BackendConnManager) tryRedirect(ctx context.Context) { // è·å–ç›®æ ‡ backend signal := mgr.redirectInfo.Load() // å¤„äºäº‹åŠ¡ä¸­ï¼Œå…ˆä¸åšè¿ç§» if !mgr.cmdProcessor.finishedTxn() { return } // ç»„è£…æ‰§è¡Œç»“æœ rs := \u0026redirectResult{ from: mgr.ServerAddr(), to: signal.newAddr, } defer func() { // ä¸è®ºæ‰§è¡ŒæˆåŠŸä¸å¦éƒ½æ¸…ç©º redirectInfoï¼Œ å¹¶å°† rs ç»“æœå‘åˆ° redirectResChï¼Œ redirectResCh çš„å¤„ç†é€»è¾‘è¿˜æ˜¯åœ¨ processSignals ä¸­å¤„ç† mgr.redirectInfo.Store(nil) mgr.redirectResCh \u003c- rs }() // ä»æº backend è·å– sessionStates, sessionToken backendIO := mgr.backendIO.Load() sessionStates, sessionToken, rs.err := mgr.querySessionStates(backendIO) // è·Ÿç›®æ ‡ backend å»ºç«‹tcpè¿æ¥ cn, rs.err := net.DialTimeout(\"tcp\", rs.to, DialTimeout) // å°† conn åŒ…è£¹ä¸º PacketIO newBackendIO := pnet.NewPacketIO(cn, mgr.logger, pnet.WithRemoteAddr(rs.to, cn.RemoteAddr()), pnet.WithWrapError(ErrBackendConn)) // ä½¿ç”¨ session tokenæ–¹å¼è·Ÿç›®æ ‡ backend è¿›è¡Œé‰´æ¡æ‰‹é‰´æƒ mgr.authenticator.handshakeSecondTime(mgr.logger, mgr.clientIO, newBackendIO, mgr.backendTLS, sessionToken) // ç™»å½•ç›®æ ‡ backend è¿›è¡Œé‰´æƒ rs.err = mgr.initSessionStates(newBackendIO, sessionStates) // å°†æ–°çš„ PacketIO å­˜å‚¨åˆ° BackendConnManager çš„æˆå‘˜å˜é‡ä¸­ï¼Œåç»­å†æœ‰è¯·æ±‚éƒ½æ˜¯ç”¨æ­¤å˜é‡ mgr.backendIO.Store(newBackendIO) } ä¸Šé¢å±•ç¤ºäº† session token çš„è®¤è¯æ–¹å¼å’Œä¸Šä¸‹æ–‡æ¢å¤çš„é€»è¾‘ï¼Œå¯¹åº” querySessionStates ã€handshakeSecondTime ã€initSessionStates ä¸‰ä¸ªæ–¹æ³• querySessionStates: tiproxy åœ¨ tidb a ä¸Šæ‰§è¡Œ SHOW SESSION_STATES è·å–åˆ° session_token session_state handshakeSecondTime: tiproxy ä½¿ç”¨ session_token è®¤è¯æ–¹å¼ç™»å½•åˆ° tidb b initSessio","date":"2023-07-30","objectID":"/tiproxy-yuan-li-he-shi-xian/:4:3","series":null,"tags":["tidb","database","tiproxy"],"title":"TiProxy åŸç†å’Œå®ç°","uri":"/tiproxy-yuan-li-he-shi-xian/#3åœ¨è‡ªåŠ¨è´Ÿè½½å‡è¡¡æ—¶tiproxyæ˜¯æ€ä¹ˆåšåˆ°ä¼˜é›…çš„sessionè¿ç§»sessionä¸Šä¸‹æ–‡æ¢å¤"},{"categories":["tidb"],"content":" 3ã€åœ¨è‡ªåŠ¨è´Ÿè½½å‡è¡¡æ—¶tiproxyæ˜¯æ€ä¹ˆåšåˆ°ä¼˜é›…çš„sessionè¿ç§»ã€sessionä¸Šä¸‹æ–‡æ¢å¤ï¼Ÿè¿™ä¸ªé—®é¢˜å¯ä»¥ç»§ç»­ç»†åˆ†ï¼š è¿ç§»æ¶ˆæ¯æ¥æ”¶ ScoreBasedRouter æ¨¡å—è®¡ç®—å‡ºå“ªä¸ª conn ä»å“ªä¸ª backend è¿ç§»åˆ°å“ªä¸ª backend åï¼Œæ€ä¹ˆé€šçŸ¥ç»™å¯¹åº”çš„ conn ï¼Ÿ è¿ç§»ä»»åŠ¡æ‰§è¡Œ conn æ¥æ”¶åˆ°æ¶ˆæ¯åè¦è¿›è¡Œsessionè¿ç§»ï¼Œé‚£ä¹ˆå¦‚ä½•è§£å†³è¿ç§»æœŸé—´ client å¯èƒ½å­˜åœ¨è®¿é—®çš„é—®é¢˜ ï¼Ÿ å› ä¸ºtiproxyæ²¡æœ‰ä¿å­˜å¯†ç ï¼Œé‚£ä¹ˆåŸºäºsession tokençš„éªŒè¯æ–¹å¼æ˜¯æ€ä¹ˆå®ç°çš„ï¼Ÿ æ–°çš„tidbèŠ‚ç‚¹ç™»å½•æˆåŠŸåï¼Œsessionä¸Šä¸‹é—®é¢˜ä¿¡æ¯æ˜¯æ€ä¹ˆæ¢å¤çš„ï¼Ÿ ä»¥ä¸Šçš„é—®é¢˜éƒ½å¯ä»¥åœ¨ BackendConnManager æ¨¡å—æ‰¾åˆ°ç­”æ¡ˆï¼š type BackendConnManager struct { // processLock makes redirecting and command processing exclusive. processLock sync.Mutex clientIO *pnet.PacketIO backendIO atomic.Pointer[pnet.PacketIO] authenticator *Authenticator } func (mgr *BackendConnManager) Redirect(newAddr string) bool {} func (mgr *BackendConnManager) processSignals(ctx context.Context) {} func (mgr *BackendConnManager) tryRedirect(ctx context.Context) {} func (mgr *BackendConnManager) querySessionStates(backendIO *pnet.PacketIO) (sessionStates, sessionToken string, err error) {} func (mgr *BackendConnManager) ExecuteCmd(ctx context.Context, request []byte) (err error) {} è¿ç§»æ¶ˆæ¯æ¥æ”¶åœ¨å‰æ–‡çš„ rebalance æ–¹æ³•æœ€åï¼Œæœ‰è¡Œè¿™æ ·çš„é€»è¾‘ conn.Redirect(idlestBackend.addr) è¿™å°±æ˜¯ ScoreBasedRouter çš„é€šçŸ¥ç»™å¯¹åº” conn çš„åœ°æ–¹ã€‚ è¿™é‡Œè°ƒç”¨çš„æ˜¯ BackendConnManager::Redirectï¼Œ å…·ä½“æ‰§è¡Œé€»è¾‘ å°†ç›®æ ‡ backend å­˜å‚¨åˆ° redirectInfo ç»™ signalReceived channel å‘ signalTypeRedirect æ¶ˆæ¯ func (mgr *BackendConnManager) Redirect(newAddr string) bool { // NOTE: BackendConnManager may be closing concurrently because of no lock. switch mgr.closeStatus.Load() { case statusNotifyClose, statusClosing, statusClosed: return false } mgr.redirectInfo.Store(\u0026signalRedirect{newAddr: newAddr}) // Generally, it won't wait because the caller won't send another signal before the previous one finishes. mgr.signalReceived \u003c- signalTypeRedirect return true } è¯¥æ¶ˆæ¯è¢« BackendConnManager::processSignals åç¨‹æ¥æ”¶ func (mgr *BackendConnManager) processSignals(ctx context.Context) { for { select { case s := \u003c-mgr.signalReceived: // Redirect the session immediately just in case the session is finishedTxn. mgr.processLock.Lock() switch s { case signalTypeGracefulClose: mgr.tryGracefulClose(ctx) case signalTypeRedirect: // \u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c mgr.tryRedirect(ctx) } mgr.processLock.Unlock() case rs := \u003c-mgr.redirectResCh: mgr.notifyRedirectResult(ctx, rs) case \u003c-mgr.checkBackendTicker.C: mgr.checkBackendActive() case \u003c-ctx.Done(): return } } } è¿™é‡Œè¡¥å……ä¸‹ processSignals æ˜¯æ€ä¹ˆæ¥çš„ã€‚æ­£å¸¸æƒ…å†µä¸‹ï¼Œclientæ¯å‘èµ·ä¸€ä¸ªè¿æ¥ï¼Œproxyå°±ä¼šèµ·ä¸¤ä¸ªåç¨‹ï¼š è¿æ¥ã€è½¬å‘ tcp æ¶ˆæ¯åç¨‹ï¼š è¿æ¥ï¼šSQLServer::Run æ–¹æ³•å¯åŠ¨ï¼Œä¹Ÿå°±æ˜¯æ¯è¿æ¥æ¯åç¨‹çš„æ„æ€ã€‚ è½¬å‘ï¼šClientConnection æ¨¡å—è°ƒç”¨ BackendConnManager::ExecuteCmd å®ç°æ¶ˆæ¯è½¬å‘ ç›‘å¬å’Œæ‰§è¡Œ redirect ä»»åŠ¡åç¨‹ï¼š BackendConnManager æ¨¡å—å¯åŠ¨ processSignals åç¨‹å¤„ç† æ‰€ä»¥ä¸Šæ–‡ç›‘å¬ signalTypeRedirect æ¶ˆæ¯çš„ processSignals åç¨‹ï¼Œåœ¨è¿æ¥å»ºç«‹æ—¶å°±å¯åŠ¨äº†ï¼Œå½“æ”¶åˆ°æ¶ˆæ¯åæ‰§è¡Œ tryRedirect æ–¹æ³•å°è¯•æ‰§è¡Œè¿ç§»ã€‚ è¿ç§»ä»»åŠ¡æ‰§è¡ŒtryRedirect å¤„ç†é€»è¾‘æ¯”è¾ƒå¤æ‚ï¼Œæˆ‘ä»¬é€‰å–æ ¸å¿ƒæµç¨‹è¿›è¡Œç®€è¿°ï¼š func (mgr *BackendConnManager) tryRedirect(ctx context.Context) { // è·å–ç›®æ ‡ backend signal := mgr.redirectInfo.Load() // å¤„äºäº‹åŠ¡ä¸­ï¼Œå…ˆä¸åšè¿ç§» if !mgr.cmdProcessor.finishedTxn() { return } // ç»„è£…æ‰§è¡Œç»“æœ rs := \u0026redirectResult{ from: mgr.ServerAddr(), to: signal.newAddr, } defer func() { // ä¸è®ºæ‰§è¡ŒæˆåŠŸä¸å¦éƒ½æ¸…ç©º redirectInfoï¼Œ å¹¶å°† rs ç»“æœå‘åˆ° redirectResChï¼Œ redirectResCh çš„å¤„ç†é€»è¾‘è¿˜æ˜¯åœ¨ processSignals ä¸­å¤„ç† mgr.redirectInfo.Store(nil) mgr.redirectResCh \u003c- rs }() // ä»æº backend è·å– sessionStates, sessionToken backendIO := mgr.backendIO.Load() sessionStates, sessionToken, rs.err := mgr.querySessionStates(backendIO) // è·Ÿç›®æ ‡ backend å»ºç«‹tcpè¿æ¥ cn, rs.err := net.DialTimeout(\"tcp\", rs.to, DialTimeout) // å°† conn åŒ…è£¹ä¸º PacketIO newBackendIO := pnet.NewPacketIO(cn, mgr.logger, pnet.WithRemoteAddr(rs.to, cn.RemoteAddr()), pnet.WithWrapError(ErrBackendConn)) // ä½¿ç”¨ session tokenæ–¹å¼è·Ÿç›®æ ‡ backend è¿›è¡Œé‰´æ¡æ‰‹é‰´æƒ mgr.authenticator.handshakeSecondTime(mgr.logger, mgr.clientIO, newBackendIO, mgr.backendTLS, sessionToken) // ç™»å½•ç›®æ ‡ backend è¿›è¡Œé‰´æƒ rs.err = mgr.initSessionStates(newBackendIO, sessionStates) // å°†æ–°çš„ PacketIO å­˜å‚¨åˆ° BackendConnManager çš„æˆå‘˜å˜é‡ä¸­ï¼Œåç»­å†æœ‰è¯·æ±‚éƒ½æ˜¯ç”¨æ­¤å˜é‡ mgr.backendIO.Store(newBackendIO) } ä¸Šé¢å±•ç¤ºäº† session token çš„è®¤è¯æ–¹å¼å’Œä¸Šä¸‹æ–‡æ¢å¤çš„é€»è¾‘ï¼Œå¯¹åº” querySessionStates ã€handshakeSecondTime ã€initSessionStates ä¸‰ä¸ªæ–¹æ³• querySessionStates: tiproxy åœ¨ tidb a ä¸Šæ‰§è¡Œ SHOW SESSION_STATES è·å–åˆ° session_token session_state handshakeSecondTime: tiproxy ä½¿ç”¨ session_token è®¤è¯æ–¹å¼ç™»å½•åˆ° tidb b initSessio","date":"2023-07-30","objectID":"/tiproxy-yuan-li-he-shi-xian/:4:3","series":null,"tags":["tidb","database","tiproxy"],"title":"TiProxy åŸç†å’Œå®ç°","uri":"/tiproxy-yuan-li-he-shi-xian/#è¿ç§»æ¶ˆæ¯æ¥æ”¶"},{"categories":["tidb"],"content":" 3ã€åœ¨è‡ªåŠ¨è´Ÿè½½å‡è¡¡æ—¶tiproxyæ˜¯æ€ä¹ˆåšåˆ°ä¼˜é›…çš„sessionè¿ç§»ã€sessionä¸Šä¸‹æ–‡æ¢å¤ï¼Ÿè¿™ä¸ªé—®é¢˜å¯ä»¥ç»§ç»­ç»†åˆ†ï¼š è¿ç§»æ¶ˆæ¯æ¥æ”¶ ScoreBasedRouter æ¨¡å—è®¡ç®—å‡ºå“ªä¸ª conn ä»å“ªä¸ª backend è¿ç§»åˆ°å“ªä¸ª backend åï¼Œæ€ä¹ˆé€šçŸ¥ç»™å¯¹åº”çš„ conn ï¼Ÿ è¿ç§»ä»»åŠ¡æ‰§è¡Œ conn æ¥æ”¶åˆ°æ¶ˆæ¯åè¦è¿›è¡Œsessionè¿ç§»ï¼Œé‚£ä¹ˆå¦‚ä½•è§£å†³è¿ç§»æœŸé—´ client å¯èƒ½å­˜åœ¨è®¿é—®çš„é—®é¢˜ ï¼Ÿ å› ä¸ºtiproxyæ²¡æœ‰ä¿å­˜å¯†ç ï¼Œé‚£ä¹ˆåŸºäºsession tokençš„éªŒè¯æ–¹å¼æ˜¯æ€ä¹ˆå®ç°çš„ï¼Ÿ æ–°çš„tidbèŠ‚ç‚¹ç™»å½•æˆåŠŸåï¼Œsessionä¸Šä¸‹é—®é¢˜ä¿¡æ¯æ˜¯æ€ä¹ˆæ¢å¤çš„ï¼Ÿ ä»¥ä¸Šçš„é—®é¢˜éƒ½å¯ä»¥åœ¨ BackendConnManager æ¨¡å—æ‰¾åˆ°ç­”æ¡ˆï¼š type BackendConnManager struct { // processLock makes redirecting and command processing exclusive. processLock sync.Mutex clientIO *pnet.PacketIO backendIO atomic.Pointer[pnet.PacketIO] authenticator *Authenticator } func (mgr *BackendConnManager) Redirect(newAddr string) bool {} func (mgr *BackendConnManager) processSignals(ctx context.Context) {} func (mgr *BackendConnManager) tryRedirect(ctx context.Context) {} func (mgr *BackendConnManager) querySessionStates(backendIO *pnet.PacketIO) (sessionStates, sessionToken string, err error) {} func (mgr *BackendConnManager) ExecuteCmd(ctx context.Context, request []byte) (err error) {} è¿ç§»æ¶ˆæ¯æ¥æ”¶åœ¨å‰æ–‡çš„ rebalance æ–¹æ³•æœ€åï¼Œæœ‰è¡Œè¿™æ ·çš„é€»è¾‘ conn.Redirect(idlestBackend.addr) è¿™å°±æ˜¯ ScoreBasedRouter çš„é€šçŸ¥ç»™å¯¹åº” conn çš„åœ°æ–¹ã€‚ è¿™é‡Œè°ƒç”¨çš„æ˜¯ BackendConnManager::Redirectï¼Œ å…·ä½“æ‰§è¡Œé€»è¾‘ å°†ç›®æ ‡ backend å­˜å‚¨åˆ° redirectInfo ç»™ signalReceived channel å‘ signalTypeRedirect æ¶ˆæ¯ func (mgr *BackendConnManager) Redirect(newAddr string) bool { // NOTE: BackendConnManager may be closing concurrently because of no lock. switch mgr.closeStatus.Load() { case statusNotifyClose, statusClosing, statusClosed: return false } mgr.redirectInfo.Store(\u0026signalRedirect{newAddr: newAddr}) // Generally, it won't wait because the caller won't send another signal before the previous one finishes. mgr.signalReceived \u003c- signalTypeRedirect return true } è¯¥æ¶ˆæ¯è¢« BackendConnManager::processSignals åç¨‹æ¥æ”¶ func (mgr *BackendConnManager) processSignals(ctx context.Context) { for { select { case s := \u003c-mgr.signalReceived: // Redirect the session immediately just in case the session is finishedTxn. mgr.processLock.Lock() switch s { case signalTypeGracefulClose: mgr.tryGracefulClose(ctx) case signalTypeRedirect: // \u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c\u003c mgr.tryRedirect(ctx) } mgr.processLock.Unlock() case rs := \u003c-mgr.redirectResCh: mgr.notifyRedirectResult(ctx, rs) case \u003c-mgr.checkBackendTicker.C: mgr.checkBackendActive() case \u003c-ctx.Done(): return } } } è¿™é‡Œè¡¥å……ä¸‹ processSignals æ˜¯æ€ä¹ˆæ¥çš„ã€‚æ­£å¸¸æƒ…å†µä¸‹ï¼Œclientæ¯å‘èµ·ä¸€ä¸ªè¿æ¥ï¼Œproxyå°±ä¼šèµ·ä¸¤ä¸ªåç¨‹ï¼š è¿æ¥ã€è½¬å‘ tcp æ¶ˆæ¯åç¨‹ï¼š è¿æ¥ï¼šSQLServer::Run æ–¹æ³•å¯åŠ¨ï¼Œä¹Ÿå°±æ˜¯æ¯è¿æ¥æ¯åç¨‹çš„æ„æ€ã€‚ è½¬å‘ï¼šClientConnection æ¨¡å—è°ƒç”¨ BackendConnManager::ExecuteCmd å®ç°æ¶ˆæ¯è½¬å‘ ç›‘å¬å’Œæ‰§è¡Œ redirect ä»»åŠ¡åç¨‹ï¼š BackendConnManager æ¨¡å—å¯åŠ¨ processSignals åç¨‹å¤„ç† æ‰€ä»¥ä¸Šæ–‡ç›‘å¬ signalTypeRedirect æ¶ˆæ¯çš„ processSignals åç¨‹ï¼Œåœ¨è¿æ¥å»ºç«‹æ—¶å°±å¯åŠ¨äº†ï¼Œå½“æ”¶åˆ°æ¶ˆæ¯åæ‰§è¡Œ tryRedirect æ–¹æ³•å°è¯•æ‰§è¡Œè¿ç§»ã€‚ è¿ç§»ä»»åŠ¡æ‰§è¡ŒtryRedirect å¤„ç†é€»è¾‘æ¯”è¾ƒå¤æ‚ï¼Œæˆ‘ä»¬é€‰å–æ ¸å¿ƒæµç¨‹è¿›è¡Œç®€è¿°ï¼š func (mgr *BackendConnManager) tryRedirect(ctx context.Context) { // è·å–ç›®æ ‡ backend signal := mgr.redirectInfo.Load() // å¤„äºäº‹åŠ¡ä¸­ï¼Œå…ˆä¸åšè¿ç§» if !mgr.cmdProcessor.finishedTxn() { return } // ç»„è£…æ‰§è¡Œç»“æœ rs := \u0026redirectResult{ from: mgr.ServerAddr(), to: signal.newAddr, } defer func() { // ä¸è®ºæ‰§è¡ŒæˆåŠŸä¸å¦éƒ½æ¸…ç©º redirectInfoï¼Œ å¹¶å°† rs ç»“æœå‘åˆ° redirectResChï¼Œ redirectResCh çš„å¤„ç†é€»è¾‘è¿˜æ˜¯åœ¨ processSignals ä¸­å¤„ç† mgr.redirectInfo.Store(nil) mgr.redirectResCh \u003c- rs }() // ä»æº backend è·å– sessionStates, sessionToken backendIO := mgr.backendIO.Load() sessionStates, sessionToken, rs.err := mgr.querySessionStates(backendIO) // è·Ÿç›®æ ‡ backend å»ºç«‹tcpè¿æ¥ cn, rs.err := net.DialTimeout(\"tcp\", rs.to, DialTimeout) // å°† conn åŒ…è£¹ä¸º PacketIO newBackendIO := pnet.NewPacketIO(cn, mgr.logger, pnet.WithRemoteAddr(rs.to, cn.RemoteAddr()), pnet.WithWrapError(ErrBackendConn)) // ä½¿ç”¨ session tokenæ–¹å¼è·Ÿç›®æ ‡ backend è¿›è¡Œé‰´æ¡æ‰‹é‰´æƒ mgr.authenticator.handshakeSecondTime(mgr.logger, mgr.clientIO, newBackendIO, mgr.backendTLS, sessionToken) // ç™»å½•ç›®æ ‡ backend è¿›è¡Œé‰´æƒ rs.err = mgr.initSessionStates(newBackendIO, sessionStates) // å°†æ–°çš„ PacketIO å­˜å‚¨åˆ° BackendConnManager çš„æˆå‘˜å˜é‡ä¸­ï¼Œåç»­å†æœ‰è¯·æ±‚éƒ½æ˜¯ç”¨æ­¤å˜é‡ mgr.backendIO.Store(newBackendIO) } ä¸Šé¢å±•ç¤ºäº† session token çš„è®¤è¯æ–¹å¼å’Œä¸Šä¸‹æ–‡æ¢å¤çš„é€»è¾‘ï¼Œå¯¹åº” querySessionStates ã€handshakeSecondTime ã€initSessionStates ä¸‰ä¸ªæ–¹æ³• querySessionStates: tiproxy åœ¨ tidb a ä¸Šæ‰§è¡Œ SHOW SESSION_STATES è·å–åˆ° session_token session_state handshakeSecondTime: tiproxy ä½¿ç”¨ session_token è®¤è¯æ–¹å¼ç™»å½•åˆ° tidb b initSessio","date":"2023-07-30","objectID":"/tiproxy-yuan-li-he-shi-xian/:4:3","series":null,"tags":["tidb","database","tiproxy"],"title":"TiProxy åŸç†å’Œå®ç°","uri":"/tiproxy-yuan-li-he-shi-xian/#è¿ç§»ä»»åŠ¡æ‰§è¡Œ"},{"categories":["tidb"],"content":" 4ã€tiproxyåœ¨è‡ªåŠ¨è´Ÿè½½å‡è¡¡æœŸé—´é‡åˆ°å¤„äºæœªæäº¤äº‹åŠ¡çš„sessionæ˜¯æ€ä¹ˆç­‰å¾…ç»“æŸçš„ï¼Ÿå¯¹äº tryRedirect æ–¹æ³•æœ‰ä¸¤ä¸ªåœ°æ–¹è¢«è°ƒç”¨ï¼Œå³å‰æ–‡æåˆ°çš„ BackendConnManager::processSignals å’Œ BackendConnManager::ExecuteCmd BackendConnManager::processSignals åªæœ‰åœ¨æ”¶åˆ°channeæ¶ˆæ¯åç«‹å³å‡ºå‘ä¸€æ¬¡ï¼Œå¦‚æœæœ‰æœªå®Œæˆçš„äº‹åŠ¡å°±ä¸å†æ‰§è¡Œäº†ã€‚ æ‰€ä»¥ä¸ºäº†ä¿è¯è¿ç§»ä»»åŠ¡å¯ç»§ç»­ï¼Œåœ¨ BackendConnManager::ExecuteCmd ä¸­æ¯æ¬¡æ‰§è¡Œå®Œ executeCmd åå°è¯•è¿ç§»ï¼Œè¿™æ ·å°±èƒ½ä¿è¯äº‹åŠ¡ç»“æŸåç«‹åˆ»è¿ç§»ã€‚ func (mgr *BackendConnManager) ExecuteCmd(ctx context.Context, request []byte) (err error) { // ... waitingRedirect := mgr.redirectInfo.Load() != nil // ... holdRequest, err = mgr.cmdProcessor.executeCmd(request, mgr.clientIO, mgr.backendIO.Load(), waitingRedirect) // ... if mgr.cmdProcessor.finishedTxn() { if waitingRedirect { mgr.tryRedirect(ctx) } // ... } // ... } åˆ¤æ–­äº‹åŠ¡æ˜¯å¦ç»“æŸçš„ finishedTxn æ–¹æ³•é€»è¾‘ï¼šè§£æ client çš„è¯·æ±‚ç±»å‹ã€è§£æ backend çš„å“åº”çŠ¶æ€ç»¼åˆåˆ¤æ–­äº‹åŠ¡æ˜¯å¦å®Œæˆï¼Œæ­¤é€»è¾‘è¿‡äºç¡¬æ ¸ï¼Œç­‰ä»¥åç ”ç©¶æ˜ç™½åå†åˆ†äº«å§ã€‚ æœ‰å…´è¶£çš„è¯»è€…å¯ä»¥åˆ†æä¸‹è¿™æ®µé€»è¾‘ï¼š func (cp *CmdProcessor) finishedTxn() bool { if cp.serverStatus\u0026(StatusInTrans|StatusQuit) \u003e 0 { return false } // If any result of the prepared statements is not fetched, we should wait. return !cp.hasPendingPreparedStmts() } func (cp *CmdProcessor) updatePrepStmtStatus(request []byte, serverStatus uint16) { var ( stmtID int prepStmtStatus uint32 ) cmd := pnet.Command(request[0]) switch cmd { case pnet.ComStmtSendLongData, pnet.ComStmtExecute, pnet.ComStmtFetch, pnet.ComStmtReset, pnet.ComStmtClose: stmtID = int(binary.LittleEndian.Uint32(request[1:5])) case pnet.ComResetConnection, pnet.ComChangeUser: cp.preparedStmtStatus = make(map[int]uint32) return default: return } switch cmd { case pnet.ComStmtSendLongData: prepStmtStatus = StatusPrepareWaitExecute case pnet.ComStmtExecute: if serverStatus\u0026mysql.ServerStatusCursorExists \u003e 0 { prepStmtStatus = StatusPrepareWaitFetch } case pnet.ComStmtFetch: if serverStatus\u0026mysql.ServerStatusLastRowSend == 0 { prepStmtStatus = StatusPrepareWaitFetch } } if prepStmtStatus \u003e 0 { cp.preparedStmtStatus[stmtID] = prepStmtStatus } else { delete(cp.preparedStmtStatus, stmtID) } } ","date":"2023-07-30","objectID":"/tiproxy-yuan-li-he-shi-xian/:4:4","series":null,"tags":["tidb","database","tiproxy"],"title":"TiProxy åŸç†å’Œå®ç°","uri":"/tiproxy-yuan-li-he-shi-xian/#4tiproxyåœ¨è‡ªåŠ¨è´Ÿè½½å‡è¡¡æœŸé—´é‡åˆ°å¤„äºæœªæäº¤äº‹åŠ¡çš„sessionæ˜¯æ€ä¹ˆç­‰å¾…ç»“æŸçš„"},{"categories":["tidb"],"content":" æ€»ç»“æœ¬æ–‡ä»4ä¸ªç–‘æƒ‘å…¥æ‰‹ï¼Œé˜…è¯»äº†ä¸‹tiproxyçš„ä»£ç å®ç°ï¼Œéƒ½æ‰¾åˆ°äº†å¯¹åº”çš„å¤„ç†é€»è¾‘ã€‚ å¯¹æ¯”äºtidbã€tikvã€pdç­‰ç»„ä»¶ä»£ç ï¼Œtiproxyå®ç®€å•å¾ˆå¤šï¼Œæ¨èå¤§å®¶å­¦ä¹ ä¸‹ã€‚ ","date":"2023-07-30","objectID":"/tiproxy-yuan-li-he-shi-xian/:5:0","series":null,"tags":["tidb","database","tiproxy"],"title":"TiProxy åŸç†å’Œå®ç°","uri":"/tiproxy-yuan-li-he-shi-xian/#æ€»ç»“"},{"categories":["tidb"],"content":" å½©è›‹åœ¨æ¢³ç†ä¸Šé¢4ä¸ªé—®é¢˜çš„æ—¶ï¼Œç†æ¸…æ€è·¯åï¼Œç»˜åˆ¶äº†å¦‚ä¸‹çš„å†…éƒ¨äº¤äº’å›¾ï¼Œæœ‰å…´è¶£çš„å¯ä»¥è‡ªå·±ç ”ç©¶ä¸‹ï¼Œä¸‹ç¯‡æ–‡ç« æˆ‘ä»¬å°†å¯¹å…¶è¿›è¡Œè¯´æ˜ã€‚ ","date":"2023-07-30","objectID":"/tiproxy-yuan-li-he-shi-xian/:6:0","series":null,"tags":["tidb","database","tiproxy"],"title":"TiProxy åŸç†å’Œå®ç°","uri":"/tiproxy-yuan-li-he-shi-xian/#å½©è›‹"},{"categories":["tidb"],"content":" è¯´æ˜æœ€è¿‘å‘ç° tidb æœ‰ä¸ª https://github.com/pingcap/TiProxy ä»“åº“ï¼ŒæŠ±ç€å¥½å¥‡çš„å¿ƒæ€æƒ³è¯•è¯•è¿™ä¸ªç»„ä»¶çš„ä½¿ç”¨æ•ˆæœã€‚äºæ˜¯æŒ‰ç…§æ–‡æ¡£çš„ä»‹ç»åœ¨æœ¬åœ°ç¯å¢ƒä½¿ç”¨tiupåšäº†ä¸€äº›å®éªŒï¼Œç°åœ¨å°†å®éªŒè¿‡ç¨‹å’Œå®éªŒç»“æœåˆ†äº«ç»™å¤§å®¶ã€‚ ","date":"2023-07-29","objectID":"/try-tiproxy/:1:0","series":null,"tags":["tidb","database","tiproxy"],"title":"TiProxy å°é²œ","uri":"/try-tiproxy/#è¯´æ˜"},{"categories":["tidb"],"content":" TiProxyä»‹ç»å®˜æ–¹READMEä»‹ç»çš„å·²ç»å¾ˆæ¸…æ¥šäº†ï¼Œæœ€é‡è¦çš„ç‰¹æ€§æ˜¯åœ¨TiDBå‡çº§ã€é‡å¯ã€æ‰©ç¼©èŠ‚ç‚¹æ—¶å€™å¯ä»¥ä¿è¯è¿æ¥ä¸æ–­ã€‚ç‰›ï¼ TiProxy is a database proxy that is based on TiDB. It keeps client connections alive while the TiDB server upgrades, restarts, scales in, and scales out. æ­¤å¤–è¿˜æœ‰ä¸€äº›ç‰¹æ€§ è¿æ¥ç®¡ç†ï¼š å½“tidbèŠ‚ç‚¹é‡å¯æˆ–è€…å…³æœºåï¼Œåœ¨è¿™ä¸ªèŠ‚ç‚¹ä¸Šå»ºç«‹çš„è¿æ¥ä¼šè¿ç§»åˆ°å…¶ä»–å®ä¾‹ä¸Šï¼Œè¿™ä¸ªåŠ¨ä½œå¯¹clientæ˜¯é€æ˜çš„ï¼Œclientæ— æ„ŸçŸ¥ è´Ÿè½½å‡è¡¡ï¼š æ–°å»ºè¿æ¥ä¼šå¯¹åç«¯tidb-serverè¿›è¡Œæ‰“åˆ†ï¼Œç„¶åè¿›è¡Œå¤šä¸ªtidbå®ä¾‹é—´çš„å‡è¡¡ æœåŠ¡å‘ç°ï¼š TiProxy é€šè¿‡è·Ÿpdäº¤äº’è·å–æœ€æ–°çš„tidbå®ä¾‹ä¿¡æ¯ï¼Œå½“æœ‰æ–°çš„tidbå¯åŠ¨æ—¶ï¼Œproxyä¼šè‡ªåŠ¨å‘ç°å¹¶è¿ç§»è¿æ¥è‡³æ­¤ã€‚ ","date":"2023-07-29","objectID":"/try-tiproxy/:2:0","series":null,"tags":["tidb","database","tiproxy"],"title":"TiProxy å°é²œ","uri":"/try-tiproxy/#tiproxyä»‹ç»"},{"categories":["tidb"],"content":" å®éªŒè¯´æ˜ä½¿ç”¨tiupæ­å»ºä¸‹æµ‹è¯•ç¯å¢ƒï¼Œå¯åŠ¨1ä¸ªpdã€1ä¸ªtikvã€1ä¸ªtidb-serverã€1ä¸ªtiproxyï¼Œé€šè¿‡tiproxyè¿æ¥æ•°æ®åº“ï¼Œæµ‹è¯•caseå¦‚ä¸‹ï¼š å¯åŠ¨ä¸¤ä¸ªç»ˆç«¯è¿æ¥æ•°æ®åº“ï¼Œç„¶ååŠ 1ä¸ªtidb-serverèŠ‚ç‚¹ï¼Œçœ‹çœ‹clientæ— æ„Ÿçš„è´Ÿè´£å‡è¡¡æ•ˆæœ ä¸Šä¸€æ­¥å®Œæˆåï¼Œæˆ‘ä»¬æœ‰äº†2ä¸ªtidb-serverï¼Œé‚£ä¹ˆç¼©æ‰ä¸€ä¸ªï¼Œçœ‹çœ‹proxyæ˜¯æ€ä¹ˆåšåˆ°ä¼šè¯è¿ç§»çš„ ","date":"2023-07-29","objectID":"/try-tiproxy/:3:0","series":null,"tags":["tidb","database","tiproxy"],"title":"TiProxy å°é²œ","uri":"/try-tiproxy/#å®éªŒè¯´æ˜"},{"categories":["tidb"],"content":" å¯åŠ¨é›†ç¾¤æŸ¥é˜…èµ„æ–™ å‘ç°TiProxyä»…æ”¯æŒv6.4.0åŠä»¥åç‰ˆæœ¬ï¼Œæ‰€ä»¥ä½¿ç”¨tiupå¯åŠ¨è¿™ä¸ªç‰ˆæœ¬çš„é›†ç¾¤ã€‚ tidb å’Œ tiproxy ä½¿ç”¨ toekn è®¤è¯æ–¹å¼ï¼Œæ‰€ä»¥ç”Ÿæˆä¸€ä¸ªè¯ä¹¦ openssl req -x509 -newkey rsa:4096 -sha256 -days 3650 -nodes -keyout key.pem -out cert.pem -subj \"/CN=example.com\" å‡†å¤‡é…ç½®æ–‡ä»¶ tidb.toml å’Œ tiproxy.yaml $ cat tidb.toml graceful-wait-before-shutdown=10 [security] auto-tls=true session-token-signing-cert='/tmp/tiup/tiproxy/cert.pem' session-token-signing-key='/tmp/tiup/tiproxy/key.pem' $ cat tiproxy.yaml [proxy] require-backend-tls = false å¯åŠ¨tidb tiup playground v6.4.0 --db 1 --kv 1 --pd 1 --tiflash 0 --without-monitor --db.config tidb.toml å¯åŠ¨tiproxy tiup tiproxy:nightly --config tiproxy.yaml ","date":"2023-07-29","objectID":"/try-tiproxy/:4:0","series":null,"tags":["tidb","database","tiproxy"],"title":"TiProxy å°é²œ","uri":"/try-tiproxy/#å¯åŠ¨é›†ç¾¤"},{"categories":["tidb"],"content":" å®éªŒ","date":"2023-07-29","objectID":"/try-tiproxy/:5:0","series":null,"tags":["tidb","database","tiproxy"],"title":"TiProxy å°é²œ","uri":"/try-tiproxy/#å®éªŒ"},{"categories":["tidb"],"content":" 1ã€åŠ èŠ‚ç‚¹è‡ªåŠ¨è´Ÿè½½å‡è¡¡é›†ç¾¤å¯åŠ¨åï¼Œä½¿ç”¨ä¸¤ä¸ªç»ˆç«¯è¿æ¥proxyï¼Œç„¶åæ‰§è¡Œshow processlistå¯ä»¥çœ‹åˆ°å¯¹æ–¹çš„ä¼šè¯ï¼Œè¯´æ˜è¿æ¥åˆ°äº†ä¸€ä¸ªtidbèŠ‚ç‚¹ä¸Š æ‰§è¡Œtiupæ·»åŠ ä¸€ä¸ªtidb-serverèŠ‚ç‚¹ tiup playground scale-out --db 1 ç„¶ååˆ†åˆ«æ‰§è¡Œshow processlitæŸ¥è¯¢ï¼Œå‘ç°æ¯ä¸ªç»ˆç«¯çœ‹ä¸åˆ°å¯¹æ–¹çš„ä¼šè¯äº†ï¼Œè¯´æ˜å„è‡ªè¿æ¥åˆ°äº†ä¸€ä¸ªtidbå®ä¾‹ã€‚ ä»”ç»†æŸ¥çœ‹å‘ç°å…¶ä¸­ä¸€ä¸ªè¿æ¥çš„ä¿¡æ¯ä»127.0.0.1:53240å˜æˆäº†127.0.0.1:54328ï¼Œä¹Ÿç¡®å®è¯´æ˜å‘ç”Ÿäº†é‡è¿æ¥ã€‚ è¿™é‡Œè¡¥å……ä¸ªè¯´æ˜ï¼šå› ä¸ºæˆ‘æµ‹è¯•çš„æ—¶å€™æ²¡æœ‰å¼€å¯proxyåè®®ï¼Œæ‰€ä»¥show processlistçœ‹åˆ°çš„hostä¸æ˜¯clientçœŸå®çš„ä¿¡æ¯ï¼Œæ˜¯proxyå’Œtidbå»ºç«‹è¿æ¥çš„ä¿¡æ¯ï¼ŒtidbæŠŠproxyå½“æˆclientå‡ºæ¥äº†ã€‚ æµ‹è¯•ç»“æœå¾ˆå¥½ï¼Œè´Ÿè½½å‡è¡¡clientæ— æ„Ÿã€‚ ","date":"2023-07-29","objectID":"/try-tiproxy/:5:1","series":null,"tags":["tidb","database","tiproxy"],"title":"TiProxy å°é²œ","uri":"/try-tiproxy/#1åŠ èŠ‚ç‚¹è‡ªåŠ¨è´Ÿè½½å‡è¡¡"},{"categories":["tidb"],"content":" 2ã€ç¼©èŠ‚ç‚¹ä¼šè¯è‡ªåŠ¨è¿ç§»åœ¨è¿™ä¸ªåŸºç¡€ä¸Šï¼Œæ‰§è¡Œtiupç¼©æ‰ä¸€ä¸ªtidb-server tiup playground scale-in --pid 91609 ç„¶åæ‰§è¡Œshow processlistï¼Œå¯ä»¥çœ‹åˆ°å¯¹æ–¹çš„ä¼šè¯ï¼Œè¯´æ˜åˆè¿æ¥åˆ°äº†åŒä¸€ä¸ªtidbèŠ‚ç‚¹ä¸Šã€‚ æ‰§è¡Œsqlçš„æ—¶å€™æ²¡æœ‰æŠ¥é”™ï¼Œclientæ— æ„ŸçŸ¥ã€‚ ","date":"2023-07-29","objectID":"/try-tiproxy/:5:2","series":null,"tags":["tidb","database","tiproxy"],"title":"TiProxy å°é²œ","uri":"/try-tiproxy/#2ç¼©èŠ‚ç‚¹ä¼šè¯è‡ªåŠ¨è¿ç§»"},{"categories":["tidb"],"content":" åŠ é¤å®éªŒè‡³æ­¤ä¸€åˆ‡éƒ½ä¸èˆ¬é¡ºæ»‘ã€ç¬¦åˆé¢„æœŸã€‚ä½†æ˜¯æµ‹è¯•çš„åœºæ™¯æœªå…æœ‰äº›ç®€å•ã€‚ä¸‹é¢åšä¸ªå¸¦æœ‰äº‹åŠ¡çš„caseï¼š ä½¿ç”¨tiupæ­å»ºä¸‹æµ‹è¯•ç¯å¢ƒï¼Œå¯åŠ¨1ä¸ªpdã€1ä¸ªtikvã€1ä¸ªtidb-serverã€1ä¸ªtiproxyï¼Œé€šè¿‡tiproxyè¿æ¥æ•°æ®åº“ï¼Œæ‰“å¼€ä¸¤ä¸ªç»ˆç«¯å¹¶æ˜¾ç¤ºæ‰§è¡Œä¸€ä¸ªbeginï¼Œç„¶ååˆ†åˆ«æ‰§è¡Œä¸ªå†™å…¥æ“ä½œï¼Œä¹‹åå†æ·»åŠ 1ä¸ªtidb-serverï¼Œçœ‹çœ‹ä¼šè¯æ˜¯å¦ä¼šè¢«è¿ç§»ã€‚ è¿™è¯´æ˜åœ¨æ‰§è¡Œä¸­çš„äº‹åŠ¡ä¸ä¼šåšè¿ç§»ã€‚åœ¨è®¾è®¡æ–‡æ¡£ ä¸­ä¹Ÿçš„ç¡®æœ‰è¿™æ ·çš„æè¿° Transactions are hard to be restored, so Session Manager doesnâ€™t support restoring a transaction. Session Manager must wait until the current transaction finishes or the TiDB instance exits due to shut down timeout. ç¬¦åˆé¢„æœŸã€‚ ","date":"2023-07-29","objectID":"/try-tiproxy/:6:0","series":null,"tags":["tidb","database","tiproxy"],"title":"TiProxy å°é²œ","uri":"/try-tiproxy/#åŠ é¤"},{"categories":["tidb"],"content":" æ€»ç»“æœ¬æ¬¡åŸºäºv6.4.0ç‰ˆæœ¬åšäº†3ä¸ªç®€å•çš„å®éªŒï¼Œå¯¹äºtidbèŠ‚ç‚¹æ‰©ç¼©æœ‰ä¼šè¯è‡ªåŠ¨è¿ç§»çš„èƒ½åŠ›çš„ç¡®å¾ˆä¸æ»‘ã€‚ æ•´ä¸ªè¿‡ç¨‹cleintæ— æŠ¥é”™ã€æ— æ„ŸçŸ¥ã€‚è¢«è¿ç§»çš„ä¼šè¯å¦‚æœæœ‰æœªæçš„äº‹åŠ¡ï¼Œåˆ™ä¼šç­‰åˆ°äº‹åŠ¡ç»“æŸåå†è¿ç§»ã€‚ èµğŸ‘ğŸ» ","date":"2023-07-29","objectID":"/try-tiproxy/:7:0","series":null,"tags":["tidb","database","tiproxy"],"title":"TiProxy å°é²œ","uri":"/try-tiproxy/#æ€»ç»“"},{"categories":["mysql-operator"],"content":" mysql-operator chart è¯´æ˜å®‰è£… mysql-operator çš„æ“ä½œå‘½ä»¤åœ¨å®˜æ–¹ä»“åº“å’Œå®˜ç½‘éƒ½æœ‰è¯´æ˜ï¼Œå…·ä½“å®‰è£…å‘½ä»¤å¦‚ä¸‹ï¼š ## For Helm v3 helm repo add bitpoke https://helm-charts.bitpoke.io helm install mysql-operator bitpoke/mysql-operator åœ¨å®‰è£…çš„chartåŒ…ä¸­ï¼Œvalues.yamlæ–‡ä»¶æœ‰äº›å†…å®¹å¯ä»¥è‡ªå®šä¹‰é…ç½®ã€‚ å¦‚æœæƒ³è¦åœ¨ä¸€ä¸ªk8sé›†ç¾¤é‡Œé¢å¯åŠ¨å¤šä¸ªmysql-operatorï¼Œä¸æƒ³è®©ä¸ç”¨operatoräº’ç›¸å¹²æ‰°ï¼Œå°±å¯ä»¥æŒ‡å®šwatchNamespaceå‚æ•° å¦‚æœk8sé›†ç¾¤ä½¿ç”¨çš„storageClassä¸æ”¯æŒåŠ¨æ€è¿ç§»ï¼Œé‚£ä¹ˆoperatorçš„replicaCountå°±å¾—è®¾ç½®ä¸º3ï¼Œå¦åˆ™èŠ‚ç‚¹æŒ‚æ‰åæ— æ³•æ‹‰èµ·æ–°çš„pod å› ä¸ºmysqlé«˜å¯ç”¨æ˜¯åŸºäºorchestratorå®ç°çš„ï¼Œorchestratoråˆéœ€è¦ä¸€ä¸ªå…ƒæ•°æ®åº“ï¼Œå…ƒæ•°æ®åº“çš„è´¦å·å¯†ç å¯ä»¥é€šè¿‡ orchestrator.topologyUser å’Œ orchestrator.topologyPassword æŒ‡å®šã€‚topologyUseré»˜è®¤å€¼æ˜¯ orchestratorï¼Œå¦‚æœå¯†ç ä¸æŒ‡å®šä¼šéšæœºç”Ÿæˆ (å…¶ä»–å‚æ•°ä»¥åç”¨åˆ°å†è¯´æ˜)â€¦ â€¦ ","date":"2023-03-12","objectID":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/:1:0","series":null,"tags":["mysql","operator"],"title":"bitpoke/mysql-operator æŠ€æœ¯è§£è¯»ç³»åˆ— - éƒ¨ç½²","uri":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/#mysql-operator-chart-è¯´æ˜"},{"categories":["mysql-operator"],"content":" åœ¨æœ¬åœ°éƒ¨ç½² mysql-operator æ·»åŠ  repo $ helm repo add bitpoke https://helm-charts.bitpoke.io å®‰è£… mysql-operator å®‰è£…ä¹‹å‰å…ˆåˆ›å»ºä¸€ä¸ªnamespaceï¼Œç„¶åå°†mysql-operatorå®‰è£…åˆ°è¿™ä¸ªnsä¸­ $ kubectl create namespace mysql namespace/mysql created $ helm install mysql-operator bitpoke/mysql-operator -n mysql NAME: mysql-operator LAST DEPLOYED: Fri Mar 17 18:53:26 2023 NAMESPACE: mysql STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: You can create a new cluster by issuing: cat \u003c\u003cEOF | kubectl apply -f- apiVersion: mysql.presslabs.org/v1alpha1 kind: MysqlCluster metadata: name: my-cluster spec: replicas: 1 secretName: my-cluster-secret --- apiVersion: v1 kind: Secret metadata: name: my-cluster-secret type: Opaque data: ROOT_PASSWORD: $(echo -n \"not-so-secure\" | base64) EOF æ‰§è¡ŒæˆåŠŸåï¼Œå¯ä»¥ç”¨kubectlå·¥å…·æŸ¥è¯¢podçŠ¶æ€ï¼Œç›´åˆ°Running $ kubectl get all -n mysql NAME READY STATUS RESTARTS AGE pod/mysql-operator-0 2/2 Running 1 3m11s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/mysql-operator ClusterIP 10.102.117.142 \u003cnone\u003e 80/TCP,9125/TCP 9m1s service/mysql-operator-orc ClusterIP None \u003cnone\u003e 80/TCP,10008/TCP 9m1s NAME READY AGE statefulset.apps/mysql-operator 1/1 9m1s è‡³æ­¤ mysql-operator å°±ç®—å®‰è£…å®Œæˆäº†ã€‚ ","date":"2023-03-12","objectID":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/:2:0","series":null,"tags":["mysql","operator"],"title":"bitpoke/mysql-operator æŠ€æœ¯è§£è¯»ç³»åˆ— - éƒ¨ç½²","uri":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/#åœ¨æœ¬åœ°éƒ¨ç½²-mysql-operator"},{"categories":["mysql-operator"],"content":" mysql-operator ä¾èµ–èµ„æºè§£è¯»ä¸Šè¿°çš„å®‰è£…æµç¨‹æ­¥éª¤å¾ˆç®€å•ï¼Œä½†æ˜¯è¿è¡Œèµ·æ¥çš„mysql-operatorå¯¹æˆ‘ä»¬æ¥è¯´å°±æ˜¯ä¸€ä¸ªé»‘ç›’ï¼Œä¸‹é¢é€šè¿‡æ‰§è¡Œ helm template å¯¹å…¶è¢«å®‰è£…çš„å†…å®¹è¿›è¡Œæ¢³ç†ã€‚ ","date":"2023-03-12","objectID":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/:3:0","series":null,"tags":["mysql","operator"],"title":"bitpoke/mysql-operator æŠ€æœ¯è§£è¯»ç³»åˆ— - éƒ¨ç½²","uri":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/#mysql-operator-ä¾èµ–èµ„æºè§£è¯»"},{"categories":["mysql-operator"],"content":" CRD èµ„æºCRD å…¨ç¨‹å«åš CustomResourceDefinitionï¼Œè¯¦ç»†ä¿¡æ¯å¯ä»¥ä»å®˜ç½‘è·å–ï¼Œè¿™é‡Œä¸è¿›è¡Œè§£é‡Šã€‚ $ helm template mysql-operator bitpoke/mysql-operator -n mysql --include-crds | grep -A 20 CustomResourceDefinition | grep -e kind kind: CustomResourceDefinition kind: MysqlBackup kind: CustomResourceDefinition kind: MysqlCluster kind: CustomResourceDefinition kind: MysqlDatabase kind: CustomResourceDefinition kind: MysqlUser ä»ä¸Šé¢å¯ä»¥çœ‹åˆ°ï¼Œå®‰è£…mysql-operatorçš„æ—¶å€™ä¼šåˆ›å»º4ä¸ªCRDèµ„æºã€‚ä»åç§°å¯ä»¥çœ‹å‡ºæ˜¯åˆ†åˆ«æ˜¯å…³äºbackupã€clusterã€databaseã€user ç›¸å…³çš„ï¼Œä¹Ÿæ˜¯æœ¬ç³»åˆ—åœ¨åç»­ç¯‡ç« ä¸­è¦é‡ç‚¹è®²è§£çš„å†…å®¹ã€‚ ","date":"2023-03-12","objectID":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/:3:1","series":null,"tags":["mysql","operator"],"title":"bitpoke/mysql-operator æŠ€æœ¯è§£è¯»ç³»åˆ— - éƒ¨ç½²","uri":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/#crd-èµ„æº"},{"categories":["mysql-operator"],"content":" å…¶ä»–èµ„æºæŸ¥çœ‹ mysql-operator ä¾èµ–çš„å…¶ä»–èµ„æº $ helm template mysql-operator bitpoke/mysql-operator -n mysql | grep -e ^kind -e name | grep -A 1 ^kind kind: ServiceAccount name: mysql-operator -- kind: Secret name: mysql-operator-orc -- kind: ConfigMap name: mysql-operator-orc -- kind: ClusterRole name: mysql-operator -- kind: ClusterRoleBinding name: mysql-operator -- kind: Service name: mysql-operator-orc -- kind: Service name: mysql-operator -- kind: StatefulSet name: mysql-operator ä»ä¸Šå¯ä»¥çœ‹å‡ºï¼Œè®¡ç®—èµ„æºç”±StatefulSetç®¡ç†ï¼Œæœ‰2ä¸ªServiceèµ„æºï¼Œ1ä¸ªConfigMapï¼Œ1ä¸ªSecretã€‚å…¶ä½™3ä¸ªæ˜¯è·Ÿæƒé™ç®¡ç†ç›¸å…³çš„é…ç½®ã€‚ æ¥ä¸‹æ¥è¯¦ç»†èŠèŠæ¯ä¸ªèµ„æºçš„ä½œç”¨ã€‚ ","date":"2023-03-12","objectID":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/:3:2","series":null,"tags":["mysql","operator"],"title":"bitpoke/mysql-operator æŠ€æœ¯è§£è¯»ç³»åˆ— - éƒ¨ç½²","uri":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/#å…¶ä»–èµ„æº"},{"categories":["mysql-operator"],"content":" StatefulSethelm install å®‰è£…æ—¶çš„å†…å®¹å¦‚ä¸‹ï¼š # Source: mysql-operator/templates/statefulset.yaml apiVersion: apps/v1 kind: StatefulSet metadata: name: mysql-operator labels: helm.sh/chart: mysql-operator-0.6.2 app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator app.kubernetes.io/version: \"v0.6.2\" app.kubernetes.io/managed-by: Helm spec: replicas: 1 serviceName: mysql-operator-orc podManagementPolicy: Parallel selector: matchLabels: app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator template: metadata: annotations: checksum/orchestrator-config: 301f994fcecde72ab6be4371173a860c68b440504210a400a8105c833311443b checksum/orchestrator-secret: 20304c64003f30460df7e5cdcc078d3bc55b882af412ed1d43ced6a765f1c160 labels: app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator spec: serviceAccountName: mysql-operator securityContext: fsGroup: 65532 runAsGroup: 65532 runAsNonRoot: true runAsUser: 65532 containers: - name: operator securityContext: {} image: \"docker.io/bitpoke/mysql-operator:v0.6.2\" imagePullPolicy: IfNotPresent ports: - containerPort: 8080 name: prometheus protocol: TCP env: - name: ORC_TOPOLOGY_USER valueFrom: secretKeyRef: name: mysql-operator-orc key: TOPOLOGY_USER - name: ORC_TOPOLOGY_PASSWORD valueFrom: secretKeyRef: name: mysql-operator-orc key: TOPOLOGY_PASSWORD args: - --leader-election-namespace=mysql - --orchestrator-uri=http://mysql-operator.mysql/api - --sidecar-image=docker.io/bitpoke/mysql-operator-sidecar-5.7:v0.6.2 - --sidecar-mysql8-image=docker.io/bitpoke/mysql-operator-sidecar-8.0:v0.6.2 - --failover-before-shutdown=true livenessProbe: httpGet: path: /healthz port: 8081 readinessProbe: httpGet: path: /readyz port: 8081 resources: {} - name: orchestrator securityContext: {} image: docker.io/bitpoke/mysql-operator-orchestrator:v0.6.2 imagePullPolicy: IfNotPresent ports: - containerPort: 3000 name: http protocol: TCP - containerPort: 10008 name: raft protocol: TCP env: - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP envFrom: - prefix: ORC_ secretRef: name: mysql-operator-orc volumeMounts: - name: data mountPath: /var/lib/orchestrator - name: config mountPath: /usr/local/share/orchestrator/templates livenessProbe: timeoutSeconds: 10 initialDelaySeconds: 200 httpGet: path: /api/lb-check port: 3000 # https://github.com/github/orchestrator/blob/master/docs/raft.md#proxy-healthy-raft-nodes readinessProbe: timeoutSeconds: 10 httpGet: path: /api/raft-health port: 3000 resources: {} volumes: - name: config configMap: name: mysql-operator-orc volumeClaimTemplates: - metadata: name: data spec: accessModes: [ ReadWriteOnce ] resources: requests: storage: 1Gi ä»æè¿°ä¸­å¯ä»¥çœ‹åˆ°ï¼Œä¸€ä¸ªpodé‡Œé¢ä¼šæœ‰ä¸¤ä¸ªcontainerï¼Œåˆ†åˆ«å¯åŠ¨ orchestrator å’Œ operatorã€‚ operator å®¹å™¨ ä½¿ç”¨åå­—ä¸º mysql-operator çš„ serviceAccount æ¥åšäº‹ä»¶å¤„ç† å£°æ˜ 8080 çš„ prometheus ç«¯å£ åœ¨ livenessProbe å’Œ readinessProbe é‡Œé¢å‡ºç°äº† 8081 ç«¯å£ï¼Œä½†æ²¡æœ‰å£°æ˜ï¼Œæ‰€ä»¥åªèƒ½ locahost ä½¿ç”¨ã€‚ éœ€è¦ä» mysql-operator-orc secret è·å–ä¸¤ä¸ªå€¼å½“åšè‡ªå·±çš„ç¯å¢ƒå˜é‡ ORC_TOPOLOGY_USER å’Œ ORC_TOPOLOGY_PASSWORDï¼› åŒæ—¶è¿˜æœ‰ä¸€äº›å¯åŠ¨å‚æ•° - --leader-election-namespace=mysql # å¦‚æœå®‰è£…æ—¶podä¸æ˜¯å•å‰¯æœ¬ï¼Œéœ€è¦é€‰ä¸»ï¼Œè¿™é‡Œæ˜¯æŒ‡å®šé€‰ä¸»çš„ns - --orchestrator-uri=http://mysql-operator.mysql/api # operator ä¸ºäº†å’Œ orchestrator é€šä¿¡ - --sidecar-image=docker.io/bitpoke/mysql-operator-sidecar-5.7:v0.6.2 # 5.7 ç‰ˆæœ¬çš„ mysql sidecar é•œåƒ - --sidecar-mysql8-image=docker.io/bitpoke/mysql-operator-sidecar-8.0:v0.6.2 # 8.0 ç‰ˆæœ¬çš„ mysql sidecar é•œåƒã€‚å› ä¸º mysql ç‰ˆæœ¬å·®å¼‚å¯¼è‡´å¤‡ä»½æ¢å¤çš„ xtrabackup è½¯ä»¶ç‰ˆæœ¬ä¸åŒã€‚ - --failover-before-shutdown=true orchestrator å®¹å™¨ å£°æ˜ 3000 çš„ http æœåŠ¡ç«¯å£ å£°æ˜ 10008 çš„ raft æ¶ˆæ¯åŒæ­¥ç«¯å£ ä¹Ÿéœ€è¦ä» mysql-operator-orc secret è·å–å€¼å½“åšè‡ªå·±çš„ç¯å¢ƒå˜é‡ å£°æ˜ä¸€ä¸ª volume å¹¶æŒ‚è½½åˆ° /var/lib/orchestrator(è¿™é‡Œæ˜¯ä¸ºäº†å­˜å‚¨ orchestrator ç®¡ç†çš„å…ƒæ•°æ®) ä» mysql-operator-orc configmap è·å–é…ç½®å¹¶æŒ‚è½½åˆ° /usr/local/share/orchestrator/templates(orchestratorå¯åŠ¨çš„é…ç½®æ–‡ä»¶) ","date":"2023-03-12","objectID":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/:3:3","series":null,"tags":["mysql","operator"],"title":"bitpoke/mysql-operator æŠ€æœ¯è§£è¯»ç³»åˆ— - éƒ¨ç½²","uri":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/#statefulset"},{"categories":["mysql-operator"],"content":" StatefulSethelm install å®‰è£…æ—¶çš„å†…å®¹å¦‚ä¸‹ï¼š # Source: mysql-operator/templates/statefulset.yaml apiVersion: apps/v1 kind: StatefulSet metadata: name: mysql-operator labels: helm.sh/chart: mysql-operator-0.6.2 app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator app.kubernetes.io/version: \"v0.6.2\" app.kubernetes.io/managed-by: Helm spec: replicas: 1 serviceName: mysql-operator-orc podManagementPolicy: Parallel selector: matchLabels: app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator template: metadata: annotations: checksum/orchestrator-config: 301f994fcecde72ab6be4371173a860c68b440504210a400a8105c833311443b checksum/orchestrator-secret: 20304c64003f30460df7e5cdcc078d3bc55b882af412ed1d43ced6a765f1c160 labels: app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator spec: serviceAccountName: mysql-operator securityContext: fsGroup: 65532 runAsGroup: 65532 runAsNonRoot: true runAsUser: 65532 containers: - name: operator securityContext: {} image: \"docker.io/bitpoke/mysql-operator:v0.6.2\" imagePullPolicy: IfNotPresent ports: - containerPort: 8080 name: prometheus protocol: TCP env: - name: ORC_TOPOLOGY_USER valueFrom: secretKeyRef: name: mysql-operator-orc key: TOPOLOGY_USER - name: ORC_TOPOLOGY_PASSWORD valueFrom: secretKeyRef: name: mysql-operator-orc key: TOPOLOGY_PASSWORD args: - --leader-election-namespace=mysql - --orchestrator-uri=http://mysql-operator.mysql/api - --sidecar-image=docker.io/bitpoke/mysql-operator-sidecar-5.7:v0.6.2 - --sidecar-mysql8-image=docker.io/bitpoke/mysql-operator-sidecar-8.0:v0.6.2 - --failover-before-shutdown=true livenessProbe: httpGet: path: /healthz port: 8081 readinessProbe: httpGet: path: /readyz port: 8081 resources: {} - name: orchestrator securityContext: {} image: docker.io/bitpoke/mysql-operator-orchestrator:v0.6.2 imagePullPolicy: IfNotPresent ports: - containerPort: 3000 name: http protocol: TCP - containerPort: 10008 name: raft protocol: TCP env: - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP envFrom: - prefix: ORC_ secretRef: name: mysql-operator-orc volumeMounts: - name: data mountPath: /var/lib/orchestrator - name: config mountPath: /usr/local/share/orchestrator/templates livenessProbe: timeoutSeconds: 10 initialDelaySeconds: 200 httpGet: path: /api/lb-check port: 3000 # https://github.com/github/orchestrator/blob/master/docs/raft.md#proxy-healthy-raft-nodes readinessProbe: timeoutSeconds: 10 httpGet: path: /api/raft-health port: 3000 resources: {} volumes: - name: config configMap: name: mysql-operator-orc volumeClaimTemplates: - metadata: name: data spec: accessModes: [ ReadWriteOnce ] resources: requests: storage: 1Gi ä»æè¿°ä¸­å¯ä»¥çœ‹åˆ°ï¼Œä¸€ä¸ªpodé‡Œé¢ä¼šæœ‰ä¸¤ä¸ªcontainerï¼Œåˆ†åˆ«å¯åŠ¨ orchestrator å’Œ operatorã€‚ operator å®¹å™¨ ä½¿ç”¨åå­—ä¸º mysql-operator çš„ serviceAccount æ¥åšäº‹ä»¶å¤„ç† å£°æ˜ 8080 çš„ prometheus ç«¯å£ åœ¨ livenessProbe å’Œ readinessProbe é‡Œé¢å‡ºç°äº† 8081 ç«¯å£ï¼Œä½†æ²¡æœ‰å£°æ˜ï¼Œæ‰€ä»¥åªèƒ½ locahost ä½¿ç”¨ã€‚ éœ€è¦ä» mysql-operator-orc secret è·å–ä¸¤ä¸ªå€¼å½“åšè‡ªå·±çš„ç¯å¢ƒå˜é‡ ORC_TOPOLOGY_USER å’Œ ORC_TOPOLOGY_PASSWORDï¼› åŒæ—¶è¿˜æœ‰ä¸€äº›å¯åŠ¨å‚æ•° - --leader-election-namespace=mysql # å¦‚æœå®‰è£…æ—¶podä¸æ˜¯å•å‰¯æœ¬ï¼Œéœ€è¦é€‰ä¸»ï¼Œè¿™é‡Œæ˜¯æŒ‡å®šé€‰ä¸»çš„ns - --orchestrator-uri=http://mysql-operator.mysql/api # operator ä¸ºäº†å’Œ orchestrator é€šä¿¡ - --sidecar-image=docker.io/bitpoke/mysql-operator-sidecar-5.7:v0.6.2 # 5.7 ç‰ˆæœ¬çš„ mysql sidecar é•œåƒ - --sidecar-mysql8-image=docker.io/bitpoke/mysql-operator-sidecar-8.0:v0.6.2 # 8.0 ç‰ˆæœ¬çš„ mysql sidecar é•œåƒã€‚å› ä¸º mysql ç‰ˆæœ¬å·®å¼‚å¯¼è‡´å¤‡ä»½æ¢å¤çš„ xtrabackup è½¯ä»¶ç‰ˆæœ¬ä¸åŒã€‚ - --failover-before-shutdown=true orchestrator å®¹å™¨ å£°æ˜ 3000 çš„ http æœåŠ¡ç«¯å£ å£°æ˜ 10008 çš„ raft æ¶ˆæ¯åŒæ­¥ç«¯å£ ä¹Ÿéœ€è¦ä» mysql-operator-orc secret è·å–å€¼å½“åšè‡ªå·±çš„ç¯å¢ƒå˜é‡ å£°æ˜ä¸€ä¸ª volume å¹¶æŒ‚è½½åˆ° /var/lib/orchestrator(è¿™é‡Œæ˜¯ä¸ºäº†å­˜å‚¨ orchestrator ç®¡ç†çš„å…ƒæ•°æ®) ä» mysql-operator-orc configmap è·å–é…ç½®å¹¶æŒ‚è½½åˆ° /usr/local/share/orchestrator/templates(orchestratorå¯åŠ¨çš„é…ç½®æ–‡ä»¶) ","date":"2023-03-12","objectID":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/:3:3","series":null,"tags":["mysql","operator"],"title":"bitpoke/mysql-operator æŠ€æœ¯è§£è¯»ç³»åˆ— - éƒ¨ç½²","uri":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/#operator-å®¹å™¨"},{"categories":["mysql-operator"],"content":" StatefulSethelm install å®‰è£…æ—¶çš„å†…å®¹å¦‚ä¸‹ï¼š # Source: mysql-operator/templates/statefulset.yaml apiVersion: apps/v1 kind: StatefulSet metadata: name: mysql-operator labels: helm.sh/chart: mysql-operator-0.6.2 app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator app.kubernetes.io/version: \"v0.6.2\" app.kubernetes.io/managed-by: Helm spec: replicas: 1 serviceName: mysql-operator-orc podManagementPolicy: Parallel selector: matchLabels: app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator template: metadata: annotations: checksum/orchestrator-config: 301f994fcecde72ab6be4371173a860c68b440504210a400a8105c833311443b checksum/orchestrator-secret: 20304c64003f30460df7e5cdcc078d3bc55b882af412ed1d43ced6a765f1c160 labels: app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator spec: serviceAccountName: mysql-operator securityContext: fsGroup: 65532 runAsGroup: 65532 runAsNonRoot: true runAsUser: 65532 containers: - name: operator securityContext: {} image: \"docker.io/bitpoke/mysql-operator:v0.6.2\" imagePullPolicy: IfNotPresent ports: - containerPort: 8080 name: prometheus protocol: TCP env: - name: ORC_TOPOLOGY_USER valueFrom: secretKeyRef: name: mysql-operator-orc key: TOPOLOGY_USER - name: ORC_TOPOLOGY_PASSWORD valueFrom: secretKeyRef: name: mysql-operator-orc key: TOPOLOGY_PASSWORD args: - --leader-election-namespace=mysql - --orchestrator-uri=http://mysql-operator.mysql/api - --sidecar-image=docker.io/bitpoke/mysql-operator-sidecar-5.7:v0.6.2 - --sidecar-mysql8-image=docker.io/bitpoke/mysql-operator-sidecar-8.0:v0.6.2 - --failover-before-shutdown=true livenessProbe: httpGet: path: /healthz port: 8081 readinessProbe: httpGet: path: /readyz port: 8081 resources: {} - name: orchestrator securityContext: {} image: docker.io/bitpoke/mysql-operator-orchestrator:v0.6.2 imagePullPolicy: IfNotPresent ports: - containerPort: 3000 name: http protocol: TCP - containerPort: 10008 name: raft protocol: TCP env: - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP envFrom: - prefix: ORC_ secretRef: name: mysql-operator-orc volumeMounts: - name: data mountPath: /var/lib/orchestrator - name: config mountPath: /usr/local/share/orchestrator/templates livenessProbe: timeoutSeconds: 10 initialDelaySeconds: 200 httpGet: path: /api/lb-check port: 3000 # https://github.com/github/orchestrator/blob/master/docs/raft.md#proxy-healthy-raft-nodes readinessProbe: timeoutSeconds: 10 httpGet: path: /api/raft-health port: 3000 resources: {} volumes: - name: config configMap: name: mysql-operator-orc volumeClaimTemplates: - metadata: name: data spec: accessModes: [ ReadWriteOnce ] resources: requests: storage: 1Gi ä»æè¿°ä¸­å¯ä»¥çœ‹åˆ°ï¼Œä¸€ä¸ªpodé‡Œé¢ä¼šæœ‰ä¸¤ä¸ªcontainerï¼Œåˆ†åˆ«å¯åŠ¨ orchestrator å’Œ operatorã€‚ operator å®¹å™¨ ä½¿ç”¨åå­—ä¸º mysql-operator çš„ serviceAccount æ¥åšäº‹ä»¶å¤„ç† å£°æ˜ 8080 çš„ prometheus ç«¯å£ åœ¨ livenessProbe å’Œ readinessProbe é‡Œé¢å‡ºç°äº† 8081 ç«¯å£ï¼Œä½†æ²¡æœ‰å£°æ˜ï¼Œæ‰€ä»¥åªèƒ½ locahost ä½¿ç”¨ã€‚ éœ€è¦ä» mysql-operator-orc secret è·å–ä¸¤ä¸ªå€¼å½“åšè‡ªå·±çš„ç¯å¢ƒå˜é‡ ORC_TOPOLOGY_USER å’Œ ORC_TOPOLOGY_PASSWORDï¼› åŒæ—¶è¿˜æœ‰ä¸€äº›å¯åŠ¨å‚æ•° - --leader-election-namespace=mysql # å¦‚æœå®‰è£…æ—¶podä¸æ˜¯å•å‰¯æœ¬ï¼Œéœ€è¦é€‰ä¸»ï¼Œè¿™é‡Œæ˜¯æŒ‡å®šé€‰ä¸»çš„ns - --orchestrator-uri=http://mysql-operator.mysql/api # operator ä¸ºäº†å’Œ orchestrator é€šä¿¡ - --sidecar-image=docker.io/bitpoke/mysql-operator-sidecar-5.7:v0.6.2 # 5.7 ç‰ˆæœ¬çš„ mysql sidecar é•œåƒ - --sidecar-mysql8-image=docker.io/bitpoke/mysql-operator-sidecar-8.0:v0.6.2 # 8.0 ç‰ˆæœ¬çš„ mysql sidecar é•œåƒã€‚å› ä¸º mysql ç‰ˆæœ¬å·®å¼‚å¯¼è‡´å¤‡ä»½æ¢å¤çš„ xtrabackup è½¯ä»¶ç‰ˆæœ¬ä¸åŒã€‚ - --failover-before-shutdown=true orchestrator å®¹å™¨ å£°æ˜ 3000 çš„ http æœåŠ¡ç«¯å£ å£°æ˜ 10008 çš„ raft æ¶ˆæ¯åŒæ­¥ç«¯å£ ä¹Ÿéœ€è¦ä» mysql-operator-orc secret è·å–å€¼å½“åšè‡ªå·±çš„ç¯å¢ƒå˜é‡ å£°æ˜ä¸€ä¸ª volume å¹¶æŒ‚è½½åˆ° /var/lib/orchestrator(è¿™é‡Œæ˜¯ä¸ºäº†å­˜å‚¨ orchestrator ç®¡ç†çš„å…ƒæ•°æ®) ä» mysql-operator-orc configmap è·å–é…ç½®å¹¶æŒ‚è½½åˆ° /usr/local/share/orchestrator/templates(orchestratorå¯åŠ¨çš„é…ç½®æ–‡ä»¶) ","date":"2023-03-12","objectID":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/:3:3","series":null,"tags":["mysql","operator"],"title":"bitpoke/mysql-operator æŠ€æœ¯è§£è¯»ç³»åˆ— - éƒ¨ç½²","uri":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/#orchestrator-å®¹å™¨"},{"categories":["mysql-operator"],"content":" ConfigMaphelm install å®‰è£…æ—¶çš„å†…å®¹å¦‚ä¸‹ï¼š # Source: mysql-operator/templates/orchestrator-config.yaml apiVersion: v1 kind: ConfigMap metadata: name: mysql-operator-orc labels: helm.sh/chart: mysql-operator-0.6.2 app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator app.kubernetes.io/version: \"v0.6.2\" app.kubernetes.io/managed-by: Helm data: orchestrator.conf.json: \"{..........}\" orc-topology.cnf: | [client] user = {{ .Env.ORC_TOPOLOGY_USER }} password = {{ .Env.ORC_TOPOLOGY_PASSWORD }} åœ¨ StatefulSet éƒ¨åˆ†å·²ç»è§£é‡Š orchestrator.conf.json æ˜¯ orchestrator å¯åŠ¨éœ€è¦ä½¿ç”¨çš„é…ç½®æ–‡ä»¶ã€‚ orc-topology.cnf æ˜¯ orchestrator è®¿é—®æ•°æ®åº“çš„è´¦å·å¯†ç ã€‚ é…ç½®ä¿¡æ¯å¦‚ä¸‹ï¼ˆç›¸å…³æè¿°å¯ä»¥ä»å®˜ç½‘æŸ¥é˜…ï¼‰ï¼š $ cat /usr/local/share/orchestrator/templates/orchestrator.conf.json { \"ApplyMySQLPromotionAfterMasterFailover\": true, \"BackendDB\": \"sqlite\", \"Debug\": false, \"DetachLostReplicasAfterMasterFailover\": true, \"DetectClusterAliasQuery\": \"SELECT CONCAT(SUBSTRING(@@hostname, 1, LENGTH(@@hostname) - 1 - LENGTH(SUBSTRING_INDEX(@@hostname,'-',-2))),'.',SUBSTRING_INDEX(@@report_host,'.',-1))\", \"DetectInstanceAliasQuery\": \"SELECT @@hostname\", \"DiscoverByShowSlaveHosts\": false, \"FailMasterPromotionIfSQLThreadNotUpToDate\": true, \"HTTPAdvertise\": \"http://{{ .Env.HOSTNAME }}.mysql-operator-orc:3000\", \"HostnameResolveMethod\": \"none\", \"InstancePollSeconds\": 5, \"ListenAddress\": \":3000\", \"MasterFailoverLostInstancesDowntimeMinutes\": 10, \"MySQLHostnameResolveMethod\": \"@@report_host\", \"MySQLTopologyCredentialsConfigFile\": \"/etc/orchestrator/orc-topology.cnf\", \"OnFailureDetectionProcesses\": [ \"/usr/local/bin/orc-helper event -w '{failureClusterAlias}' 'OrcFailureDetection' 'Failure: {failureType}, failed host: {failedHost}, lost replcas: {lostReplicas}' || true\", \"/usr/local/bin/orc-helper failover-in-progress '{failureClusterAlias}' '{failureDescription}' || true\" ], \"PostIntermediateMasterFailoverProcesses\": [ \"/usr/local/bin/orc-helper event '{failureClusterAlias}' 'OrcPostIntermediateMasterFailover' 'Failure type: {failureType}, failed hosts: {failedHost}, slaves: {countSlaves}' || true\" ], \"PostMasterFailoverProcesses\": [ \"/usr/local/bin/orc-helper event '{failureClusterAlias}' 'OrcPostMasterFailover' 'Failure type: {failureType}, new master: {successorHost}, slaves: {slaveHosts}' || true\" ], \"PostUnsuccessfulFailoverProcesses\": [ \"/usr/local/bin/orc-helper event -w '{failureClusterAlias}' 'OrcPostUnsuccessfulFailover' 'Failure: {failureType}, failed host: {failedHost} with {countSlaves} slaves' || true\" ], \"PreFailoverProcesses\": [ \"/usr/local/bin/orc-helper failover-in-progress '{failureClusterAlias}' '{failureDescription}' || true\" ], \"ProcessesShellCommand\": \"sh\", \"RaftAdvertise\": \"{{ .Env.HOSTNAME }}.mysql-operator-orc\", \"RaftBind\": \"{{ .Env.HOSTNAME }}\", \"RaftDataDir\": \"/var/lib/orchestrator\", \"RaftEnabled\": true, \"RaftNodes\": [], \"RecoverIntermediateMasterClusterFilters\": [ \".*\" ], \"RecoverMasterClusterFilters\": [ \".*\" ], \"RecoveryIgnoreHostnameFilters\": [], \"RecoveryPeriodBlockSeconds\": 300, \"RemoveTextFromHostnameDisplay\": \":3306\", \"SQLite3DataFile\": \"/var/lib/orchestrator/orc.db\", \"SlaveLagQuery\": \"SELECT TIMESTAMPDIFF(SECOND,ts,UTC_TIMESTAMP()) as drift FROM sys_operator.heartbeat ORDER BY drift ASC LIMIT 1\", \"UnseenInstanceForgetHours\": 1 } ","date":"2023-03-12","objectID":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/:3:4","series":null,"tags":["mysql","operator"],"title":"bitpoke/mysql-operator æŠ€æœ¯è§£è¯»ç³»åˆ— - éƒ¨ç½²","uri":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/#configmap"},{"categories":["mysql-operator"],"content":" Secrethelm install å®‰è£…æ—¶çš„å†…å®¹å¦‚ä¸‹ï¼š # Source: mysql-operator/templates/orchestrator-secret.yaml apiVersion: v1 kind: Secret metadata: name: mysql-operator-orc labels: helm.sh/chart: mysql-operator-0.6.2 app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator app.kubernetes.io/version: \"v0.6.2\" app.kubernetes.io/managed-by: Helm data: TOPOLOGY_USER: \"b3JjaGVzdHJhdG9y\" TOPOLOGY_PASSWORD: \"aVdZZndhMzJHZw==\" ä» key å¯ä»¥çœ‹å‡ºè¿™å°±æ˜¯ orchestrator éœ€è¦çš„å…ƒæ•°æ®è´¦å·å¯†ç ä¿¡æ¯ï¼Œä½¿ç”¨base64è§£æå‡ºåŸæ–‡ $ echo 'b3JjaGVzdHJhdG9y' | base64 -d orchestrator $ echo 'aVdZZndhMzJHZw==' | base64 -d iWYfwa32Gg ","date":"2023-03-12","objectID":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/:3:5","series":null,"tags":["mysql","operator"],"title":"bitpoke/mysql-operator æŠ€æœ¯è§£è¯»ç³»åˆ— - éƒ¨ç½²","uri":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/#secret"},{"categories":["mysql-operator"],"content":" Servicehelm install å®‰è£…æ—¶çš„å†…å®¹å¦‚ä¸‹ï¼š # Source: mysql-operator/templates/service.yaml apiVersion: v1 kind: Service metadata: name: mysql-operator labels: helm.sh/chart: mysql-operator-0.6.2 app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator app.kubernetes.io/version: \"v0.6.2\" app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: operator spec: type: ClusterIP ports: - port: 80 name: http protocol: TCP targetPort: http - port: 9125 name: prometheus protocol: TCP targetPort: prometheus selector: app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator --- # Source: mysql-operator/templates/orchestrator-raft-service.yaml apiVersion: v1 kind: Service metadata: name: mysql-operator-orc labels: app.kubernetes.io/component: orchestrator-raft helm.sh/chart: mysql-operator-0.6.2 app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator app.kubernetes.io/version: \"v0.6.2\" app.kubernetes.io/managed-by: Helm spec: type: ClusterIP clusterIP: None publishNotReadyAddresses: true ports: - name: http port: 80 targetPort: 3000 - name: raft port: 10008 targetPort: 10008 selector: app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator ä»¥ä¸Šæœ‰2ä¸ªserviceï¼Œè™½ç„¶ type éƒ½æ˜¯ ClusterIPï¼Œä½†æ˜¯ mysql-operator-orc è¿˜æœ‰ä¸ª clusterIP: Noneï¼Œè¿™ä¸ªé…ç½®è¡¨ç¤ºè¯¥ service æ˜¯ä¸ª Headless Serviceï¼Œä¸éœ€è¦é¢å¤–çš„ ipã€‚ mysql-operator-orc æœ‰ä¸¤ä¸ªç«¯å£ 80ï¼štargetPort æ˜¯ 3000ï¼Œorchestrator çš„å¯è§†åŒ–é¡µé¢å’ŒapiæœåŠ¡æ¥å£ 10008ï¼štargetPort æ˜¯ 10008ï¼Œè¿™ä¸ªç«¯å£æ˜¯ orchestrator å¤šå‰¯æœ¬ä¹‹å‰è¿›è¡Œ raft æ¶ˆæ¯åŒæ­¥ä½¿ç”¨ mysql-operator ä¹Ÿæœ‰ä¸¤ä¸ªç«¯å£ 80ï¼štargetPort æ˜¯ httpï¼ŒæŸ¥è¯¢å‘ç°è¿˜æ˜¯ orchestrator çš„ 3000 ç«¯å£ 9125ï¼štargetPort æ˜¯ prometheusï¼Œprometheus åœ¨ StatefulSetä¸­è¢«å®šä¹‰åˆ°äº† operator å®¹å™¨çš„ 8080 ç«¯å£ï¼Œé€šè¿‡ curl http://127.0.0.1:8080/metrics å¯ä»¥æŸ¥çœ‹åˆ° operator çš„æŒ‡æ ‡æ•°æ® ","date":"2023-03-12","objectID":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/:3:6","series":null,"tags":["mysql","operator"],"title":"bitpoke/mysql-operator æŠ€æœ¯è§£è¯»ç³»åˆ— - éƒ¨ç½²","uri":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/#service"},{"categories":["mysql-operator"],"content":" ServiceAccountã€ClusterRoleã€ClusterRoleBindinghelm install å®‰è£…æ—¶çš„å†…å®¹å¦‚ä¸‹ï¼š # Source: mysql-operator/templates/serviceaccount.yaml apiVersion: v1 kind: ServiceAccount metadata: name: mysql-operator labels: helm.sh/chart: mysql-operator-0.6.2 app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator app.kubernetes.io/version: \"v0.6.2\" app.kubernetes.io/managed-by: Helm --- # Source: mysql-operator/templates/clusterrole.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: mysql-operator labels: helm.sh/chart: mysql-operator-0.6.2 app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator app.kubernetes.io/version: \"v0.6.2\" app.kubernetes.io/managed-by: Helm rules: - apiGroups: - apps resources: - statefulsets verbs: - create - delete - get - list - patch - update - watch - apiGroups: - batch resources: - jobs verbs: - create - delete - get - list - patch - update - watch - apiGroups: - coordination.k8s.io resources: - leases verbs: - create - delete - get - list - patch - update - watch - apiGroups: - \"\" resources: - configmaps - events - jobs - persistentvolumeclaims - pods - secrets - services verbs: - create - delete - get - list - patch - update - watch - apiGroups: - \"\" resources: - pods/status verbs: - create - delete - get - list - patch - update - watch - apiGroups: - mysql.presslabs.org resources: - mysqlbackups verbs: - create - delete - get - list - patch - update - watch - apiGroups: - mysql.presslabs.org resources: - mysqlclusters - mysqlclusters/status verbs: - create - delete - get - list - patch - update - watch - apiGroups: - mysql.presslabs.org resources: - mysqldatabases - mysqldatabases/status verbs: - create - delete - get - list - patch - update - watch - apiGroups: - mysql.presslabs.org resources: - mysqlusers - mysqlusers/status verbs: - create - delete - get - list - patch - update - watch - apiGroups: - policy resources: - poddisruptionbudgets verbs: - create - delete - get - list - patch - update - watch --- # Source: mysql-operator/templates/clusterrolebinding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: mysql-operator labels: helm.sh/chart: mysql-operator-0.6.2 app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator app.kubernetes.io/version: \"v0.6.2\" app.kubernetes.io/managed-by: Helm roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: mysql-operator subjects: - name: mysql-operator namespace: \"mysql\" kind: ServiceAccount åœ¨è¿™é‡Œè¦å¼•å…¥k8sä¸€ä¸ªé‡è¦çš„æ¦‚å¿µ RBAC(Role Base Access Control)ï¼Œæ˜¯ä¸€ç§æƒé™æ§åˆ¶æœºåˆ¶ï¼Œç”¨äºå®ç°è´¦æˆ·å’Œæƒé™çš„ç»„åˆç®¡ç† é€šè¿‡ ClusterRole æ¥å£°æ˜éƒ½èƒ½æ“ä½œé‚£äº›apiï¼› é€šè¿‡ ServiceAccount åˆ›å»ºä¸€ä¸ªè´¦æˆ·ï¼› æœ€åé€šè¿‡ ClusterRoleBinding å°†ä¸¤è€…ç»‘å®šã€‚ è¡¥å……å¦å¤–æ¯æ¬¡åˆ›å»ºä¸€ä¸ª sa çš„æ—¶å€™éƒ½ä¼šç”Ÿæˆä¸€ä¸ªå¯¹åº”çš„ token secretï¼š $ kubectl create namespace sa-test namespace/sa-test created $ kubectl create serviceaccount test serviceaccount/test created $ kubectl get secret -n sa-test NAME TYPE DATA AGE default-token-6mdbt kubernetes.io/service-account-token 3 31s $ kubectl get secret -n sa-test -o yaml apiVersion: v1 items: - apiVersion: v1 data: ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJek1ETXhOekF5TlRrek1sb1hEVE16TURNeE5EQXlOVGt6TWxvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBT3lKCm1ScE51Q21tVGM5dXNKbVJGMEo0Umw1bElQeE1iM0psQ0dONGF5Ry9IZXFEcmJlZnFLMzZJWmFYZVhNbi9Zb0wKMkM3Mktmd1Z3UnJuUldmZkt6L3k0UFR1bkRVUHpNa1BOMloybVRmbENuRG1ENEVjdGk4SVVNeS81ZWtVZ0kvLwppaEh6MllrKzVLb1RISmozc1VrTFRyV2llc0E3WVV6WkZnTGRmZkU2Yjh1WVExSzZtTW1yeWtjSVdyTVJqNEMyCkZ1SjM2a1VqNEJWK1luRVRMNnAxb1FxZWJqa1I2dFZWajZvb25ZNmNHejBUc0JldE03M3FrRTBBUnBFVm1kb3cKSWY1MmVJN0U0WFBuNWtHQk9RZ3k1OE5tNHdzQmEvYTlIbEtCSUFMUm9vaDdyamhHbmpCT1ZtNnFMcDRjdUVkUwpqellONE9WdWtaQ2M1cGRLRVdNQ0F3RUFBYU5DTUVBd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZFME5mWDRsdnEyRjF3MXlVVElYSE5QNVRGSFVNQTBHQ1NxR1NJYjMKRFFFQkN3VUFBNElCQVFC","date":"2023-03-12","objectID":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/:3:7","series":null,"tags":["mysql","operator"],"title":"bitpoke/mysql-operator æŠ€æœ¯è§£è¯»ç³»åˆ— - éƒ¨ç½²","uri":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/#serviceaccountclusterroleclusterrolebinding"},{"categories":["mysql-operator"],"content":" ServiceAccountã€ClusterRoleã€ClusterRoleBindinghelm install å®‰è£…æ—¶çš„å†…å®¹å¦‚ä¸‹ï¼š # Source: mysql-operator/templates/serviceaccount.yaml apiVersion: v1 kind: ServiceAccount metadata: name: mysql-operator labels: helm.sh/chart: mysql-operator-0.6.2 app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator app.kubernetes.io/version: \"v0.6.2\" app.kubernetes.io/managed-by: Helm --- # Source: mysql-operator/templates/clusterrole.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: mysql-operator labels: helm.sh/chart: mysql-operator-0.6.2 app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator app.kubernetes.io/version: \"v0.6.2\" app.kubernetes.io/managed-by: Helm rules: - apiGroups: - apps resources: - statefulsets verbs: - create - delete - get - list - patch - update - watch - apiGroups: - batch resources: - jobs verbs: - create - delete - get - list - patch - update - watch - apiGroups: - coordination.k8s.io resources: - leases verbs: - create - delete - get - list - patch - update - watch - apiGroups: - \"\" resources: - configmaps - events - jobs - persistentvolumeclaims - pods - secrets - services verbs: - create - delete - get - list - patch - update - watch - apiGroups: - \"\" resources: - pods/status verbs: - create - delete - get - list - patch - update - watch - apiGroups: - mysql.presslabs.org resources: - mysqlbackups verbs: - create - delete - get - list - patch - update - watch - apiGroups: - mysql.presslabs.org resources: - mysqlclusters - mysqlclusters/status verbs: - create - delete - get - list - patch - update - watch - apiGroups: - mysql.presslabs.org resources: - mysqldatabases - mysqldatabases/status verbs: - create - delete - get - list - patch - update - watch - apiGroups: - mysql.presslabs.org resources: - mysqlusers - mysqlusers/status verbs: - create - delete - get - list - patch - update - watch - apiGroups: - policy resources: - poddisruptionbudgets verbs: - create - delete - get - list - patch - update - watch --- # Source: mysql-operator/templates/clusterrolebinding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: mysql-operator labels: helm.sh/chart: mysql-operator-0.6.2 app.kubernetes.io/name: mysql-operator app.kubernetes.io/instance: mysql-operator app.kubernetes.io/version: \"v0.6.2\" app.kubernetes.io/managed-by: Helm roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: mysql-operator subjects: - name: mysql-operator namespace: \"mysql\" kind: ServiceAccount åœ¨è¿™é‡Œè¦å¼•å…¥k8sä¸€ä¸ªé‡è¦çš„æ¦‚å¿µ RBAC(Role Base Access Control)ï¼Œæ˜¯ä¸€ç§æƒé™æ§åˆ¶æœºåˆ¶ï¼Œç”¨äºå®ç°è´¦æˆ·å’Œæƒé™çš„ç»„åˆç®¡ç† é€šè¿‡ ClusterRole æ¥å£°æ˜éƒ½èƒ½æ“ä½œé‚£äº›apiï¼› é€šè¿‡ ServiceAccount åˆ›å»ºä¸€ä¸ªè´¦æˆ·ï¼› æœ€åé€šè¿‡ ClusterRoleBinding å°†ä¸¤è€…ç»‘å®šã€‚ è¡¥å……å¦å¤–æ¯æ¬¡åˆ›å»ºä¸€ä¸ª sa çš„æ—¶å€™éƒ½ä¼šç”Ÿæˆä¸€ä¸ªå¯¹åº”çš„ token secretï¼š $ kubectl create namespace sa-test namespace/sa-test created $ kubectl create serviceaccount test serviceaccount/test created $ kubectl get secret -n sa-test NAME TYPE DATA AGE default-token-6mdbt kubernetes.io/service-account-token 3 31s $ kubectl get secret -n sa-test -o yaml apiVersion: v1 items: - apiVersion: v1 data: ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJek1ETXhOekF5TlRrek1sb1hEVE16TURNeE5EQXlOVGt6TWxvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBT3lKCm1ScE51Q21tVGM5dXNKbVJGMEo0Umw1bElQeE1iM0psQ0dONGF5Ry9IZXFEcmJlZnFLMzZJWmFYZVhNbi9Zb0wKMkM3Mktmd1Z3UnJuUldmZkt6L3k0UFR1bkRVUHpNa1BOMloybVRmbENuRG1ENEVjdGk4SVVNeS81ZWtVZ0kvLwppaEh6MllrKzVLb1RISmozc1VrTFRyV2llc0E3WVV6WkZnTGRmZkU2Yjh1WVExSzZtTW1yeWtjSVdyTVJqNEMyCkZ1SjM2a1VqNEJWK1luRVRMNnAxb1FxZWJqa1I2dFZWajZvb25ZNmNHejBUc0JldE03M3FrRTBBUnBFVm1kb3cKSWY1MmVJN0U0WFBuNWtHQk9RZ3k1OE5tNHdzQmEvYTlIbEtCSUFMUm9vaDdyamhHbmpCT1ZtNnFMcDRjdUVkUwpqellONE9WdWtaQ2M1cGRLRVdNQ0F3RUFBYU5DTUVBd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZFME5mWDRsdnEyRjF3MXlVVElYSE5QNVRGSFVNQTBHQ1NxR1NJYjMKRFFFQkN3VUFBNElCQVFC","date":"2023-03-12","objectID":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/:3:7","series":null,"tags":["mysql","operator"],"title":"bitpoke/mysql-operator æŠ€æœ¯è§£è¯»ç³»åˆ— - éƒ¨ç½²","uri":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/#è¡¥å……"},{"categories":["mysql-operator"],"content":" æ€»ç»“è‡³æ­¤ï¼Œé€šè¿‡å®‰è£…mysql-operatorå’Œåˆ†æchartåˆæ­¥äº†è§£äº†èµ„æºç»„æˆï¼Œåç»­æ–‡ç« æˆ‘ä»¬è¿›å…¥åˆ°mysql-operatorå†…éƒ¨ï¼Œçœ‹çœ‹åŸç†æ˜¯å¦‚ä½•å®ç°çš„ã€‚ ","date":"2023-03-12","objectID":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/:4:0","series":null,"tags":["mysql","operator"],"title":"bitpoke/mysql-operator æŠ€æœ¯è§£è¯»ç³»åˆ— - éƒ¨ç½²","uri":"/mysql-operator-1-%E9%83%A8%E7%BD%B2/#æ€»ç»“"},{"categories":["mysql-operator"],"content":" ç›®çš„æ¥ä¸‹æ¥çš„ä¸€æ®µæ—¶é—´æ‰“ç®—åšä¸ªbitpoke/mysql-operatorçš„æŠ€æœ¯è§£è¯»ã€‚ å‡ºäºä¸¤ç‚¹è€ƒè™‘ï¼š æ·±åº¦ä½¿ç”¨å’Œæ”¹é€ è¿‡ï¼Œç†Ÿæ‚‰è¿™ä¸ªé¡¹ç›® åœ¨ k8s ä¸Šéƒ¨ç½² mysql-operator çš„éœ€æ±‚æ¯”è¾ƒå¤§ï¼Œä½†æ˜¯æœ‰æ²¡æœ‰æ¯”è¾ƒå¥½çš„åŸç†è§£è¯» ","date":"2023-03-07","objectID":"/mysql-operator-0/:1:0","series":null,"tags":["mysql","operator"],"title":"bitpoke/mysql-operator æŠ€æœ¯è§£è¯»ç³»åˆ— - åº","uri":"/mysql-operator-0/#ç›®çš„"},{"categories":["mysql-operator"],"content":" é¡¹ç›® https://github.com/bitpoke/mysql-operator ","date":"2023-03-07","objectID":"/mysql-operator-0/:2:0","series":null,"tags":["mysql","operator"],"title":"bitpoke/mysql-operator æŠ€æœ¯è§£è¯»ç³»åˆ— - åº","uri":"/mysql-operator-0/#é¡¹ç›®"},{"categories":["mysql-operator"],"content":" èŒƒå›´åŒ…æ‹¬ä½†ä¸é™äºï¼š åˆ›å»ºé›†ç¾¤ åˆ é™¤é›†ç¾¤ å‚æ•°ä¿®æ”¹ db user æ“ä½œ é…ç½®å˜æ›´ å¤‡ä»½æ•°æ® æ¢å¤æ•°æ® è‡ªåŠ¨é«˜å¯ç”¨ ä¸»å¤‡åˆ‡æ¢ â€¦ å¦‚æœæœ‰å…¶ä»–å†…å®¹æƒ³è¦äº†è§£ä¹Ÿå¯ä»¥ç•™è¨€~ ","date":"2023-03-07","objectID":"/mysql-operator-0/:3:0","series":null,"tags":["mysql","operator"],"title":"bitpoke/mysql-operator æŠ€æœ¯è§£è¯»ç³»åˆ— - åº","uri":"/mysql-operator-0/#èŒƒå›´"},{"categories":["Blog"],"content":" å®‰è£…æœ€æ–°ç‰ˆgohttps://www.runoob.com/go/go-environment.html ","date":"2023-03-04","objectID":"/how-to-run/:1:0","series":null,"tags":["Hugo"],"title":"How to Run","uri":"/how-to-run/#å®‰è£…æœ€æ–°ç‰ˆgo"},{"categories":["Blog"],"content":" å®‰è£…hugohttps://github.com/gohugoio/hugo CGO_ENABLED=1 go install --tags extended github.com/gohugoio/hugo@latest ","date":"2023-03-04","objectID":"/how-to-run/:2:0","series":null,"tags":["Hugo"],"title":"How to Run","uri":"/how-to-run/#å®‰è£…hugo"},{"categories":["Blog"],"content":" æ–°å»ºç§æœ‰ä»“åº“å¹¶åˆå§‹åŒ–æ–°å»ºç§æœ‰ä»“åº“ https://github.com/xxx/xxx git clone https://github.com/xxxx/xxx cd xxxx hugo new site . --force ","date":"2023-03-04","objectID":"/how-to-run/:3:0","series":null,"tags":["Hugo"],"title":"How to Run","uri":"/how-to-run/#æ–°å»ºç§æœ‰ä»“åº“å¹¶åˆå§‹åŒ–"},{"categories":["Blog"],"content":" æ›´æ–°ä¸»é¢˜ git submodule add https://github.com/HEIGE-PCloud/DoIt.git themes/DoIt echo 'theme = \"DoIt\"' \u003e\u003e config.toml ","date":"2023-03-04","objectID":"/how-to-run/:4:0","series":null,"tags":["Hugo"],"title":"How to Run","uri":"/how-to-run/#æ›´æ–°ä¸»é¢˜"},{"categories":["Blog"],"content":" æ›´å¤šé…ç½®https://hugodoit.pages.dev/zh-cn/theme-documentation-basics/ (éå¿…é¡»)å¼€å¯è¯„è®ºhttps://giscus.app/zh-CN [params.page.comment.giscus] enable = true # owner/repo dataRepo = \"xxxx/xxxx.github.io\" dataRepoId = \"xxxx\" dataCategory = \"Announcements\" dataCategoryId = \"xxxxx\" dataMapping = \"pathname\" dataReactionsEnabled = \"1\" dataEmitMetadata = \"0\" dataInputPosition = \"top\" lightTheme = \"light\" darkTheme = \"dark\" dataLang = \"en\" ","date":"2023-03-04","objectID":"/how-to-run/:4:1","series":null,"tags":["Hugo"],"title":"How to Run","uri":"/how-to-run/#æ›´å¤šé…ç½®"},{"categories":["Blog"],"content":" æ›´å¤šé…ç½®https://hugodoit.pages.dev/zh-cn/theme-documentation-basics/ (éå¿…é¡»)å¼€å¯è¯„è®ºhttps://giscus.app/zh-CN [params.page.comment.giscus] enable = true # owner/repo dataRepo = \"xxxx/xxxx.github.io\" dataRepoId = \"xxxx\" dataCategory = \"Announcements\" dataCategoryId = \"xxxxx\" dataMapping = \"pathname\" dataReactionsEnabled = \"1\" dataEmitMetadata = \"0\" dataInputPosition = \"top\" lightTheme = \"light\" darkTheme = \"dark\" dataLang = \"en\" ","date":"2023-03-04","objectID":"/how-to-run/:4:1","series":null,"tags":["Hugo"],"title":"How to Run","uri":"/how-to-run/#éå¿…é¡»å¼€å¯è¯„è®º"},{"categories":["Blog"],"content":" æ–°å»ºæ–‡ç«  hugo new posts/how-to-run.md vim content/posts/how-to-run.md ","date":"2023-03-04","objectID":"/how-to-run/:5:0","series":null,"tags":["Hugo"],"title":"How to Run","uri":"/how-to-run/#æ–°å»ºæ–‡ç« "},{"categories":["Blog"],"content":" è¿è¡Œçœ‹æ•ˆæœ é»˜è®¤æƒ…å†µä¸‹, æ‰€æœ‰æ–‡ç« å’Œé¡µé¢å‡ä½œä¸ºè‰ç¨¿åˆ›å»º. å¦‚æœæƒ³è¦æ¸²æŸ“è¿™äº›é¡µé¢, è¯·ä»å…ƒæ•°æ®ä¸­åˆ é™¤å±æ€§ draft: true, è®¾ç½®å±æ€§ draft: false æˆ–è€…ä¸º hugo å‘½ä»¤æ·»åŠ  -D/â€“buildDrafts å‚æ•°. ç”±äºæœ¬ä¸»é¢˜ä½¿ç”¨äº† Hugo ä¸­çš„ .Scratch æ¥å®ç°ä¸€äº›ç‰¹æ€§, éå¸¸å»ºè®®ä½ ä¸º hugo server å‘½ä»¤æ·»åŠ  â€“disableFastRender å‚æ•°æ¥å®æ—¶é¢„è§ˆä½ æ­£åœ¨ç¼–è¾‘çš„æ–‡ç« é¡µé¢. hugo serve çš„é»˜è®¤è¿è¡Œç¯å¢ƒæ˜¯ development, è€Œ hugo çš„é»˜è®¤è¿è¡Œç¯å¢ƒæ˜¯ production. ç”±äºæœ¬åœ° development ç¯å¢ƒçš„é™åˆ¶, è¯„è®ºç³»ç»Ÿ, CDN å’Œ fingerprint ä¸ä¼šåœ¨ development ç¯å¢ƒä¸‹å¯ç”¨. ä½ å¯ä»¥ä½¿ç”¨ hugo serve -e production å‘½ä»¤æ¥å¼€å¯è¿™äº›ç‰¹æ€§. hugo server -D --disableFastRender -e production ","date":"2023-03-04","objectID":"/how-to-run/:6:0","series":null,"tags":["Hugo"],"title":"How to Run","uri":"/how-to-run/#è¿è¡Œçœ‹æ•ˆæœ"},{"categories":["Blog"],"content":" é…ç½®github action å®ç°æäº¤åŒæ­¥åˆ°ä¸»é¡µ","date":"2023-03-04","objectID":"/how-to-run/:7:0","series":null,"tags":["Hugo"],"title":"How to Run","uri":"/how-to-run/#é…ç½®github-action-å®ç°æäº¤åŒæ­¥åˆ°ä¸»é¡µ"},{"categories":["Blog"],"content":" é…ç½® api tokenhttps://zhuanlan.zhihu.com/p/568764664 åœ¨ç§æœ‰ä»“åº“ä¸­é…ç½® PERSONAL_TOKEN ","date":"2023-03-04","objectID":"/how-to-run/:7:1","series":null,"tags":["Hugo"],"title":"How to Run","uri":"/how-to-run/#é…ç½®-api-token"},{"categories":["Blog"],"content":" æ–°å»ºä¸ªäººä¸»é¡µhttps://github.com/xxxx/xxx.github.io ","date":"2023-03-04","objectID":"/how-to-run/:7:2","series":null,"tags":["Hugo"],"title":"How to Run","uri":"/how-to-run/#æ–°å»ºä¸ªäººä¸»é¡µ"},{"categories":["Blog"],"content":" æ·»åŠ  workflowvim .github/workflows/gh-pages.yml name: GitHub Pages on: push: branches: - main # Set a branch to deploy pull_request: jobs: deploy: runs-on: ubuntu-latest concurrency: group: ${{ github.workflow }}-${{ github.ref }} steps: - uses: actions/checkout@v3 with: submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: '0.111.1' extended: true - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 if: ${{ github.ref == 'refs/heads/main' }} with: external_repository: xxx/xxx.github.io publish_dir: ./public publish_branch: gh-page personal_token: ${{ secrets.PERSONAL_TOKEN }} commit_message: ${{ github.event.head_commit.message }} ","date":"2023-03-04","objectID":"/how-to-run/:7:3","series":null,"tags":["Hugo"],"title":"How to Run","uri":"/how-to-run/#æ·»åŠ -workflow"},{"categories":["Blog"],"content":" æ¨é€ä»£ç ç­‰å¾…æ•ˆæœ git add . git commit -m \"add workflow\" git push ","date":"2023-03-04","objectID":"/how-to-run/:7:4","series":null,"tags":["Hugo"],"title":"How to Run","uri":"/how-to-run/#æ¨é€ä»£ç ç­‰å¾…æ•ˆæœ"}]